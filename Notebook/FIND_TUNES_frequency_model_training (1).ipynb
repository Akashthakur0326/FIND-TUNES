{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We train the Siamese model on multiple randomly sampled short audio segments per song. Positive pairs are drawn both from temporally corresponding regions between originals and covers, and from non-aligned regions to encourage robustness. This allows the model to first learn harmonic similarity and then generalize across structural variation, while negatives are sampled from different songs to enforce discrimination."
      ],
      "metadata": {
        "id": "qPxXdDzhWsHz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTEjF2ssWPCn"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP & DEPENDENCIES\n",
        "# ==============================================================================\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install necessary libraries if not present\n",
        "packages = [\"audiomentations\", \"torchaudio\"]\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import random\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from audiomentations import Compose, AddBackgroundNoise, PitchShift, TimeStretch, Gain, PolarityInversion\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ==============================================================================\n",
        " #DATA INGESTION (Drive -> Local)\n",
        "# ==============================================================================\n",
        "print(\"\\nüöÄ STARTING DATA EXTRACTION...\")\n",
        "\n",
        "# Define Source (Drive) and Destination (Local)\n",
        "ZIP_SOURCE_DIR = '/content/drive/MyDrive/FINE_TUNE_V3'\n",
        "LOCAL_BASE_DIR = '/content/data'\n",
        "\n",
        "# Paths for specific zips on Drive\n",
        "ZIP_ORIGINALS = os.path.join(ZIP_SOURCE_DIR, 'train_originals_1300.zip')\n",
        "ZIP_COVERS = os.path.join(ZIP_SOURCE_DIR, 'train_covers_1300.zip')\n",
        "ZIP_NOISE_16K = os.path.join(ZIP_SOURCE_DIR, 'noise_data_16k.zip')\n",
        "\n",
        "# Define Extraction Destinations\n",
        "DIR_ORIGINALS = os.path.join(LOCAL_BASE_DIR, 'originals')\n",
        "DIR_COVERS = os.path.join(LOCAL_BASE_DIR, 'covers')\n",
        "DIR_NOISE_16K = os.path.join(LOCAL_BASE_DIR, 'noise_16k')\n",
        "\n",
        "# Create Directories\n",
        "for d in [DIR_ORIGINALS, DIR_COVERS, DIR_NOISE_16K]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Unzip Functions\n",
        "def safe_unzip(zip_path, dest_path, desc):\n",
        "    if not os.listdir(dest_path):\n",
        "        print(f\"üìÇ Unzipping {desc}...\")\n",
        "        subprocess.run(f\"unzip -q -n '{zip_path}' -d '{dest_path}'\", shell=True)\n",
        "    else:\n",
        "        print(f\"‚úÖ {desc} already extracted.\")\n",
        "\n",
        "safe_unzip(ZIP_ORIGINALS, DIR_ORIGINALS, \"Training Originals\")\n",
        "safe_unzip(ZIP_COVERS, DIR_COVERS, \"Training Covers\")\n",
        "safe_unzip(ZIP_NOISE_16K, DIR_NOISE_16K, \"Noise Data\")\n",
        "\n",
        "# --- NOISE CONVERSION (MP3 -> WAV) ---\n",
        "# Ensures multi-thread safety for DataLoader\n",
        "def convert_mp3_to_wav(noise_dir):\n",
        "    mp3_files = glob.glob(os.path.join(noise_dir, '**', '*.mp3'), recursive=True)\n",
        "    if not mp3_files:\n",
        "        return\n",
        "    print(f\"üîÑ Converting {len(mp3_files)} noise MP3s to WAV...\")\n",
        "    for mp3_path in tqdm(mp3_files):\n",
        "        wav_path = mp3_path.replace('.mp3', '.wav')\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(mp3_path)\n",
        "            torchaudio.save(wav_path, waveform, sr)\n",
        "            os.remove(mp3_path)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "convert_mp3_to_wav(DIR_NOISE_16K)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ROBUST CSV MAPPING & VALIDATION\n",
        "# ==============================================================================\n",
        "def load_verified_pairs(csv_path, originals_dir, covers_dir):\n",
        "    \"\"\"\n",
        "    Reads the CSV and validates that the file pairs actually exist on the local disk.\n",
        "    Adapted for columns: 'original_filename', 'augmented_filename'\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"‚ùå Error: CSV not found at {csv_path}\")\n",
        "        return [], {}\n",
        "\n",
        "    print(f\"üìñ Reading CSV: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    valid_anchors = []\n",
        "    pair_map = {}\n",
        "    missing_count = 0\n",
        "\n",
        "    # --- DETECTED COLUMNS FROM YOUR INFO ---\n",
        "    col_orig = 'original_filename'\n",
        "    col_pair = 'augmented_filename'\n",
        "\n",
        "    print(f\"üîç Validating {len(df)} pairs...\")\n",
        "    print(f\"   Anchor Col: '{col_orig}'\")\n",
        "    print(f\"   Pair Col:   '{col_pair}'\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        orig_name = row[col_orig]\n",
        "        pair_name = row[col_pair]\n",
        "\n",
        "        # 1. Check Anchor in Originals Dir\n",
        "        path_orig = os.path.join(originals_dir, str(orig_name))\n",
        "\n",
        "        # 2. Check Pair in Covers Dir (or fallback to the absolute path column if present)\n",
        "        path_pair = os.path.join(covers_dir, str(pair_name))\n",
        "\n",
        "        # Fallback: If not in covers_dir, checks if 'path' column has a valid full path\n",
        "        if not os.path.exists(path_pair) and 'path' in row:\n",
        "             if os.path.exists(row['path']):\n",
        "                 path_pair = row['path']\n",
        "\n",
        "        # 3. Validate existence\n",
        "        if os.path.exists(path_orig) and os.path.exists(path_pair):\n",
        "            valid_anchors.append(orig_name)\n",
        "            pair_map[orig_name] = pair_name\n",
        "        else:\n",
        "            missing_count += 1\n",
        "            if missing_count <= 5:\n",
        "                # Debug print to help you see WHICH path is wrong\n",
        "                if not os.path.exists(path_orig):\n",
        "                    print(f\"   ‚ö†Ô∏è Missing Anchor: {path_orig}\")\n",
        "                if not os.path.exists(path_pair):\n",
        "                    print(f\"   ‚ö†Ô∏è Missing Pair:   {path_pair}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"‚úÖ Found {len(valid_anchors)} valid pairs locally.\")\n",
        "    if missing_count > 0:\n",
        "        print(f\"‚ùå Skipped {missing_count} pairs (files missing).\")\n",
        "\n",
        "    return valid_anchors, pair_map\n",
        "\n",
        "# --- EXECUTE PAIRING ---\n",
        "CSV_PATH = \"/content/drive/Othercomputers/My laptop/Desktop/FINE-TUNE/Data/dataset_tracking.csv\"\n",
        "DIR_ORIGINALS = '/content/data/originals'\n",
        "DIR_COVERS = '/content/data/covers' # Points to where 'augmented_filename' files live\n",
        "NOISE_PATH_FINAL = DIR_NOISE_16K\n",
        "\n",
        "# Run Validation\n",
        "VALID_ANCHORS, PAIR_MAP = load_verified_pairs(CSV_PATH, DIR_ORIGINALS, DIR_COVERS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define Paths\n",
        "ZIP_SOURCE_DIR = '/content/drive/MyDrive/FINE_TUNE_V3'\n",
        "ZIP_EVAL = os.path.join(ZIP_SOURCE_DIR, 'eval_originals_300.zip')\n",
        "DIR_EVAL = '/content/data/eval'\n",
        "\n",
        "# Unzip\n",
        "if not os.path.exists(DIR_EVAL):\n",
        "    os.makedirs(DIR_EVAL, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Unzipping Evaluation Set to {DIR_EVAL}...\")\n",
        "if os.path.exists(ZIP_EVAL):\n",
        "    subprocess.run(f\"unzip -q -n '{ZIP_EVAL}' -d '{DIR_EVAL}'\", shell=True)\n",
        "    num_files = len(os.listdir(DIR_EVAL))\n",
        "    print(f\"‚úÖ Success! Found {num_files} wav files ready for evaluation.\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find {ZIP_EVAL}. Check your Drive.\")"
      ],
      "metadata": {
        "id": "EAxSottTWYPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from audiomentations import (\n",
        "    Compose,\n",
        "    AddBackgroundNoise,\n",
        "    PitchShift,\n",
        "    TimeStretch,\n",
        "    Gain,\n",
        "    PolarityInversion\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DualObjectiveSiameseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Triplet dataset with:\n",
        "    - multiple stochastic 3s crops per song\n",
        "    - aligned + unaligned cover positives\n",
        "    - self-invariance task\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        anchor_list,\n",
        "        pair_map,\n",
        "        originals_dir,\n",
        "        covers_dir,\n",
        "        noise_dir,\n",
        "        sample_rate=16000,\n",
        "        duration=3.0,\n",
        "        aligned_cover_prob=0.6,\n",
        "        max_align_jitter_sec=2.0\n",
        "    ):\n",
        "        self.anchor_list = anchor_list\n",
        "        self.pair_map = pair_map\n",
        "        self.originals_dir = originals_dir\n",
        "        self.covers_dir = covers_dir\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.num_samples = int(sample_rate * duration)\n",
        "        self.aligned_cover_prob = aligned_cover_prob\n",
        "        self.max_align_jitter = int(max_align_jitter_sec * sample_rate)\n",
        "\n",
        "        self.num_songs = len(anchor_list)\n",
        "\n",
        "        # Augmentation\n",
        "        self.augment = Compose([\n",
        "            AddBackgroundNoise(\n",
        "                sounds_path=noise_dir,\n",
        "                min_snr_db=3.0,\n",
        "                max_snr_db=15.0,\n",
        "                p=0.8\n",
        "            ),\n",
        "            Gain(min_gain_db=-6.0, max_gain_db=6.0, p=0.2),\n",
        "            PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
        "            TimeStretch(min_rate=0.9, max_rate=1.1, p=0.4),\n",
        "            PolarityInversion(p=0.2),\n",
        "        ])\n",
        "\n",
        "        # Spectrogram\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=1024,\n",
        "            hop_length=512,\n",
        "            n_mels=64\n",
        "        )\n",
        "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _load_audio(self, path):\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(path)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
        "\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        return wav\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _crop(self, waveform, start_sample):\n",
        "        total_len = waveform.shape[1]\n",
        "\n",
        "        if total_len < self.num_samples:\n",
        "            waveform = F.pad(waveform, (0, self.num_samples - total_len))\n",
        "            return waveform[:, :self.num_samples]\n",
        "\n",
        "        start_sample = max(0, min(start_sample, total_len - self.num_samples))\n",
        "        return waveform[:, start_sample:start_sample + self.num_samples]\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _to_spec(self, audio_np):\n",
        "        tensor = torch.from_numpy(audio_np).unsqueeze(0)\n",
        "        spec = self.mel_transform(tensor)\n",
        "        spec = self.db_transform(spec)\n",
        "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
        "        return spec\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # -------- task selection --------\n",
        "        if idx < self.num_songs:\n",
        "            real_idx = idx\n",
        "            is_cover_task = True\n",
        "        else:\n",
        "            real_idx = idx - self.num_songs\n",
        "            is_cover_task = False\n",
        "\n",
        "        anchor_name = self.anchor_list[real_idx]\n",
        "        anchor_path = os.path.join(self.originals_dir, anchor_name)\n",
        "\n",
        "        anchor_wav = self._load_audio(anchor_path)\n",
        "        if anchor_wav is None:\n",
        "            anchor_wav = torch.zeros(1, self.num_samples)\n",
        "\n",
        "        total_len = anchor_wav.shape[1]\n",
        "        anchor_start = (\n",
        "            0 if total_len <= self.num_samples\n",
        "            else random.randint(0, total_len - self.num_samples)\n",
        "        )\n",
        "\n",
        "        anchor_crop = self._crop(anchor_wav, anchor_start)\n",
        "        anchor_raw = anchor_crop.squeeze(0).numpy()\n",
        "\n",
        "        # -------- positive --------\n",
        "        if is_cover_task:\n",
        "            cover_name = self.pair_map[anchor_name]\n",
        "            cover_path = os.path.join(self.covers_dir, cover_name)\n",
        "            cover_wav = self._load_audio(cover_path)\n",
        "\n",
        "            if cover_wav is None:\n",
        "                positive_raw = anchor_raw.copy()\n",
        "            else:\n",
        "                if random.random() < self.aligned_cover_prob:\n",
        "                    jitter = random.randint(-self.max_align_jitter, self.max_align_jitter)\n",
        "                    pos_start = anchor_start + jitter\n",
        "                else:\n",
        "                    pos_start = random.randint(\n",
        "                        0,\n",
        "                        max(0, cover_wav.shape[1] - self.num_samples)\n",
        "                    )\n",
        "\n",
        "                pos_crop = self._crop(cover_wav, pos_start)\n",
        "                positive_raw = pos_crop.squeeze(0).numpy()\n",
        "        else:\n",
        "            # self-invariance\n",
        "            positive_raw = anchor_raw.copy()\n",
        "\n",
        "        try:\n",
        "            positive_aug = self.augment(samples=positive_raw, sample_rate=self.sample_rate)\n",
        "        except Exception:\n",
        "            positive_aug = positive_raw\n",
        "\n",
        "        # -------- negative --------\n",
        "        neg_idx = random.randint(0, self.num_songs - 1)\n",
        "        while neg_idx == real_idx:\n",
        "            neg_idx = random.randint(0, self.num_songs - 1)\n",
        "\n",
        "        neg_path = os.path.join(self.originals_dir, self.anchor_list[neg_idx])\n",
        "        neg_wav = self._load_audio(neg_path)\n",
        "\n",
        "        if neg_wav is None:\n",
        "            negative_raw = anchor_raw.copy()\n",
        "        else:\n",
        "            neg_start = random.randint(\n",
        "                0,\n",
        "                max(0, neg_wav.shape[1] - self.num_samples)\n",
        "            )\n",
        "            neg_crop = self._crop(neg_wav, neg_start)\n",
        "            negative_raw = neg_crop.squeeze(0).numpy()\n",
        "\n",
        "        try:\n",
        "            negative_aug = self.augment(samples=negative_raw, sample_rate=self.sample_rate)\n",
        "        except Exception:\n",
        "            negative_aug = negative_raw\n",
        "\n",
        "        return (\n",
        "            self._to_spec(anchor_raw),\n",
        "            self._to_spec(positive_aug),\n",
        "            self._to_spec(negative_aug),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_songs * 2\n"
      ],
      "metadata": {
        "id": "jykc7h_SWYW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß± BLOCK 5: FINAL TRAINING LOOP (WITH MODEL B)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "\n",
        "# --- 1. MODEL DEFINITION (MODEL B - CHOSEN) ---\n",
        "class AudioSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # ----------- CONV BLOCK 1 -----------\n",
        "        # Input: (Batch, 1, 64, ~94)\n",
        "        \"\"\"\n",
        "        1 ‚Üí mono\n",
        "        64 ‚Üí mel bins\n",
        "        ~94 ‚Üí time frames (3 sec @ hop 512)\n",
        "\n",
        "        Conv Block 1 (learns Local time‚Äìfrequency edges)\n",
        "          Conv2d(1 ‚Üí 32)\n",
        "          BatchNorm\n",
        "          ReLU\n",
        "          MaxPool(2√ó2) (Early pooling removes noise)\n",
        "\n",
        "        Conv Block 2 (learns Harmonic stacks)\n",
        "          Conv2d(32 ‚Üí 64)\n",
        "          BatchNorm\n",
        "          ReLU\n",
        "          MaxPool(2√ó2)\n",
        "\n",
        "        Conv Block 3 (no pooling)\n",
        "          Conv2d(64 ‚Üí 128)\n",
        "          BatchNorm\n",
        "          ReLU\n",
        "        Why no MaxPool here?\n",
        "          Pooling here would destroy: melody contour | rhythmic micro-patterns\n",
        "          At this stage, the model is learning song identity, not noise suppression\n",
        "\n",
        "        Global Average Pooling\n",
        "          AdaptiveAvgPool2d((1,1)) (B, 128, H, W) ‚Üí (B, 128)\n",
        "\n",
        "        Projection head (embedding layer)\n",
        "          Linear(128 ‚Üí 256) (Increase representational capacity)\n",
        "          ReLU\n",
        "          Dropout(0.3)\n",
        "          Linear(256 ‚Üí embed_dim)\n",
        "\n",
        "        F.normalize(x, p=2)\n",
        "        This enforces:||embedding|| = 1\n",
        "            So:\n",
        "            cosine similarity = dot product\n",
        "            perfect for FAISS\n",
        "            stable for contrastive / triplet loss\n",
        "\n",
        "        \"\"\"\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)          # Downsample: 32 mels, 47 time\n",
        "        )\n",
        "\n",
        "        # ----------- CONV BLOCK 2 -----------\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)          # Downsample: 16 mels, 23 time\n",
        "        )\n",
        "\n",
        "        # ----------- CONV BLOCK 3 -----------\n",
        "        # No MaxPool here to preserve melody resolution\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "            # Output: (Batch, 128, 16, 23)\n",
        "        )\n",
        "\n",
        "        # ----------- GLOBAL POOL -----------\n",
        "        # Averages the feature map into a single vector\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # ----------- PROJECTION HEAD -----------\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.3),       # Safety against noise overfitting\n",
        "            nn.Linear(256, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.global_pool(x)          # -> (B, 128, 1, 1)\n",
        "        x = x.view(x.size(0), -1)        # -> (B, 128)\n",
        "        x = self.fc(x)                   # -> (B, embed_dim)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "    def forward(self, anchor, positive, negative,*args):\n",
        "        return self.forward_one(anchor), self.forward_one(positive), self.forward_one(negative)\n"
      ],
      "metadata": {
        "id": "bK5NRp10WYXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß± SHALLOW CNN TRAINING LOOP (TRIPLET LOSS + RESUME + AMP)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def train_siamese_network():\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nüî• Training on: {DEVICE}\")\n",
        "\n",
        "    # ---------------- CONFIG ----------------\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    LR = 1e-4\n",
        "    MARGIN = 0.75\n",
        "    PATIENCE = 4\n",
        "\n",
        "    # ---------------- OUTPUT PATHS ----------------\n",
        "    BASE_DIR = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN\"\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "    BEST_MODEL_PATH = os.path.join(BASE_DIR, \"best_shallow_cnn.pth\")\n",
        "    LATEST_CKPT_PATH = os.path.join(BASE_DIR, \"checkpoint_latest.pth\")\n",
        "\n",
        "    print(f\"üìÇ Models will be saved to: {BASE_DIR}\")\n",
        "\n",
        "    # ---------------- DATASET ----------------\n",
        "    if \"VALID_ANCHORS\" not in globals() or not VALID_ANCHORS:\n",
        "        raise RuntimeError(\"‚ùå VALID_ANCHORS not found. Run CSV mapping first.\")\n",
        "\n",
        "    dataset = DualObjectiveSiameseDataset(\n",
        "        anchor_list=VALID_ANCHORS,\n",
        "        pair_map=PAIR_MAP,\n",
        "        originals_dir=\"/content/data/originals\",\n",
        "        covers_dir=\"/content/data/covers\",\n",
        "        noise_dir=\"/content/data/noise_16k\",\n",
        "        sample_rate=16000,\n",
        "        duration=3.0\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Dataset ready: {len(dataset)} samples\")\n",
        "\n",
        "    # ---------------- MODEL ----------------\n",
        "    model = AudioSiameseNet().to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"min\",\n",
        "        factor=0.5,\n",
        "        patience=PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    # ---------------- RESUME LOGIC ----------------\n",
        "    start_epoch = 0\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    if os.path.exists(LATEST_CKPT_PATH):\n",
        "        print(f\"üîÑ Resuming from {LATEST_CKPT_PATH}\")\n",
        "        ckpt = torch.load(LATEST_CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
        "\n",
        "        start_epoch = ckpt[\"epoch\"] + 1\n",
        "        best_loss = ckpt[\"best_loss\"]\n",
        "\n",
        "        print(f\"   ‚ñ∂ Resumed at epoch {start_epoch}\")\n",
        "        print(f\"   ‚ñ∂ Best loss so far: {best_loss:.4f}\")\n",
        "    else:\n",
        "        print(\"üÜï No checkpoint found. Starting fresh training.\")\n",
        "\n",
        "    # ---------------- TRAINING ----------------\n",
        "    print(\"üöÄ Starting Shallow CNN Training...\")\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (anc, pos, neg) in enumerate(dataloader):\n",
        "            anc = anc.to(DEVICE, non_blocking=True)\n",
        "            pos = pos.to(DEVICE, non_blocking=True)\n",
        "            neg = neg.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast():\n",
        "                emb_a, emb_p, emb_n = model(anc, pos, neg)\n",
        "                loss = criterion(emb_a, emb_p, emb_n)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                print(\n",
        "                    f\"   Epoch {epoch+1} | Batch {batch_idx}/{len(dataloader)} \"\n",
        "                    f\"| Loss: {loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"\\nüì¢ Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # ---------------- LR Scheduler ----------------\n",
        "        old_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        scheduler.step(avg_loss)\n",
        "        new_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        if new_lr != old_lr:\n",
        "            print(f\"üìâ LR reduced: {old_lr:.2e} ‚Üí {new_lr:.2e}\")\n",
        "\n",
        "        # ---------------- SAVE BEST ----------------\n",
        "        if avg_loss < best_loss:\n",
        "            print(f\"‚≠ê New BEST: {best_loss:.4f} ‚Üí {avg_loss:.4f}\")\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "\n",
        "        # ---------------- SAVE CHECKPOINTS ----------------\n",
        "        epoch_ckpt_path = os.path.join(BASE_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "            \"loss\": avg_loss,\n",
        "            \"best_loss\": best_loss\n",
        "        }\n",
        "\n",
        "        torch.save(ckpt, LATEST_CKPT_PATH)      # rolling checkpoint\n",
        "        torch.save(ckpt, epoch_ckpt_path)       # backup checkpoint\n",
        "\n",
        "        print(f\"üíæ Saved checkpoint: {epoch_ckpt_path}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_siamese_network()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_FXkTSYWYYs",
        "outputId": "64bd9691-a72d-445e-fb1a-aa20e1275d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî• Training on: cuda\n",
            "üìÇ Models will be saved to: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN\n",
            "‚úÖ Dataset ready: 1900 samples\n",
            "üÜï No checkpoint found. Starting fresh training.\n",
            "üöÄ Starting Shallow CNN Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3059972637.py:62: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-3059972637.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Epoch 1 | Batch 0/30 | Loss: 0.7440\n",
            "   Epoch 1 | Batch 20/30 | Loss: 0.7497\n",
            "\n",
            "üì¢ Epoch 1/100 | Avg Loss: 0.7470\n",
            "‚≠ê New BEST: inf ‚Üí 0.7470\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_0.pth\n",
            "\n",
            "   Epoch 2 | Batch 0/30 | Loss: 0.7324\n",
            "   Epoch 2 | Batch 20/30 | Loss: 0.7319\n",
            "\n",
            "üì¢ Epoch 2/100 | Avg Loss: 0.7439\n",
            "‚≠ê New BEST: 0.7470 ‚Üí 0.7439\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_1.pth\n",
            "\n",
            "   Epoch 3 | Batch 0/30 | Loss: 0.7440\n",
            "   Epoch 3 | Batch 20/30 | Loss: 0.7407\n",
            "\n",
            "üì¢ Epoch 3/100 | Avg Loss: 0.7426\n",
            "‚≠ê New BEST: 0.7439 ‚Üí 0.7426\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_2.pth\n",
            "\n",
            "   Epoch 4 | Batch 0/30 | Loss: 0.7451\n",
            "   Epoch 4 | Batch 20/30 | Loss: 0.7343\n",
            "\n",
            "üì¢ Epoch 4/100 | Avg Loss: 0.7383\n",
            "‚≠ê New BEST: 0.7426 ‚Üí 0.7383\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_3.pth\n",
            "\n",
            "   Epoch 5 | Batch 0/30 | Loss: 0.7437\n",
            "   Epoch 5 | Batch 20/30 | Loss: 0.7304\n",
            "\n",
            "üì¢ Epoch 5/100 | Avg Loss: 0.7332\n",
            "‚≠ê New BEST: 0.7383 ‚Üí 0.7332\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_4.pth\n",
            "\n",
            "   Epoch 6 | Batch 0/30 | Loss: 0.7130\n",
            "   Epoch 6 | Batch 20/30 | Loss: 0.7192\n",
            "\n",
            "üì¢ Epoch 6/100 | Avg Loss: 0.7239\n",
            "‚≠ê New BEST: 0.7332 ‚Üí 0.7239\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_5.pth\n",
            "\n",
            "   Epoch 7 | Batch 0/30 | Loss: 0.7157\n",
            "   Epoch 7 | Batch 20/30 | Loss: 0.7111\n",
            "\n",
            "üì¢ Epoch 7/100 | Avg Loss: 0.7107\n",
            "‚≠ê New BEST: 0.7239 ‚Üí 0.7107\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_6.pth\n",
            "\n",
            "   Epoch 8 | Batch 0/30 | Loss: 0.7248\n",
            "   Epoch 8 | Batch 20/30 | Loss: 0.6397\n",
            "\n",
            "üì¢ Epoch 8/100 | Avg Loss: 0.6755\n",
            "‚≠ê New BEST: 0.7107 ‚Üí 0.6755\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_7.pth\n",
            "\n",
            "   Epoch 9 | Batch 0/30 | Loss: 0.6400\n",
            "   Epoch 9 | Batch 20/30 | Loss: 0.5992\n",
            "\n",
            "üì¢ Epoch 9/100 | Avg Loss: 0.6259\n",
            "‚≠ê New BEST: 0.6755 ‚Üí 0.6259\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_8.pth\n",
            "\n",
            "   Epoch 10 | Batch 0/30 | Loss: 0.5874\n",
            "   Epoch 10 | Batch 20/30 | Loss: 0.5743\n",
            "\n",
            "üì¢ Epoch 10/100 | Avg Loss: 0.5750\n",
            "‚≠ê New BEST: 0.6259 ‚Üí 0.5750\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_9.pth\n",
            "\n",
            "   Epoch 11 | Batch 0/30 | Loss: 0.5648\n",
            "   Epoch 11 | Batch 20/30 | Loss: 0.5475\n",
            "\n",
            "üì¢ Epoch 11/100 | Avg Loss: 0.5356\n",
            "‚≠ê New BEST: 0.5750 ‚Üí 0.5356\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_10.pth\n",
            "\n",
            "   Epoch 12 | Batch 0/30 | Loss: 0.5139\n",
            "   Epoch 12 | Batch 20/30 | Loss: 0.5144\n",
            "\n",
            "üì¢ Epoch 12/100 | Avg Loss: 0.5043\n",
            "‚≠ê New BEST: 0.5356 ‚Üí 0.5043\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_11.pth\n",
            "\n",
            "   Epoch 13 | Batch 0/30 | Loss: 0.5245\n",
            "   Epoch 13 | Batch 20/30 | Loss: 0.4781\n",
            "\n",
            "üì¢ Epoch 13/100 | Avg Loss: 0.4761\n",
            "‚≠ê New BEST: 0.5043 ‚Üí 0.4761\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_12.pth\n",
            "\n",
            "   Epoch 14 | Batch 0/30 | Loss: 0.4607\n",
            "   Epoch 14 | Batch 20/30 | Loss: 0.4656\n",
            "\n",
            "üì¢ Epoch 14/100 | Avg Loss: 0.4464\n",
            "‚≠ê New BEST: 0.4761 ‚Üí 0.4464\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_13.pth\n",
            "\n",
            "   Epoch 15 | Batch 0/30 | Loss: 0.4435\n",
            "   Epoch 15 | Batch 20/30 | Loss: 0.4324\n",
            "\n",
            "üì¢ Epoch 15/100 | Avg Loss: 0.4291\n",
            "‚≠ê New BEST: 0.4464 ‚Üí 0.4291\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_14.pth\n",
            "\n",
            "   Epoch 16 | Batch 0/30 | Loss: 0.4517\n",
            "   Epoch 16 | Batch 20/30 | Loss: 0.3920\n",
            "\n",
            "üì¢ Epoch 16/100 | Avg Loss: 0.4092\n",
            "‚≠ê New BEST: 0.4291 ‚Üí 0.4092\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_15.pth\n",
            "\n",
            "   Epoch 17 | Batch 0/30 | Loss: 0.4176\n",
            "   Epoch 17 | Batch 20/30 | Loss: 0.3557\n",
            "\n",
            "üì¢ Epoch 17/100 | Avg Loss: 0.4061\n",
            "‚≠ê New BEST: 0.4092 ‚Üí 0.4061\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_16.pth\n",
            "\n",
            "   Epoch 18 | Batch 0/30 | Loss: 0.3801\n",
            "   Epoch 18 | Batch 20/30 | Loss: 0.3855\n",
            "\n",
            "üì¢ Epoch 18/100 | Avg Loss: 0.3783\n",
            "‚≠ê New BEST: 0.4061 ‚Üí 0.3783\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_17.pth\n",
            "\n",
            "   Epoch 19 | Batch 0/30 | Loss: 0.3990\n",
            "   Epoch 19 | Batch 20/30 | Loss: 0.4021\n",
            "\n",
            "üì¢ Epoch 19/100 | Avg Loss: 0.3886\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_18.pth\n",
            "\n",
            "   Epoch 20 | Batch 0/30 | Loss: 0.3561\n",
            "   Epoch 20 | Batch 20/30 | Loss: 0.3807\n",
            "\n",
            "üì¢ Epoch 20/100 | Avg Loss: 0.3846\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_19.pth\n",
            "\n",
            "   Epoch 21 | Batch 0/30 | Loss: 0.4198\n",
            "   Epoch 21 | Batch 20/30 | Loss: 0.3736\n",
            "\n",
            "üì¢ Epoch 21/100 | Avg Loss: 0.3777\n",
            "‚≠ê New BEST: 0.3783 ‚Üí 0.3777\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_20.pth\n",
            "\n",
            "   Epoch 22 | Batch 0/30 | Loss: 0.3547\n",
            "   Epoch 22 | Batch 20/30 | Loss: 0.3548\n",
            "\n",
            "üì¢ Epoch 22/100 | Avg Loss: 0.3587\n",
            "‚≠ê New BEST: 0.3777 ‚Üí 0.3587\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_21.pth\n",
            "\n",
            "   Epoch 23 | Batch 0/30 | Loss: 0.3648\n",
            "   Epoch 23 | Batch 20/30 | Loss: 0.3367\n",
            "\n",
            "üì¢ Epoch 23/100 | Avg Loss: 0.3660\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_22.pth\n",
            "\n",
            "   Epoch 24 | Batch 0/30 | Loss: 0.3618\n",
            "   Epoch 24 | Batch 20/30 | Loss: 0.3839\n",
            "\n",
            "üì¢ Epoch 24/100 | Avg Loss: 0.3598\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_23.pth\n",
            "\n",
            "   Epoch 25 | Batch 0/30 | Loss: 0.4012\n",
            "   Epoch 25 | Batch 20/30 | Loss: 0.3918\n",
            "\n",
            "üì¢ Epoch 25/100 | Avg Loss: 0.3661\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_24.pth\n",
            "\n",
            "   Epoch 26 | Batch 0/30 | Loss: 0.3533\n",
            "   Epoch 26 | Batch 20/30 | Loss: 0.3479\n",
            "\n",
            "üì¢ Epoch 26/100 | Avg Loss: 0.3581\n",
            "‚≠ê New BEST: 0.3587 ‚Üí 0.3581\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_25.pth\n",
            "\n",
            "   Epoch 27 | Batch 0/30 | Loss: 0.3358\n",
            "   Epoch 27 | Batch 20/30 | Loss: 0.3131\n",
            "\n",
            "üì¢ Epoch 27/100 | Avg Loss: 0.3642\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_26.pth\n",
            "\n",
            "   Epoch 28 | Batch 0/30 | Loss: 0.3380\n",
            "   Epoch 28 | Batch 20/30 | Loss: 0.3565\n",
            "\n",
            "üì¢ Epoch 28/100 | Avg Loss: 0.3511\n",
            "‚≠ê New BEST: 0.3581 ‚Üí 0.3511\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_27.pth\n",
            "\n",
            "   Epoch 29 | Batch 0/30 | Loss: 0.3274\n",
            "   Epoch 29 | Batch 20/30 | Loss: 0.4011\n",
            "\n",
            "üì¢ Epoch 29/100 | Avg Loss: 0.3607\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_28.pth\n",
            "\n",
            "   Epoch 30 | Batch 0/30 | Loss: 0.3377\n",
            "   Epoch 30 | Batch 20/30 | Loss: 0.3629\n",
            "\n",
            "üì¢ Epoch 30/100 | Avg Loss: 0.3599\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_29.pth\n",
            "\n",
            "   Epoch 31 | Batch 0/30 | Loss: 0.3352\n",
            "   Epoch 31 | Batch 20/30 | Loss: 0.3127\n",
            "\n",
            "üì¢ Epoch 31/100 | Avg Loss: 0.3501\n",
            "‚≠ê New BEST: 0.3511 ‚Üí 0.3501\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_30.pth\n",
            "\n",
            "   Epoch 32 | Batch 0/30 | Loss: 0.3031\n",
            "   Epoch 32 | Batch 20/30 | Loss: 0.3378\n",
            "\n",
            "üì¢ Epoch 32/100 | Avg Loss: 0.3527\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_31.pth\n",
            "\n",
            "   Epoch 33 | Batch 0/30 | Loss: 0.3498\n",
            "   Epoch 33 | Batch 20/30 | Loss: 0.3898\n",
            "\n",
            "üì¢ Epoch 33/100 | Avg Loss: 0.3551\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_32.pth\n",
            "\n",
            "   Epoch 34 | Batch 0/30 | Loss: 0.3697\n",
            "   Epoch 34 | Batch 20/30 | Loss: 0.3566\n",
            "\n",
            "üì¢ Epoch 34/100 | Avg Loss: 0.3400\n",
            "‚≠ê New BEST: 0.3501 ‚Üí 0.3400\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_33.pth\n",
            "\n",
            "   Epoch 35 | Batch 0/30 | Loss: 0.3451\n",
            "   Epoch 35 | Batch 20/30 | Loss: 0.3846\n",
            "\n",
            "üì¢ Epoch 35/100 | Avg Loss: 0.3479\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\n",
            "\n",
            "   Epoch 36 | Batch 0/30 | Loss: 0.3443\n",
            "   Epoch 36 | Batch 20/30 | Loss: 0.3693\n",
            "\n",
            "üì¢ Epoch 36/100 | Avg Loss: 0.3417\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_35.pth\n",
            "\n",
            "   Epoch 37 | Batch 0/30 | Loss: 0.3629\n",
            "   Epoch 37 | Batch 20/30 | Loss: 0.3467\n",
            "\n",
            "üì¢ Epoch 37/100 | Avg Loss: 0.3523\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_36.pth\n",
            "\n",
            "   Epoch 38 | Batch 0/30 | Loss: 0.3585\n",
            "   Epoch 38 | Batch 20/30 | Loss: 0.3673\n",
            "\n",
            "üì¢ Epoch 38/100 | Avg Loss: 0.3454\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_37.pth\n",
            "\n",
            "   Epoch 39 | Batch 0/30 | Loss: 0.3370\n",
            "   Epoch 39 | Batch 20/30 | Loss: 0.3636\n",
            "\n",
            "üì¢ Epoch 39/100 | Avg Loss: 0.3508\n",
            "üìâ LR reduced: 1.00e-04 ‚Üí 5.00e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_38.pth\n",
            "\n",
            "   Epoch 40 | Batch 0/30 | Loss: 0.3634\n",
            "   Epoch 40 | Batch 20/30 | Loss: 0.3590\n",
            "\n",
            "üì¢ Epoch 40/100 | Avg Loss: 0.3530\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_39.pth\n",
            "\n",
            "   Epoch 41 | Batch 0/30 | Loss: 0.3456\n",
            "   Epoch 41 | Batch 20/30 | Loss: 0.3663\n",
            "\n",
            "üì¢ Epoch 41/100 | Avg Loss: 0.3479\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_40.pth\n",
            "\n",
            "   Epoch 42 | Batch 0/30 | Loss: 0.3087\n",
            "   Epoch 42 | Batch 20/30 | Loss: 0.3279\n",
            "\n",
            "üì¢ Epoch 42/100 | Avg Loss: 0.3423\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_41.pth\n",
            "\n",
            "   Epoch 43 | Batch 0/30 | Loss: 0.3775\n",
            "   Epoch 43 | Batch 20/30 | Loss: 0.3577\n",
            "\n",
            "üì¢ Epoch 43/100 | Avg Loss: 0.3532\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_42.pth\n",
            "\n",
            "   Epoch 44 | Batch 0/30 | Loss: 0.3304\n",
            "   Epoch 44 | Batch 20/30 | Loss: 0.3600\n",
            "\n",
            "üì¢ Epoch 44/100 | Avg Loss: 0.3438\n",
            "üìâ LR reduced: 5.00e-05 ‚Üí 2.50e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_43.pth\n",
            "\n",
            "   Epoch 45 | Batch 0/30 | Loss: 0.3174\n",
            "   Epoch 45 | Batch 20/30 | Loss: 0.3336\n",
            "\n",
            "üì¢ Epoch 45/100 | Avg Loss: 0.3468\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_44.pth\n",
            "\n",
            "   Epoch 46 | Batch 0/30 | Loss: 0.3385\n",
            "   Epoch 46 | Batch 20/30 | Loss: 0.3708\n",
            "\n",
            "üì¢ Epoch 46/100 | Avg Loss: 0.3560\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_45.pth\n",
            "\n",
            "   Epoch 47 | Batch 0/30 | Loss: 0.3340\n",
            "   Epoch 47 | Batch 20/30 | Loss: 0.3529\n",
            "\n",
            "üì¢ Epoch 47/100 | Avg Loss: 0.3410\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_46.pth\n",
            "\n",
            "   Epoch 48 | Batch 0/30 | Loss: 0.3762\n",
            "   Epoch 48 | Batch 20/30 | Loss: 0.3872\n",
            "\n",
            "üì¢ Epoch 48/100 | Avg Loss: 0.3524\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_47.pth\n",
            "\n",
            "   Epoch 49 | Batch 0/30 | Loss: 0.3744\n",
            "   Epoch 49 | Batch 20/30 | Loss: 0.3826\n",
            "\n",
            "üì¢ Epoch 49/100 | Avg Loss: 0.3528\n",
            "üìâ LR reduced: 2.50e-05 ‚Üí 1.25e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_48.pth\n",
            "\n",
            "   Epoch 50 | Batch 0/30 | Loss: 0.3669\n",
            "   Epoch 50 | Batch 20/30 | Loss: 0.3757\n",
            "\n",
            "üì¢ Epoch 50/100 | Avg Loss: 0.3500\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_49.pth\n",
            "\n",
            "   Epoch 51 | Batch 0/30 | Loss: 0.3532\n",
            "   Epoch 51 | Batch 20/30 | Loss: 0.3098\n",
            "\n",
            "üì¢ Epoch 51/100 | Avg Loss: 0.3503\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_50.pth\n",
            "\n",
            "   Epoch 52 | Batch 0/30 | Loss: 0.4058\n",
            "   Epoch 52 | Batch 20/30 | Loss: 0.3609\n",
            "\n",
            "üì¢ Epoch 52/100 | Avg Loss: 0.3422\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_51.pth\n",
            "\n",
            "   Epoch 53 | Batch 0/30 | Loss: 0.3294\n",
            "   Epoch 53 | Batch 20/30 | Loss: 0.3147\n",
            "\n",
            "üì¢ Epoch 53/100 | Avg Loss: 0.3404\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_52.pth\n",
            "\n",
            "   Epoch 54 | Batch 0/30 | Loss: 0.3447\n",
            "   Epoch 54 | Batch 20/30 | Loss: 0.3667\n",
            "\n",
            "üì¢ Epoch 54/100 | Avg Loss: 0.3499\n",
            "üìâ LR reduced: 1.25e-05 ‚Üí 6.25e-06\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_53.pth\n",
            "\n",
            "   Epoch 55 | Batch 0/30 | Loss: 0.3566\n",
            "   Epoch 55 | Batch 20/30 | Loss: 0.3526\n",
            "\n",
            "üì¢ Epoch 55/100 | Avg Loss: 0.3499\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_54.pth\n",
            "\n",
            "   Epoch 56 | Batch 0/30 | Loss: 0.3087\n",
            "   Epoch 56 | Batch 20/30 | Loss: 0.3058\n",
            "\n",
            "üì¢ Epoch 56/100 | Avg Loss: 0.3484\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_55.pth\n",
            "\n",
            "   Epoch 57 | Batch 0/30 | Loss: 0.4022\n",
            "   Epoch 57 | Batch 20/30 | Loss: 0.3698\n",
            "\n",
            "üì¢ Epoch 57/100 | Avg Loss: 0.3527\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_56.pth\n",
            "\n",
            "   Epoch 58 | Batch 0/30 | Loss: 0.2944\n",
            "   Epoch 58 | Batch 20/30 | Loss: 0.3328\n",
            "\n",
            "üì¢ Epoch 58/100 | Avg Loss: 0.3438\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_57.pth\n",
            "\n",
            "   Epoch 59 | Batch 0/30 | Loss: 0.3893\n",
            "   Epoch 59 | Batch 20/30 | Loss: 0.3766\n",
            "\n",
            "üì¢ Epoch 59/100 | Avg Loss: 0.3521\n",
            "üìâ LR reduced: 6.25e-06 ‚Üí 3.13e-06\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_58.pth\n",
            "\n",
            "   Epoch 60 | Batch 0/30 | Loss: 0.3517\n",
            "   Epoch 60 | Batch 20/30 | Loss: 0.3471\n",
            "\n",
            "üì¢ Epoch 60/100 | Avg Loss: 0.3450\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_59.pth\n",
            "\n",
            "   Epoch 61 | Batch 0/30 | Loss: 0.3282\n",
            "   Epoch 61 | Batch 20/30 | Loss: 0.3906\n",
            "\n",
            "üì¢ Epoch 61/100 | Avg Loss: 0.3516\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_60.pth\n",
            "\n",
            "   Epoch 62 | Batch 0/30 | Loss: 0.3351\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3059972637.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mtrain_siamese_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3059972637.py\u001b[0m in \u001b[0;36mtrain_siamese_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0manc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;31m#               msg, ancdata, flags, addr = server.recvmsg(1, socket.CMSG_SPACE(a.itemsize))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;31m#               assert(len(ancdata) == 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m     \u001b[0;31m#               cmsg_level, cmsg_type, cmsg_data = ancdata[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0;31m#               a.frombytes(cmsg_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m#               print(\"Received fd \", a[0], \" (iteration #\", i, \")\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m# Decremented when that data has been given to the main thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;31m# Each worker should have at most self._prefetch_factor tasks outstanding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers_num_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m         \u001b[0;31m# Reset the worker queue cycle so it resumes next epoch at worker 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_queue_idx_cycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVAL"
      ],
      "metadata": {
        "id": "3a6LzgpLb4aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß™ BLOCK 8: EVALUATE CHECKPOINT 34\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import os, glob, random, math\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from audiomentations import Compose, AddBackgroundNoise, PitchShift, TimeStretch, Gain\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# ‚öôÔ∏è CONFIG\n",
        "# ------------------------------------------------------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EVAL_DIR = \"/content/data/eval\"\n",
        "NOISE_DIR = \"/content/data/noise_16k\"\n",
        "\n",
        "# üîë TARGET CHECKPOINT: EPOCH 34\n",
        "# Update this path if your checkpoints are stored elsewhere\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN\"\n",
        "MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch_34.pth\")\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "WIN_SEC = 3.0\n",
        "HOP_SEC = 1.5\n",
        "QUERY_LEN = 15\n",
        "INFERENCE_BATCH_SIZE = 64\n",
        "TOLERANCE = 1.5\n",
        "SIGMA = 0.5\n",
        "SPREAD_FACTOR = 0.3\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. ARCHITECTURE: SHALLOW CNN\n",
        "# ------------------------------------------------------------------------------\n",
        "class AudioSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.pool(x).view(x.size(0), -1)\n",
        "        return F.normalize(self.fc(x), p=2, dim=1)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. HELPER: ROBUST LOADING\n",
        "# ------------------------------------------------------------------------------\n",
        "def robust_load(path):\n",
        "    try:\n",
        "        wav_np, sr = sf.read(path)\n",
        "        wav_np = wav_np.astype(np.float32)\n",
        "        wav = torch.from_numpy(wav_np)\n",
        "        if wav.ndim == 1: wav = wav.unsqueeze(0)\n",
        "        else: wav = wav.t()\n",
        "        if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
        "        return wav, sr\n",
        "    except: return None, 0\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. HELPER: EMBEDDING\n",
        "# ------------------------------------------------------------------------------\n",
        "mel = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
        ").to(DEVICE)\n",
        "db = torchaudio.transforms.AmplitudeToDB().to(DEVICE)\n",
        "\n",
        "def audio_to_embedding(model, wav):\n",
        "    samples_win = int(SAMPLE_RATE * WIN_SEC)\n",
        "    samples_hop = int(SAMPLE_RATE * HOP_SEC)\n",
        "    if wav.shape[1] < samples_win: wav = F.pad(wav, (0, samples_win - wav.shape[1]))\n",
        "\n",
        "    windows, times = [], []\n",
        "    for i in range(0, wav.shape[1] - samples_win + 1, samples_hop):\n",
        "        windows.append(wav[:, i:i + samples_win])\n",
        "        times.append(i / SAMPLE_RATE)\n",
        "\n",
        "    if not windows: return None, None\n",
        "\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(windows), INFERENCE_BATCH_SIZE):\n",
        "        batch = torch.stack(windows[i : i + INFERENCE_BATCH_SIZE]).to(DEVICE)\n",
        "        spec = db(mel(batch))\n",
        "        spec = (spec - spec.mean(dim=(2,3), keepdim=True)) / (spec.std(dim=(2,3), keepdim=True) + 1e-6)\n",
        "        with torch.no_grad():\n",
        "            all_embeddings.append(model.forward_one(spec).cpu())\n",
        "\n",
        "    return torch.cat(all_embeddings), times\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. BUILD DB\n",
        "# ------------------------------------------------------------------------------\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, file_paths): self.files = file_paths\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        wav, sr = robust_load(path)\n",
        "        if wav is None: return torch.zeros(1, SAMPLE_RATE), \"ERROR\"\n",
        "        if sr != SAMPLE_RATE: wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wav)\n",
        "        return wav, os.path.basename(path)\n",
        "\n",
        "def build_database(model):\n",
        "    print(\"üèóÔ∏è Building vector database...\")\n",
        "    files = glob.glob(os.path.join(EVAL_DIR, \"*.wav\"))\n",
        "    loader = DataLoader(EvalDataset(files), batch_size=1, shuffle=False, num_workers=2)\n",
        "    vectors, metadata = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for wav, name in tqdm(loader):\n",
        "            if name[0] == \"ERROR\": continue\n",
        "            emb, times = audio_to_embedding(model, wav.squeeze(0))\n",
        "            if emb is None: continue\n",
        "            for i, t in enumerate(times):\n",
        "                vectors.append(emb[i])\n",
        "                metadata.append({\"name\": name[0], \"offset\": t})\n",
        "\n",
        "    if not vectors: return None, None\n",
        "    return torch.stack(vectors).to(DEVICE), metadata\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. RUNNER\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_scores(matches):\n",
        "    scores = defaultdict(lambda: defaultdict(float))\n",
        "    for dist, meta, q_t in matches:\n",
        "        w = math.exp(-(dist**2) / (2 * SIGMA**2))\n",
        "        if w < 0.01: continue\n",
        "        b = int(round((meta[\"offset\"] - q_t) / TOLERANCE))\n",
        "        scores[meta[\"name\"]][b] += w\n",
        "        scores[meta[\"name\"]][b-1] += w * SPREAD_FACTOR\n",
        "        scores[meta[\"name\"]][b+1] += w * SPREAD_FACTOR\n",
        "    return sorted([(k, max(v.values())) for k, v in scores.items()], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def run_evaluation(model, db_vecs, db_meta, trials=100):\n",
        "    if db_vecs is None: return\n",
        "    db_vecs = db_vecs.to(DEVICE)\n",
        "\n",
        "    modes = [\"clean\", \"soft\", \"hard\"]\n",
        "    augmenters = {\n",
        "        \"soft\": Compose([Gain(-3, 3, p=0.5), PitchShift(-1, 1, p=0.3)]),\n",
        "        \"hard\": Compose([AddBackgroundNoise(NOISE_DIR, 5, 15, p=1.0) if os.path.exists(NOISE_DIR) else Gain(0,0,p=0), PitchShift(-2, 2, p=0.8)])\n",
        "    }\n",
        "\n",
        "    results = {m: {\"t1\":0, \"t5\":0} for m in modes}\n",
        "    songs = list(set(m[\"name\"] for m in db_meta))\n",
        "\n",
        "    print(f\"\\n‚ö° Eval: {trials} trials/mode...\")\n",
        "    for mode in modes:\n",
        "        print(f\"‚ñ∂ {mode.upper()}\")\n",
        "        aug = augmenters.get(mode, None)\n",
        "        for _ in tqdm(range(trials)):\n",
        "            target = random.choice(songs)\n",
        "            wav, sr = robust_load(os.path.join(EVAL_DIR, target))\n",
        "            if wav is None: continue\n",
        "            if sr != SAMPLE_RATE: wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wav)\n",
        "\n",
        "            max_len = int(QUERY_LEN * SAMPLE_RATE)\n",
        "            if wav.shape[1] > max_len:\n",
        "                s = random.randint(0, wav.shape[1] - max_len)\n",
        "                wav = wav[:, s:s + max_len]\n",
        "\n",
        "            if aug:\n",
        "                try: wav = torch.from_numpy(aug(samples=wav.squeeze(0).numpy(), sample_rate=SAMPLE_RATE)).unsqueeze(0)\n",
        "                except: pass\n",
        "\n",
        "            q_emb, q_times = audio_to_embedding(model, wav)\n",
        "            if q_emb is None: continue\n",
        "\n",
        "            dists = torch.cdist(q_emb.to(DEVICE), db_vecs)\n",
        "            vals, idxs = torch.topk(dists, k=5, largest=False)\n",
        "\n",
        "            matches = []\n",
        "            for i in range(q_emb.shape[0]):\n",
        "                for k in range(5):\n",
        "                    matches.append((vals[i,k].item(), db_meta[idxs[i,k]], q_times[i]))\n",
        "\n",
        "            ranked = calculate_scores(matches)\n",
        "            if not ranked: continue\n",
        "\n",
        "            if target == ranked[0][0]: results[mode][\"t1\"] += 1\n",
        "            if target in [x[0] for x in ranked[:5]]: results[mode][\"t5\"] += 1\n",
        "\n",
        "    print(\"\\nüèÜ RESULTS (Epoch 34)\")\n",
        "    print(f\"{'MODE':<10} | {'TOP-1':<8} | {'TOP-5':<8}\")\n",
        "    print(\"-\" * 32)\n",
        "    for m in modes:\n",
        "        print(f\"{m.upper():<10} | {results[m]['t1']/trials*100:.1f}%     | {results[m]['t5']/trials*100:.1f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"‚ùå Checkpoint not found: {MODEL_PATH}\")\n",
        "        print(f\"   Check your drive path or epoch number.\")\n",
        "    else:\n",
        "        print(f\"üìÇ Loading Checkpoint 34: {MODEL_PATH}\")\n",
        "        model = AudioSiameseNet(embed_dim=128).to(DEVICE)\n",
        "\n",
        "        # üîë Extract 'model_state_dict' from checkpoint\n",
        "        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "        if \"model_state_dict\" in checkpoint:\n",
        "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "            print(f\"‚úÖ Weights loaded from Epoch {checkpoint.get('epoch', '?')}\")\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint) # Legacy/Raw weights fallback\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        db_vecs, db_meta = build_database(model)\n",
        "        run_evaluation(model, db_vecs, db_meta, trials=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl8NaXABWYZ5",
        "outputId": "54d471a0-4867-4ff8-c3a8-c11f0207251a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading Checkpoint 34: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\n",
            "‚úÖ Weights loaded from Epoch 34\n",
            "üèóÔ∏è Building vector database...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 284/284 [00:48<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö° Eval: 100 trials/mode...\n",
            "‚ñ∂ CLEAN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂ SOFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12<00:00,  8.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂ HARD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:14<00:00,  6.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ RESULTS (Epoch 34)\n",
            "MODE       | TOP-1    | TOP-5   \n",
            "--------------------------------\n",
            "CLEAN      | 98.0%     | 100.0%\n",
            "SOFT       | 97.0%     | 100.0%\n",
            "HARD       | 39.0%     | 52.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMAPRISON IN BOTH MODEL(shallow CNN and CRNN(CNN+bilstm+attention )) ACCURACY\n",
        "\n",
        "SHALLOW CNN\n",
        "        MODE       | TOP-1    | TOP-5   \n",
        "        \n",
        "        CLEAN      | 98.0%     | 100.0%\n",
        "        SOFT       | 97.0%     | 100.0%\n",
        "        HARD       | 39.0%     | 52.0%\n",
        "\n",
        "        Audio (3 sec)\n",
        "          ‚Üì\n",
        "        Mel Spectrogram (64 √ó ~94)\n",
        "          ‚Üì\n",
        "        CNN Block 1\n",
        "          ‚Üì\n",
        "        CNN Block 2\n",
        "          ‚Üì\n",
        "        CNN Block 3\n",
        "          ‚Üì\n",
        "        Global Average Pool\n",
        "          ‚Üì\n",
        "        Projection Head\n",
        "          ‚Üì\n",
        "        L2-Normalized 128-D Embedding\n",
        "\n",
        "\n",
        "CRNN\n",
        "      \n",
        "      CLEAN  | Top1: 27.0% | Top5: 43.0% | Top10: 48.0%\n",
        "      SOFT   | Top1: 25.0% | Top5: 32.0% | Top10: 40.0%\n",
        "      HARD   | Top1: 4.0% | Top5: 13.0% | Top10: 20.0%\n",
        "\n",
        "      Mel Spectrogram (64 √ó ~94)\n",
        "              ‚Üì\n",
        "      CNN  ‚Üí local timbre + pitch invariance\n",
        "              ‚Üì\n",
        "      BiLSTM ‚Üí temporal melody progression\n",
        "              ‚Üì\n",
        "      Attention ‚Üí focus on salient moments\n",
        "              ‚Üì\n",
        "      Projection ‚Üí fixed 128-D embedding\n",
        "\n",
        "\n",
        "SHALLOW CNN benefits\n",
        "\n",
        "      Early pooling removes noise\n",
        "\n",
        "      No temporal modeling = no overthinking\n",
        "\n",
        "      GlobalAvgPool enforces invariance\n",
        "\n",
        "      Embedding represents what, not when\n",
        "\n",
        "Why the BiLSTM CRNN underperforms (despite being ‚Äúsmarter‚Äù)\n",
        "\n",
        "\n",
        "      BiLSTM introduces ordering sensitivity\n",
        "\n",
        "      Your CRNN explicitly models:\n",
        "\n",
        "      melody progression over time\n",
        "\n",
        "      But in retrieval:\n",
        "        Covers reorder sections\n",
        "        Queries start at arbitrary points\n",
        "        Chorus ‚â† verse\n",
        "        3 seconds ‚â† meaningful musical sentence\n",
        "\n",
        "So the LSTM is forced to answer:‚ÄúDoes this sequence match another sequence?‚Äù\n",
        "\n",
        "But the correct question is:‚ÄúDoes this fragment contain song-identity evidence?‚Äù\n",
        "\n",
        "‚ÄúAttention focuses on salient moments like chorus‚Äù\n",
        "\n",
        "      That is correct for classification.\n",
        "\n",
        "      But for retrieval:\n",
        "      Problem:\n",
        "\n",
        "          Attention suppresses ‚Äúboring‚Äù frames\n",
        "          But boring frames still contain fingerprint info\n",
        "\n",
        "      In shallow CNN:\n",
        "\n",
        "          Everything votes equally\n",
        "          Weak evidence still accumulates\n",
        "\n",
        "In CRNN:\n",
        "\n",
        "Attention throws information away\n",
        "\n",
        "If the ‚Äúsalient‚Äù moment is not present in the 3s clip ‚Üí embedding collapses\n",
        "\n",
        "This explains:\n",
        "\n",
        "CLEAN drops from 98% ‚Üí 27%\n",
        "\n",
        "HARD collapses to 4%"
      ],
      "metadata": {
        "id": "tdySK0mZZU_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"import torch\n",
        "import os\n",
        "\n",
        "# 1. Define Paths\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN\"\n",
        "SOURCE_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch_34.pth\")\n",
        "DESTINATION_MODEL = os.path.join(CHECKPOINT_DIR, \"best_model.pth\")\n",
        "\n",
        "print(f\"üìÇ Source: {SOURCE_CHECKPOINT}\")\n",
        "print(f\"üìÇ Dest:   {DESTINATION_MODEL}\")\n",
        "\n",
        "if not os.path.exists(SOURCE_CHECKPOINT):\n",
        "    print(\"‚ùå Error: Source checkpoint not found.\")\n",
        "else:\n",
        "    # 2. Load the checkpoint\n",
        "    # Map to CPU to avoid GPU OOM if you are just doing file ops\n",
        "    checkpoint = torch.load(SOURCE_CHECKPOINT, map_location=\"cpu\")\n",
        "\n",
        "    # 3. Extract the weights\n",
        "    if \"model_state_dict\" in checkpoint:\n",
        "        print(f\"‚úÖ Found state dict for Epoch {checkpoint.get('epoch', '?')}\")\n",
        "        clean_weights = checkpoint[\"model_state_dict\"]\n",
        "    else:\n",
        "        # If it was already a clean weight file\n",
        "        clean_weights = checkpoint\n",
        "\n",
        "    # 4. Save as best_model.pth\n",
        "    torch.save(clean_weights, DESTINATION_MODEL)\n",
        "    print(f\"üíæ Saved successfully to: {DESTINATION_MODEL}\")\n",
        "\n",
        "    # 5. Verify file size\n",
        "    size_mb = os.path.getsize(DESTINATION_MODEL) / (1024 * 1024)\n",
        "    print(f\"üì¶ File Size: {size_mb:.2f} MB\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM10OpA3WYa6",
        "outputId": "f07cb5c1-c0db-46be-f4d7-1b2cf89deb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Source: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\n",
            "üìÇ Dest:   /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/best_model.pth\n",
            "‚úÖ Found state dict for Epoch 34\n",
            "üíæ Saved successfully to: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/best_model.pth\n",
            "üì¶ File Size: 0.62 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFtVjziiWYcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model B ‚Äî CRNN (CNN + BiLSTM + Attention)\n",
        "\n",
        "Initially I used a CNN-based Siamese network where temporal alignment was handled entirely in post-processing via offset clustering, similar to Shazam.\n",
        "Later, I explored a CRNN architecture with BiLSTM and attention, which pushes temporal modeling into the embedding itself. This improved robustness for noisy and cover-style audio, at the cost of higher latency. In the final design, I treat the CRNN as a high-confidence fallback when classical fingerprinting fails.\n",
        "\n",
        "\n",
        "Key properties\n",
        "\n",
        "        Preserves time ordering\n",
        "\n",
        "        Learns melodic evolution\n",
        "\n",
        "        Embedding answers: ‚Äúhow does this evolve over time?‚Äù\n"
      ],
      "metadata": {
        "id": "qfXt2EnYgaP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP & DEPENDENCIES\n",
        "# ==============================================================================\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install necessary libraries if not present\n",
        "packages = [\"audiomentations\", \"torchaudio\"]\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import random\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from audiomentations import Compose, AddBackgroundNoise, PitchShift, TimeStretch, Gain, PolarityInversion\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ==============================================================================\n",
        " #DATA INGESTION (Drive -> Local)\n",
        "# ==============================================================================\n",
        "print(\"\\nüöÄ STARTING DATA EXTRACTION...\")\n",
        "\n",
        "# Define Source (Drive) and Destination (Local)\n",
        "ZIP_SOURCE_DIR = '/content/drive/MyDrive/FINE_TUNE_V3'\n",
        "LOCAL_BASE_DIR = '/content/data'\n",
        "\n",
        "# Paths for specific zips on Drive\n",
        "ZIP_ORIGINALS = os.path.join(ZIP_SOURCE_DIR, 'train_originals_1300.zip')\n",
        "ZIP_COVERS = os.path.join(ZIP_SOURCE_DIR, 'train_covers_1300.zip')\n",
        "ZIP_NOISE_16K = os.path.join(ZIP_SOURCE_DIR, 'noise_data_16k.zip')\n",
        "\n",
        "# Define Extraction Destinations\n",
        "DIR_ORIGINALS = os.path.join(LOCAL_BASE_DIR, 'originals')\n",
        "DIR_COVERS = os.path.join(LOCAL_BASE_DIR, 'covers')\n",
        "DIR_NOISE_16K = os.path.join(LOCAL_BASE_DIR, 'noise_16k')\n",
        "\n",
        "# Create Directories\n",
        "for d in [DIR_ORIGINALS, DIR_COVERS, DIR_NOISE_16K]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Unzip Functions\n",
        "def safe_unzip(zip_path, dest_path, desc):\n",
        "    if not os.listdir(dest_path):\n",
        "        print(f\"üìÇ Unzipping {desc}...\")\n",
        "        subprocess.run(f\"unzip -q -n '{zip_path}' -d '{dest_path}'\", shell=True)\n",
        "    else:\n",
        "        print(f\"‚úÖ {desc} already extracted.\")\n",
        "\n",
        "safe_unzip(ZIP_ORIGINALS, DIR_ORIGINALS, \"Training Originals\")\n",
        "safe_unzip(ZIP_COVERS, DIR_COVERS, \"Training Covers\")\n",
        "safe_unzip(ZIP_NOISE_16K, DIR_NOISE_16K, \"Noise Data\")\n",
        "\n",
        "# --- NOISE CONVERSION (MP3 -> WAV) ---\n",
        "# Ensures multi-thread safety for DataLoader\n",
        "def convert_mp3_to_wav(noise_dir):\n",
        "    mp3_files = glob.glob(os.path.join(noise_dir, '**', '*.mp3'), recursive=True)\n",
        "    if not mp3_files:\n",
        "        return\n",
        "    print(f\"üîÑ Converting {len(mp3_files)} noise MP3s to WAV...\")\n",
        "    for mp3_path in tqdm(mp3_files):\n",
        "        wav_path = mp3_path.replace('.mp3', '.wav')\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(mp3_path)\n",
        "            torchaudio.save(wav_path, waveform, sr)\n",
        "            os.remove(mp3_path)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "convert_mp3_to_wav(DIR_NOISE_16K)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ROBUST CSV MAPPING & VALIDATION\n",
        "# ==============================================================================\n",
        "def load_verified_pairs(csv_path, originals_dir, covers_dir):\n",
        "    \"\"\"\n",
        "    Reads the CSV and validates that the file pairs actually exist on the local disk.\n",
        "    Adapted for columns: 'original_filename', 'augmented_filename'\n",
        "    \"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"‚ùå Error: CSV not found at {csv_path}\")\n",
        "        return [], {}\n",
        "\n",
        "    print(f\"üìñ Reading CSV: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    valid_anchors = []\n",
        "    pair_map = {}\n",
        "    missing_count = 0\n",
        "\n",
        "    # --- DETECTED COLUMNS FROM YOUR INFO ---\n",
        "    col_orig = 'original_filename'\n",
        "    col_pair = 'augmented_filename'\n",
        "\n",
        "    print(f\"üîç Validating {len(df)} pairs...\")\n",
        "    print(f\"   Anchor Col: '{col_orig}'\")\n",
        "    print(f\"   Pair Col:   '{col_pair}'\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        orig_name = row[col_orig]\n",
        "        pair_name = row[col_pair]\n",
        "\n",
        "        # 1. Check Anchor in Originals Dir\n",
        "        path_orig = os.path.join(originals_dir, str(orig_name))\n",
        "\n",
        "        # 2. Check Pair in Covers Dir (or fallback to the absolute path column if present)\n",
        "        path_pair = os.path.join(covers_dir, str(pair_name))\n",
        "\n",
        "        # Fallback: If not in covers_dir, checks if 'path' column has a valid full path\n",
        "        if not os.path.exists(path_pair) and 'path' in row:\n",
        "             if os.path.exists(row['path']):\n",
        "                 path_pair = row['path']\n",
        "\n",
        "        # 3. Validate existence\n",
        "        if os.path.exists(path_orig) and os.path.exists(path_pair):\n",
        "            valid_anchors.append(orig_name)\n",
        "            pair_map[orig_name] = pair_name\n",
        "        else:\n",
        "            missing_count += 1\n",
        "            if missing_count <= 5:\n",
        "                # Debug print to help you see WHICH path is wrong\n",
        "                if not os.path.exists(path_orig):\n",
        "                    print(f\"   ‚ö†Ô∏è Missing Anchor: {path_orig}\")\n",
        "                if not os.path.exists(path_pair):\n",
        "                    print(f\"   ‚ö†Ô∏è Missing Pair:   {path_pair}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"‚úÖ Found {len(valid_anchors)} valid pairs locally.\")\n",
        "    if missing_count > 0:\n",
        "        print(f\"‚ùå Skipped {missing_count} pairs (files missing).\")\n",
        "\n",
        "    return valid_anchors, pair_map\n",
        "\n",
        "# --- EXECUTE PAIRING ---\n",
        "CSV_PATH = \"/content/drive/Othercomputers/My laptop/Desktop/FINE-TUNE/Data/dataset_tracking.csv\"\n",
        "DIR_ORIGINALS = '/content/data/originals'\n",
        "DIR_COVERS = '/content/data/covers' # Points to where 'augmented_filename' files live\n",
        "NOISE_PATH_FINAL = DIR_NOISE_16K\n",
        "\n",
        "# Run Validation\n",
        "VALID_ANCHORS, PAIR_MAP = load_verified_pairs(CSV_PATH, DIR_ORIGINALS, DIR_COVERS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0gajs3aoL5S",
        "outputId": "bb1ccbdb-8449-4082-cd3d-65aefa399309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "üöÄ STARTING DATA EXTRACTION...\n",
            "üìÇ Unzipping Training Originals...\n",
            "üìÇ Unzipping Training Covers...\n",
            "üìÇ Unzipping Noise Data...\n",
            "üîÑ Converting 110 noise MP3s to WAV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [00:00<00:00, 10179.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Reading CSV: /content/drive/Othercomputers/My laptop/Desktop/FINE-TUNE/Data/dataset_tracking.csv\n",
            "üîç Validating 950 pairs...\n",
            "   Anchor Col: 'original_filename'\n",
            "   Pair Col:   'augmented_filename'\n",
            "----------------------------------------\n",
            "‚úÖ Found 950 valid pairs locally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define Paths\n",
        "ZIP_SOURCE_DIR = '/content/drive/MyDrive/FINE_TUNE_V3'\n",
        "ZIP_EVAL = os.path.join(ZIP_SOURCE_DIR, 'eval_originals_300.zip')\n",
        "DIR_EVAL = '/content/data/eval'\n",
        "\n",
        "# Unzip\n",
        "if not os.path.exists(DIR_EVAL):\n",
        "    os.makedirs(DIR_EVAL, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Unzipping Evaluation Set to {DIR_EVAL}...\")\n",
        "if os.path.exists(ZIP_EVAL):\n",
        "    subprocess.run(f\"unzip -q -n '{ZIP_EVAL}' -d '{DIR_EVAL}'\", shell=True)\n",
        "    num_files = len(os.listdir(DIR_EVAL))\n",
        "    print(f\"‚úÖ Success! Found {num_files} wav files ready for evaluation.\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find {ZIP_EVAL}. Check your Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5NCj4pPoaFT",
        "outputId": "e8f89954-ee75-41d7-9576-3d25f994ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Unzipping Evaluation Set to /content/data/eval...\n",
            "‚úÖ Success! Found 284 wav files ready for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from audiomentations import (\n",
        "    Compose,\n",
        "    AddBackgroundNoise,\n",
        "    PitchShift,\n",
        "    TimeStretch,\n",
        "    Gain,\n",
        "    PolarityInversion\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DualObjectiveSiameseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Triplet Dataset for CRNN-based Audio Embedding\n",
        "\n",
        "    Key properties:\n",
        "    - Multiple random 3-sec clips per song across epochs\n",
        "    - Aligned + unaligned cover positives (curriculum)\n",
        "    - Self-invariance task included\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        anchor_list,\n",
        "        pair_map,\n",
        "        originals_dir,\n",
        "        covers_dir,\n",
        "        noise_dir,\n",
        "        sample_rate=16000,\n",
        "        duration=3.0,\n",
        "        aligned_cover_prob=0.6,   # üîë aligned vs random cover positives\n",
        "        max_align_jitter_sec=2.0 # üîë allow loose alignment (¬± seconds)\n",
        "    ):\n",
        "        self.anchor_list = anchor_list\n",
        "        self.pair_map = pair_map\n",
        "        self.originals_dir = originals_dir\n",
        "        self.covers_dir = covers_dir\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.num_samples = int(sample_rate * duration)\n",
        "        self.max_align_jitter = int(max_align_jitter_sec * sample_rate)\n",
        "\n",
        "        self.aligned_cover_prob = aligned_cover_prob\n",
        "        self.num_songs = len(anchor_list)\n",
        "\n",
        "        # üîä Strong but realistic augmentation\n",
        "        self.augment = Compose([\n",
        "            AddBackgroundNoise(\n",
        "                sounds_path=noise_dir,\n",
        "                min_snr_db=3.0,\n",
        "                max_snr_db=15.0,\n",
        "                p=0.8\n",
        "            ),\n",
        "            Gain(min_gain_db=-6.0, max_gain_db=6.0, p=0.2),\n",
        "            PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
        "            TimeStretch(min_rate=0.9, max_rate=1.1, p=0.4),\n",
        "            PolarityInversion(p=0.2),\n",
        "        ])\n",
        "\n",
        "        # Spectrogram pipeline (CRNN-friendly)\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=1024,\n",
        "            hop_length=512,\n",
        "            n_mels=64\n",
        "        )\n",
        "        self.db_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Audio loading + stochastic cropping\n",
        "    # ------------------------------------------------------------------\n",
        "    def _load_crop_process(self, path, start_sample=None):\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "        except Exception:\n",
        "            return np.zeros(self.num_samples, dtype=np.float32)\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        total_len = waveform.shape[1]\n",
        "\n",
        "        # Pad if too short\n",
        "        if total_len < self.num_samples:\n",
        "            waveform = F.pad(waveform, (0, self.num_samples - total_len))\n",
        "            return waveform.squeeze(0).numpy()\n",
        "\n",
        "        # Decide crop start\n",
        "        if start_sample is None:\n",
        "            start_sample = random.randint(0, total_len - self.num_samples)\n",
        "        else:\n",
        "            start_sample = max(0, min(start_sample, total_len - self.num_samples))\n",
        "\n",
        "        crop = waveform[:, start_sample:start_sample + self.num_samples]\n",
        "        return crop.squeeze(0).numpy()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Waveform ‚Üí Mel Spectrogram\n",
        "    # ------------------------------------------------------------------\n",
        "    def _to_spec(self, audio_np):\n",
        "        tensor = torch.from_numpy(audio_np).unsqueeze(0)\n",
        "        spec = self.mel_transform(tensor)\n",
        "        spec = self.db_transform(spec)\n",
        "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
        "        return spec\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Triplet sampling logic\n",
        "    # ------------------------------------------------------------------\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 1. Task selection (dual objective)\n",
        "        # --------------------------------------------------------------\n",
        "        if idx < self.num_songs:\n",
        "            real_idx = idx\n",
        "            is_cover_task = True\n",
        "        else:\n",
        "            real_idx = idx - self.num_songs\n",
        "            is_cover_task = False\n",
        "\n",
        "        anchor_name = self.anchor_list[real_idx]\n",
        "        anchor_path = os.path.join(self.originals_dir, anchor_name)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 2. Anchor: RANDOM 3-sec window (every call)\n",
        "        # --------------------------------------------------------------\n",
        "        anchor_raw = self._load_crop_process(anchor_path)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 3. Positive sampling\n",
        "        # --------------------------------------------------------------\n",
        "        if is_cover_task:\n",
        "            cover_name = self.pair_map[anchor_name]\n",
        "            cover_path = os.path.join(self.covers_dir, cover_name)\n",
        "\n",
        "            if random.random() < self.aligned_cover_prob:\n",
        "                # üîë ALIGNED (loose) ‚Äî SAFE VERSION\n",
        "                anchor_total_len = len(anchor_raw)\n",
        "\n",
        "                anchor_start = random.randint(\n",
        "                    0, max(0, anchor_total_len - self.num_samples)\n",
        "                )\n",
        "\n",
        "                jitter = random.randint(-self.max_align_jitter, self.max_align_jitter)\n",
        "                cover_start = anchor_start + jitter\n",
        "\n",
        "                positive_raw = self._load_crop_process(\n",
        "                    cover_path,\n",
        "                    start_sample=cover_start\n",
        "                )\n",
        "            else:\n",
        "                # üîë UNALIGNED\n",
        "                positive_raw = self._load_crop_process(cover_path)\n",
        "        else:\n",
        "            # Self-invariance task\n",
        "            positive_raw = anchor_raw.copy()\n",
        "\n",
        "        # Augment positive\n",
        "        try:\n",
        "            positive_aug = self.augment(\n",
        "                samples=positive_raw,\n",
        "                sample_rate=self.sample_rate\n",
        "            )\n",
        "        except Exception:\n",
        "            positive_aug = positive_raw\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # 4. Negative sampling (different song)\n",
        "        # --------------------------------------------------------------\n",
        "        neg_idx = random.randint(0, self.num_songs - 1)\n",
        "        while neg_idx == real_idx:\n",
        "            neg_idx = random.randint(0, self.num_songs - 1)\n",
        "\n",
        "        neg_name = self.anchor_list[neg_idx]\n",
        "        neg_path = os.path.join(self.originals_dir, neg_name)\n",
        "\n",
        "        negative_raw = self._load_crop_process(neg_path)\n",
        "\n",
        "        try:\n",
        "            negative_aug = self.augment(\n",
        "                samples=negative_raw,\n",
        "                sample_rate=self.sample_rate\n",
        "            )\n",
        "        except Exception:\n",
        "            negative_aug = negative_raw\n",
        "\n",
        "        return (\n",
        "            self._to_spec(anchor_raw),\n",
        "            self._to_spec(positive_aug),\n",
        "            self._to_spec(negative_aug)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        # Each song participates in:\n",
        "        # 1√ó cover task + 1√ó self task per epoch\n",
        "        return self.num_songs * 2\n"
      ],
      "metadata": {
        "id": "HcLYQ1ijlASX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mel Spectrogram (64 √ó ~94)\n",
        "‚Üí CNN backbone (freq reduced ‚Üí 1)\n",
        "‚Üí Sequence of length T\n",
        "‚Üí BiLSTM (2 layers)\n",
        "‚Üí Attention pooling\n",
        "‚Üí FC ‚Üí 128-D embedding\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "x1Z0qjB0WYdV",
        "outputId": "cd568d40-00f9-471c-93c2-da0f0e49be3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMel Spectrogram (64 √ó ~94)\\n‚Üí CNN backbone (freq reduced ‚Üí 1)\\n‚Üí Sequence of length T\\n‚Üí BiLSTM (2 layers)\\n‚Üí Attention pooling\\n‚Üí FC ‚Üí 128-D embedding\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mel Spectrogram (64 √ó ~94)\n",
        "        ‚Üì\n",
        "CNN  ‚Üí local timbre + pitch invariance\n",
        "        ‚Üì\n",
        "BiLSTM ‚Üí temporal melody progression\n",
        "        ‚Üì\n",
        "Attention ‚Üí focus on salient moments\n",
        "        ‚Üì\n",
        "Projection ‚Üí fixed 128-D embedding\n",
        "\n",
        "Input tensor\n",
        "(B, 1, F, T) ‚Üí (B, 1, 64, ~94)\n",
        "F = 64 Mel bands ‚Üí perceptual frequency scale\n",
        "T ‚âà 94 frames (3 sec @ hop=1.5 sec)\n",
        "\n",
        "Block 1\n",
        "Conv2d(1 ‚Üí 32, 3√ó3) BatchNorm || ReLU || MaxPool(2√ó2)(improves invariance)\n",
        "\n",
        "Input ‚Üí Output\n",
        "(B, 1, 64, 94) ‚Üí (B, 32, 32, 47)\n",
        "\n",
        "Block 2\n",
        "Conv2d(32 ‚Üí 64, 3√ó3)\n",
        "    BatchNorm\n",
        "    ReLU\n",
        "    MaxPool(2√ó2)\n",
        "    Shape\n",
        "\n",
        "(B, 32, 32, 47)‚Üí (B, 64, 16, 23)\n",
        "\n",
        "\n",
        "Block 3 + Adaptive Pool\n",
        "Conv2d(64 ‚Üí 128)\n",
        "BatchNorm\n",
        "ReLU\n",
        "AdaptiveAvgPool((1, None))\n",
        "\n",
        "\n",
        "Shape\n",
        "\n",
        "(B, 64, 16, 23)\n",
        "‚Üí (B, 128, 1, 23)\n",
        "\n",
        "Reshape for Temporal Modelingx = x.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "Shape\n",
        "(B, 128, 1, 23)\n",
        "‚Üí (B, 23, 128)\n",
        "\n",
        "BiLSTM (Temporal Modeling)\n",
        "LSTM(\n",
        "  input_size=128,\n",
        "  hidden_size=128,\n",
        "  num_layers=2,\n",
        "  bidirectional=True\n",
        ")\n",
        "\n",
        "\n",
        "Output shape\n",
        "\n",
        "(B, 23, 256)\n",
        "\n",
        "\n",
        "(128 forward + 128 backward)\n",
        "\n",
        "Why BiLSTM? Music is not causal:chorus explains verse ,melody context matters ,backward info is important\n",
        "\n",
        "Attention Pooling\n",
        "attn = softmax(W2(tanh(W1(x))))\n",
        "embedding = Œ£(attn_t * x_t)\n",
        "\n",
        "\n",
        "Shape (B, 23, 256)--> (B, 256)\n",
        "\n",
        "Why attention instead of average pooling?\n",
        "Because not all moments matter.\n",
        "Attention learns to emphasize:\n",
        "\n",
        "    hooks\n",
        "    chorus\n",
        "    strong melodic regions\n",
        "\n",
        "Projection Head (Metric Learning)\n",
        "Linear(256 ‚Üí 256)\n",
        "ReLU\n",
        "Dropout(0.3)\n",
        "Linear(256 ‚Üí 128)\n",
        "\n",
        "Why projection?\n",
        "decouples representation learning from embedding geometry\n",
        "improves triplet loss convergence\n",
        "standard practice (SimCLR, CLIP, FaceNet)\n",
        "\"\"\"\n",
        "\n",
        "# --- 2. NEW MODEL: AudioCRNN_Fast (CNN + BiLSTM + Attention) ---\n",
        "class AudioCRNN_Fast(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        #\n",
        "        # -------- 1. FAST CNN BACKBONE --------\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),\n",
        "\n",
        "            # Block 3 + Adaptive Pooling (Freq -> 1)\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, None))\n",
        "        )\n",
        "\n",
        "        # -------- 2. TEMPORAL MODEL (BiLSTM) --------\n",
        "        # Input: 128 channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=128,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        # Output dim: 128 * 2 = 256\n",
        "\n",
        "        # -------- 3. ATTENTION POOLING --------\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(256, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # -------- 4. PROJECTION HEAD --------\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # 1. CNN: (B, 1, F, T) -> (B, 128, 1, T)\n",
        "        x = self.cnn(x)\n",
        "\n",
        "        # 2. Reshape: (B, 128, 1, T) -> (B, T, 128)\n",
        "        x = x.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "        # 3. LSTM\n",
        "        x, _ = self.lstm(x)         # (B, T, 256)\n",
        "\n",
        "        # 4. Attention\n",
        "        attn_weights = self.attention(x)       # (B, T, 1)\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "        x = torch.sum(x * attn_weights, dim=1) # (B, 256)\n",
        "\n",
        "        # 5. Projection\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "    def forward(self, anchor, positive, negative, *args):\n",
        "        return self.forward_one(anchor), self.forward_one(positive), self.forward_one(negative)\n",
        "\n",
        "print(\"‚úÖ Model and Dataset definitions ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyKTAPvKWYer",
        "outputId": "852b2248-ccb6-4388-ec05-27067b4620a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and Dataset definitions ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade torch torchaudio"
      ],
      "metadata": {
        "id": "JPU9yLzwtNcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß± BLOCK 3: FINAL TRAINING LOOP (CRNN + TRIPLET LOSS)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Utility: find latest checkpoint\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_latest_checkpoint(ckpt_dir):\n",
        "    pattern = re.compile(r\"checkpoint_epoch_(\\d+)\\.pth\")\n",
        "    checkpoints = []\n",
        "\n",
        "    for f in os.listdir(ckpt_dir):\n",
        "        match = pattern.match(f)\n",
        "        if match:\n",
        "            checkpoints.append((int(match.group(1)), f))\n",
        "\n",
        "    if not checkpoints:\n",
        "        return None, 0\n",
        "\n",
        "    checkpoints.sort()\n",
        "    epoch, fname = checkpoints[-1]\n",
        "    return os.path.join(ckpt_dir, fname), epoch\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_audio_crnn():\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nüî• Training on: {DEVICE}\")\n",
        "\n",
        "    # ---------------- CONFIG ----------------\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    LR = 3e-4\n",
        "    MARGIN = 0.75\n",
        "    PATIENCE = 6\n",
        "    GRAD_CLIP = 1.0\n",
        "\n",
        "    # ---------------- OUTPUT DIRS ----------------\n",
        "    BASE_DRIVE_DIR = \"/content/drive/MyDrive/FIND_TUNE\"\n",
        "    MODEL_DIR = os.path.join(BASE_DRIVE_DIR, \"spectrogram_based_model\")\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    BEST_MODEL_PATH = os.path.join(\n",
        "        MODEL_DIR, \"best_spectrogram_model.pth\"\n",
        "    )\n",
        "\n",
        "    print(f\"üìÇ Models will be saved to: {MODEL_DIR}\")\n",
        "\n",
        "    # ---------------- DATASET ----------------\n",
        "    if \"VALID_ANCHORS\" not in globals() or not VALID_ANCHORS:\n",
        "        print(\"‚ùå VALID_ANCHORS not found. Run CSV mapping first.\")\n",
        "        return\n",
        "\n",
        "    dataset = DualObjectiveSiameseDataset(\n",
        "        anchor_list=VALID_ANCHORS,\n",
        "        pair_map=PAIR_MAP,\n",
        "        originals_dir=\"/content/data/originals\",\n",
        "        covers_dir=\"/content/data/covers\",\n",
        "        noise_dir=\"/content/data/noise_16k\",\n",
        "        sample_rate=16000,\n",
        "        duration=3.0,\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # ---------------- MODEL ----------------\n",
        "    model = AudioCRNN_Fast(embed_dim=128).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"min\",\n",
        "        factor=0.5,\n",
        "        patience=PATIENCE,\n",
        "        min_lr=1e-7,\n",
        "    )\n",
        "\n",
        "    # ---------------- RESUME LOGIC ----------------\n",
        "    start_epoch = 0\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    latest_ckpt, last_epoch = get_latest_checkpoint(MODEL_DIR)\n",
        "\n",
        "    if latest_ckpt:\n",
        "        print(f\"üîÑ Resuming from {latest_ckpt}\")\n",
        "        ckpt = torch.load(latest_ckpt, map_location=DEVICE)\n",
        "\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
        "\n",
        "        start_epoch = last_epoch + 1\n",
        "        best_loss = ckpt.get(\"best_loss\", float(\"inf\"))\n",
        "\n",
        "        print(f\"   Resumed at epoch {start_epoch}, best loss {best_loss:.4f}\")\n",
        "    else:\n",
        "        print(\"üÜï No checkpoint found. Starting fresh training.\")\n",
        "\n",
        "    # ---------------- TRAIN ----------------\n",
        "    print(\"üöÄ Starting CRNN Training...\")\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (anc, pos, neg) in enumerate(dataloader):\n",
        "            anc = anc.to(DEVICE, non_blocking=True)\n",
        "            pos = pos.to(DEVICE, non_blocking=True)\n",
        "            neg = neg.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                emb_a, emb_p, emb_n = model(anc, pos, neg)\n",
        "                loss = criterion(emb_a, emb_p, emb_n)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                print(\n",
        "                    f\"   Epoch {epoch+1} | Batch {batch_idx}/{len(dataloader)} \"\n",
        "                    f\"| Loss: {loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"\\nüì¢ Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # ---- Scheduler ----\n",
        "        old_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        scheduler.step(avg_loss)\n",
        "        new_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        if new_lr != old_lr:\n",
        "            print(f\"üìâ LR reduced: {old_lr:.2e} ‚Üí {new_lr:.2e}\")\n",
        "\n",
        "        # ---- Save Best ----\n",
        "        if avg_loss < best_loss:\n",
        "            print(f\"‚≠ê New BEST: {best_loss:.4f} ‚Üí {avg_loss:.4f}\")\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "\n",
        "        # ---- Epoch Checkpoint ----\n",
        "        ckpt_path = os.path.join(\n",
        "            MODEL_DIR, f\"checkpoint_epoch_{epoch}.pth\"\n",
        "        )\n",
        "\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "                \"best_loss\": best_loss,\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "\n",
        "        print(f\"üíæ Saved checkpoint: {ckpt_path}\\n\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Entry Point\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    train_audio_crnn()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Om634cifWYgP",
        "outputId": "29a53f8b-764b-4a4e-9570-be3a50f26dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî• Training on: cuda\n",
            "üìÇ Models will be saved to: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model\n",
            "üÜï No checkpoint found. Starting fresh training.\n",
            "üöÄ Starting CRNN Training...\n",
            "   Epoch 1 | Batch 0/30 | Loss: 0.7609\n",
            "   Epoch 1 | Batch 20/30 | Loss: 0.7268\n",
            "\n",
            "üì¢ Epoch 1/100 | Avg Loss: 0.7455\n",
            "‚≠ê New BEST: inf ‚Üí 0.7455\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_0.pth\n",
            "\n",
            "   Epoch 2 | Batch 0/30 | Loss: 0.7438\n",
            "   Epoch 2 | Batch 20/30 | Loss: 0.6903\n",
            "\n",
            "üì¢ Epoch 2/100 | Avg Loss: 0.7092\n",
            "‚≠ê New BEST: 0.7455 ‚Üí 0.7092\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_1.pth\n",
            "\n",
            "   Epoch 3 | Batch 0/30 | Loss: 0.6619\n",
            "   Epoch 3 | Batch 20/30 | Loss: 0.5423\n",
            "\n",
            "üì¢ Epoch 3/100 | Avg Loss: 0.5830\n",
            "‚≠ê New BEST: 0.7092 ‚Üí 0.5830\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_2.pth\n",
            "\n",
            "   Epoch 4 | Batch 0/30 | Loss: 0.5905\n",
            "   Epoch 4 | Batch 20/30 | Loss: 0.5192\n",
            "\n",
            "üì¢ Epoch 4/100 | Avg Loss: 0.4823\n",
            "‚≠ê New BEST: 0.5830 ‚Üí 0.4823\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_3.pth\n",
            "\n",
            "   Epoch 5 | Batch 0/30 | Loss: 0.4225\n",
            "   Epoch 5 | Batch 20/30 | Loss: 0.5028\n",
            "\n",
            "üì¢ Epoch 5/100 | Avg Loss: 0.4227\n",
            "‚≠ê New BEST: 0.4823 ‚Üí 0.4227\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_4.pth\n",
            "\n",
            "   Epoch 6 | Batch 0/30 | Loss: 0.3834\n",
            "   Epoch 6 | Batch 20/30 | Loss: 0.4334\n",
            "\n",
            "üì¢ Epoch 6/100 | Avg Loss: 0.3842\n",
            "‚≠ê New BEST: 0.4227 ‚Üí 0.3842\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_5.pth\n",
            "\n",
            "   Epoch 7 | Batch 0/30 | Loss: 0.3612\n",
            "   Epoch 7 | Batch 20/30 | Loss: 0.4050\n",
            "\n",
            "üì¢ Epoch 7/100 | Avg Loss: 0.3651\n",
            "‚≠ê New BEST: 0.3842 ‚Üí 0.3651\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_6.pth\n",
            "\n",
            "   Epoch 8 | Batch 0/30 | Loss: 0.3397\n",
            "   Epoch 8 | Batch 20/30 | Loss: 0.3051\n",
            "\n",
            "üì¢ Epoch 8/100 | Avg Loss: 0.3639\n",
            "‚≠ê New BEST: 0.3651 ‚Üí 0.3639\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_7.pth\n",
            "\n",
            "   Epoch 9 | Batch 0/30 | Loss: 0.3671\n",
            "   Epoch 9 | Batch 20/30 | Loss: 0.4056\n",
            "\n",
            "üì¢ Epoch 9/100 | Avg Loss: 0.3611\n",
            "‚≠ê New BEST: 0.3639 ‚Üí 0.3611\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_8.pth\n",
            "\n",
            "   Epoch 10 | Batch 0/30 | Loss: 0.3791\n",
            "   Epoch 10 | Batch 20/30 | Loss: 0.3388\n",
            "\n",
            "üì¢ Epoch 10/100 | Avg Loss: 0.3654\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_9.pth\n",
            "\n",
            "   Epoch 11 | Batch 0/30 | Loss: 0.3840\n",
            "   Epoch 11 | Batch 20/30 | Loss: 0.3750\n",
            "\n",
            "üì¢ Epoch 11/100 | Avg Loss: 0.3728\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_10.pth\n",
            "\n",
            "   Epoch 12 | Batch 0/30 | Loss: 0.3550\n",
            "   Epoch 12 | Batch 20/30 | Loss: 0.3574\n",
            "\n",
            "üì¢ Epoch 12/100 | Avg Loss: 0.3577\n",
            "‚≠ê New BEST: 0.3611 ‚Üí 0.3577\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_11.pth\n",
            "\n",
            "   Epoch 13 | Batch 0/30 | Loss: 0.3438\n",
            "   Epoch 13 | Batch 20/30 | Loss: 0.3843\n",
            "\n",
            "üì¢ Epoch 13/100 | Avg Loss: 0.3560\n",
            "‚≠ê New BEST: 0.3577 ‚Üí 0.3560\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_12.pth\n",
            "\n",
            "   Epoch 14 | Batch 0/30 | Loss: 0.3564\n",
            "   Epoch 14 | Batch 20/30 | Loss: 0.3595\n",
            "\n",
            "üì¢ Epoch 14/100 | Avg Loss: 0.3652\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_13.pth\n",
            "\n",
            "   Epoch 15 | Batch 0/30 | Loss: 0.2756\n",
            "   Epoch 15 | Batch 20/30 | Loss: 0.3939\n",
            "\n",
            "üì¢ Epoch 15/100 | Avg Loss: 0.3551\n",
            "‚≠ê New BEST: 0.3560 ‚Üí 0.3551\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_14.pth\n",
            "\n",
            "   Epoch 16 | Batch 0/30 | Loss: 0.4070\n",
            "   Epoch 16 | Batch 20/30 | Loss: 0.3142\n",
            "\n",
            "üì¢ Epoch 16/100 | Avg Loss: 0.3524\n",
            "‚≠ê New BEST: 0.3551 ‚Üí 0.3524\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_15.pth\n",
            "\n",
            "   Epoch 17 | Batch 0/30 | Loss: 0.3816\n",
            "   Epoch 17 | Batch 20/30 | Loss: 0.3783\n",
            "\n",
            "üì¢ Epoch 17/100 | Avg Loss: 0.3618\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_16.pth\n",
            "\n",
            "   Epoch 18 | Batch 0/30 | Loss: 0.3079\n",
            "   Epoch 18 | Batch 20/30 | Loss: 0.3680\n",
            "\n",
            "üì¢ Epoch 18/100 | Avg Loss: 0.3537\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_17.pth\n",
            "\n",
            "   Epoch 19 | Batch 0/30 | Loss: 0.3235\n",
            "   Epoch 19 | Batch 20/30 | Loss: 0.3355\n",
            "\n",
            "üì¢ Epoch 19/100 | Avg Loss: 0.3569\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_18.pth\n",
            "\n",
            "   Epoch 20 | Batch 0/30 | Loss: 0.2985\n",
            "   Epoch 20 | Batch 20/30 | Loss: 0.3276\n",
            "\n",
            "üì¢ Epoch 20/100 | Avg Loss: 0.3617\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_19.pth\n",
            "\n",
            "   Epoch 21 | Batch 0/30 | Loss: 0.3685\n",
            "   Epoch 21 | Batch 20/30 | Loss: 0.3132\n",
            "\n",
            "üì¢ Epoch 21/100 | Avg Loss: 0.3612\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_20.pth\n",
            "\n",
            "   Epoch 22 | Batch 0/30 | Loss: 0.3338\n",
            "   Epoch 22 | Batch 20/30 | Loss: 0.3798\n",
            "\n",
            "üì¢ Epoch 22/100 | Avg Loss: 0.3527\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_21.pth\n",
            "\n",
            "   Epoch 23 | Batch 0/30 | Loss: 0.3549\n",
            "   Epoch 23 | Batch 20/30 | Loss: 0.3710\n",
            "\n",
            "üì¢ Epoch 23/100 | Avg Loss: 0.3675\n",
            "üìâ LR reduced: 3.00e-04 ‚Üí 1.50e-04\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_22.pth\n",
            "\n",
            "   Epoch 24 | Batch 0/30 | Loss: 0.3798\n",
            "   Epoch 24 | Batch 20/30 | Loss: 0.3485\n",
            "\n",
            "üì¢ Epoch 24/100 | Avg Loss: 0.3678\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_23.pth\n",
            "\n",
            "   Epoch 25 | Batch 0/30 | Loss: 0.3247\n",
            "   Epoch 25 | Batch 20/30 | Loss: 0.3647\n",
            "\n",
            "üì¢ Epoch 25/100 | Avg Loss: 0.3617\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_24.pth\n",
            "\n",
            "   Epoch 26 | Batch 0/30 | Loss: 0.3138\n",
            "   Epoch 26 | Batch 20/30 | Loss: 0.3509\n",
            "\n",
            "üì¢ Epoch 26/100 | Avg Loss: 0.3557\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_25.pth\n",
            "\n",
            "   Epoch 27 | Batch 0/30 | Loss: 0.3626\n",
            "   Epoch 27 | Batch 20/30 | Loss: 0.4011\n",
            "\n",
            "üì¢ Epoch 27/100 | Avg Loss: 0.3668\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_26.pth\n",
            "\n",
            "   Epoch 28 | Batch 0/30 | Loss: 0.4182\n",
            "   Epoch 28 | Batch 20/30 | Loss: 0.4065\n",
            "\n",
            "üì¢ Epoch 28/100 | Avg Loss: 0.3603\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_27.pth\n",
            "\n",
            "   Epoch 29 | Batch 0/30 | Loss: 0.3816\n",
            "   Epoch 29 | Batch 20/30 | Loss: 0.3699\n",
            "\n",
            "üì¢ Epoch 29/100 | Avg Loss: 0.3595\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_28.pth\n",
            "\n",
            "   Epoch 30 | Batch 0/30 | Loss: 0.3851\n",
            "   Epoch 30 | Batch 20/30 | Loss: 0.4017\n",
            "\n",
            "üì¢ Epoch 30/100 | Avg Loss: 0.3585\n",
            "üìâ LR reduced: 1.50e-04 ‚Üí 7.50e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_29.pth\n",
            "\n",
            "   Epoch 31 | Batch 0/30 | Loss: 0.3384\n",
            "   Epoch 31 | Batch 20/30 | Loss: 0.3580\n",
            "\n",
            "üì¢ Epoch 31/100 | Avg Loss: 0.3566\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_30.pth\n",
            "\n",
            "   Epoch 32 | Batch 0/30 | Loss: 0.3554\n",
            "   Epoch 32 | Batch 20/30 | Loss: 0.3175\n",
            "\n",
            "üì¢ Epoch 32/100 | Avg Loss: 0.3573\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_31.pth\n",
            "\n",
            "   Epoch 33 | Batch 0/30 | Loss: 0.3586\n",
            "   Epoch 33 | Batch 20/30 | Loss: 0.3281\n",
            "\n",
            "üì¢ Epoch 33/100 | Avg Loss: 0.3502\n",
            "‚≠ê New BEST: 0.3524 ‚Üí 0.3502\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_32.pth\n",
            "\n",
            "   Epoch 34 | Batch 0/30 | Loss: 0.3514\n",
            "   Epoch 34 | Batch 20/30 | Loss: 0.3939\n",
            "\n",
            "üì¢ Epoch 34/100 | Avg Loss: 0.3545\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_33.pth\n",
            "\n",
            "   Epoch 35 | Batch 0/30 | Loss: 0.3588\n",
            "   Epoch 35 | Batch 20/30 | Loss: 0.3568\n",
            "\n",
            "üì¢ Epoch 35/100 | Avg Loss: 0.3600\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_34.pth\n",
            "\n",
            "   Epoch 36 | Batch 0/30 | Loss: 0.3518\n",
            "   Epoch 36 | Batch 20/30 | Loss: 0.3485\n",
            "\n",
            "üì¢ Epoch 36/100 | Avg Loss: 0.3473\n",
            "‚≠ê New BEST: 0.3502 ‚Üí 0.3473\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_35.pth\n",
            "\n",
            "   Epoch 37 | Batch 0/30 | Loss: 0.3475\n",
            "   Epoch 37 | Batch 20/30 | Loss: 0.3875\n",
            "\n",
            "üì¢ Epoch 37/100 | Avg Loss: 0.3460\n",
            "‚≠ê New BEST: 0.3473 ‚Üí 0.3460\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_36.pth\n",
            "\n",
            "   Epoch 38 | Batch 0/30 | Loss: 0.3482\n",
            "   Epoch 38 | Batch 20/30 | Loss: 0.3634\n",
            "\n",
            "üì¢ Epoch 38/100 | Avg Loss: 0.3474\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_37.pth\n",
            "\n",
            "   Epoch 39 | Batch 0/30 | Loss: 0.3336\n",
            "   Epoch 39 | Batch 20/30 | Loss: 0.3456\n",
            "\n",
            "üì¢ Epoch 39/100 | Avg Loss: 0.3551\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_38.pth\n",
            "\n",
            "   Epoch 40 | Batch 0/30 | Loss: 0.3435\n",
            "   Epoch 40 | Batch 20/30 | Loss: 0.3061\n",
            "\n",
            "üì¢ Epoch 40/100 | Avg Loss: 0.3518\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_39.pth\n",
            "\n",
            "   Epoch 41 | Batch 0/30 | Loss: 0.3770\n",
            "   Epoch 41 | Batch 20/30 | Loss: 0.3903\n",
            "\n",
            "üì¢ Epoch 41/100 | Avg Loss: 0.3512\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_40.pth\n",
            "\n",
            "   Epoch 42 | Batch 0/30 | Loss: 0.3957\n",
            "   Epoch 42 | Batch 20/30 | Loss: 0.3541\n",
            "\n",
            "üì¢ Epoch 42/100 | Avg Loss: 0.3511\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_41.pth\n",
            "\n",
            "   Epoch 43 | Batch 0/30 | Loss: 0.3658\n",
            "   Epoch 43 | Batch 20/30 | Loss: 0.3333\n",
            "\n",
            "üì¢ Epoch 43/100 | Avg Loss: 0.3644\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_42.pth\n",
            "\n",
            "   Epoch 44 | Batch 0/30 | Loss: 0.3172\n",
            "   Epoch 44 | Batch 20/30 | Loss: 0.3667\n",
            "\n",
            "üì¢ Epoch 44/100 | Avg Loss: 0.3430\n",
            "‚≠ê New BEST: 0.3460 ‚Üí 0.3430\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_43.pth\n",
            "\n",
            "   Epoch 45 | Batch 0/30 | Loss: 0.3785\n",
            "   Epoch 45 | Batch 20/30 | Loss: 0.3709\n",
            "\n",
            "üì¢ Epoch 45/100 | Avg Loss: 0.3625\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_44.pth\n",
            "\n",
            "   Epoch 46 | Batch 0/30 | Loss: 0.3513\n",
            "   Epoch 46 | Batch 20/30 | Loss: 0.3867\n",
            "\n",
            "üì¢ Epoch 46/100 | Avg Loss: 0.3540\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_45.pth\n",
            "\n",
            "   Epoch 47 | Batch 0/30 | Loss: 0.3415\n",
            "   Epoch 47 | Batch 20/30 | Loss: 0.3749\n",
            "\n",
            "üì¢ Epoch 47/100 | Avg Loss: 0.3592\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_46.pth\n",
            "\n",
            "   Epoch 48 | Batch 0/30 | Loss: 0.3536\n",
            "   Epoch 48 | Batch 20/30 | Loss: 0.3558\n",
            "\n",
            "üì¢ Epoch 48/100 | Avg Loss: 0.3507\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_47.pth\n",
            "\n",
            "   Epoch 49 | Batch 0/30 | Loss: 0.3244\n",
            "   Epoch 49 | Batch 20/30 | Loss: 0.3761\n",
            "\n",
            "üì¢ Epoch 49/100 | Avg Loss: 0.3464\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_48.pth\n",
            "\n",
            "   Epoch 50 | Batch 0/30 | Loss: 0.4100\n",
            "   Epoch 50 | Batch 20/30 | Loss: 0.3618\n",
            "\n",
            "üì¢ Epoch 50/100 | Avg Loss: 0.3652\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_49.pth\n",
            "\n",
            "   Epoch 51 | Batch 0/30 | Loss: 0.3465\n",
            "   Epoch 51 | Batch 20/30 | Loss: 0.3853\n",
            "\n",
            "üì¢ Epoch 51/100 | Avg Loss: 0.3549\n",
            "üìâ LR reduced: 7.50e-05 ‚Üí 3.75e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_50.pth\n",
            "\n",
            "   Epoch 52 | Batch 0/30 | Loss: 0.3456\n",
            "   Epoch 52 | Batch 20/30 | Loss: 0.3153\n",
            "\n",
            "üì¢ Epoch 52/100 | Avg Loss: 0.3556\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_51.pth\n",
            "\n",
            "   Epoch 53 | Batch 0/30 | Loss: 0.3683\n",
            "   Epoch 53 | Batch 20/30 | Loss: 0.3599\n",
            "\n",
            "üì¢ Epoch 53/100 | Avg Loss: 0.3573\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_52.pth\n",
            "\n",
            "   Epoch 54 | Batch 0/30 | Loss: 0.3553\n",
            "   Epoch 54 | Batch 20/30 | Loss: 0.3063\n",
            "\n",
            "üì¢ Epoch 54/100 | Avg Loss: 0.3556\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_53.pth\n",
            "\n",
            "   Epoch 55 | Batch 0/30 | Loss: 0.3256\n",
            "   Epoch 55 | Batch 20/30 | Loss: 0.3733\n",
            "\n",
            "üì¢ Epoch 55/100 | Avg Loss: 0.3474\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_54.pth\n",
            "\n",
            "   Epoch 56 | Batch 0/30 | Loss: 0.3264\n",
            "   Epoch 56 | Batch 20/30 | Loss: 0.3207\n",
            "\n",
            "üì¢ Epoch 56/100 | Avg Loss: 0.3488\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_55.pth\n",
            "\n",
            "   Epoch 57 | Batch 0/30 | Loss: 0.3103\n",
            "   Epoch 57 | Batch 20/30 | Loss: 0.4080\n",
            "\n",
            "üì¢ Epoch 57/100 | Avg Loss: 0.3503\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_56.pth\n",
            "\n",
            "   Epoch 58 | Batch 0/30 | Loss: 0.3578\n",
            "   Epoch 58 | Batch 20/30 | Loss: 0.3681\n",
            "\n",
            "üì¢ Epoch 58/100 | Avg Loss: 0.3619\n",
            "üìâ LR reduced: 3.75e-05 ‚Üí 1.87e-05\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_57.pth\n",
            "\n",
            "   Epoch 59 | Batch 0/30 | Loss: 0.2989\n",
            "   Epoch 59 | Batch 20/30 | Loss: 0.3271\n",
            "\n",
            "üì¢ Epoch 59/100 | Avg Loss: 0.3463\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_58.pth\n",
            "\n",
            "   Epoch 60 | Batch 0/30 | Loss: 0.3471\n",
            "   Epoch 60 | Batch 20/30 | Loss: 0.3751\n",
            "\n",
            "üì¢ Epoch 60/100 | Avg Loss: 0.3468\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_59.pth\n",
            "\n",
            "   Epoch 61 | Batch 0/30 | Loss: 0.3805\n",
            "   Epoch 61 | Batch 20/30 | Loss: 0.3771\n",
            "\n",
            "üì¢ Epoch 61/100 | Avg Loss: 0.3592\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_60.pth\n",
            "\n",
            "   Epoch 62 | Batch 0/30 | Loss: 0.3242\n",
            "   Epoch 62 | Batch 20/30 | Loss: 0.3879\n",
            "\n",
            "üì¢ Epoch 62/100 | Avg Loss: 0.3532\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_61.pth\n",
            "\n",
            "   Epoch 63 | Batch 0/30 | Loss: 0.3624\n",
            "   Epoch 63 | Batch 20/30 | Loss: 0.3626\n",
            "\n",
            "üì¢ Epoch 63/100 | Avg Loss: 0.3519\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_62.pth\n",
            "\n",
            "   Epoch 64 | Batch 0/30 | Loss: 0.3809\n",
            "   Epoch 64 | Batch 20/30 | Loss: 0.3751\n",
            "\n",
            "üì¢ Epoch 64/100 | Avg Loss: 0.3569\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_63.pth\n",
            "\n",
            "   Epoch 65 | Batch 0/30 | Loss: 0.3323\n",
            "   Epoch 65 | Batch 20/30 | Loss: 0.4001\n",
            "\n",
            "üì¢ Epoch 65/100 | Avg Loss: 0.3518\n",
            "üìâ LR reduced: 1.87e-05 ‚Üí 9.37e-06\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_64.pth\n",
            "\n",
            "   Epoch 66 | Batch 0/30 | Loss: 0.3283\n",
            "   Epoch 66 | Batch 20/30 | Loss: 0.3514\n",
            "\n",
            "üì¢ Epoch 66/100 | Avg Loss: 0.3555\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_65.pth\n",
            "\n",
            "   Epoch 67 | Batch 0/30 | Loss: 0.3133\n",
            "   Epoch 67 | Batch 20/30 | Loss: 0.3704\n",
            "\n",
            "üì¢ Epoch 67/100 | Avg Loss: 0.3580\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_66.pth\n",
            "\n",
            "   Epoch 68 | Batch 0/30 | Loss: 0.3284\n",
            "   Epoch 68 | Batch 20/30 | Loss: 0.4171\n",
            "\n",
            "üì¢ Epoch 68/100 | Avg Loss: 0.3456\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_67.pth\n",
            "\n",
            "   Epoch 69 | Batch 0/30 | Loss: 0.3650\n",
            "   Epoch 69 | Batch 20/30 | Loss: 0.3403\n",
            "\n",
            "üì¢ Epoch 69/100 | Avg Loss: 0.3495\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_68.pth\n",
            "\n",
            "   Epoch 70 | Batch 0/30 | Loss: 0.2908\n",
            "   Epoch 70 | Batch 20/30 | Loss: 0.3370\n",
            "\n",
            "üì¢ Epoch 70/100 | Avg Loss: 0.3534\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_69.pth\n",
            "\n",
            "   Epoch 71 | Batch 0/30 | Loss: 0.3601\n",
            "   Epoch 71 | Batch 20/30 | Loss: 0.3277\n",
            "\n",
            "üì¢ Epoch 71/100 | Avg Loss: 0.3521\n",
            "üíæ Saved checkpoint: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/checkpoint_epoch_70.pth\n",
            "\n",
            "   Epoch 72 | Batch 0/30 | Loss: 0.3414\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4196386509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtrain_audio_crnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4196386509.py\u001b[0m in \u001b[0;36mtrain_audio_crnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0manc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;31m#               msg, ancdata, flags, addr = server.recvmsg(1, socket.CMSG_SPACE(a.itemsize))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;31m#               assert(len(ancdata) == 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m     \u001b[0;31m#               cmsg_level, cmsg_type, cmsg_data = ancdata[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0;31m#               a.frombytes(cmsg_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;31m#               print(\"Received fd \", a[0], \" (iteration #\", i, \")\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m# Decremented when that data has been given to the main thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;31m# Each worker should have at most self._prefetch_factor tasks outstanding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers_num_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m         \u001b[0;31m# Reset the worker queue cycle so it resumes next epoch at worker 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_queue_idx_cycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVAL"
      ],
      "metadata": {
        "id": "uSPhHaHbqVQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall the broken decoder\n",
        "!pip uninstall torchcodec -y\n",
        "\n",
        "# 2. Install the stable decoder (libsndfile based)\n",
        "!pip install soundfile\n",
        "\n",
        "# 3. Re-install torchaudio to ensure clean bindings\n",
        "!pip install --upgrade torchaudio"
      ],
      "metadata": {
        "id": "kC5Uv5Q-Chhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install soundfile torchcodec\n"
      ],
      "metadata": {
        "id": "dh4OxyAm_-Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERmUBcjiB0ZL",
        "outputId": "849c12f2-9eb7-40a9-bade-46e1a4593342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 1. Grab the first file\n",
        "files = glob.glob(\"/content/data/eval/*.wav\")\n",
        "if not files:\n",
        "    raise ValueError(\"‚ùå No files found in directory!\")\n",
        "\n",
        "test_file = files[0]\n",
        "print(f\"üßê Inspecting: {test_file}\")\n",
        "print(f\"   Size: {os.path.getsize(test_file)} bytes\")\n",
        "\n",
        "# 2. Try to load it EXACTLY like the dataset does (no try/except)\n",
        "print(\"\\nüí• Attempting to load with backend='soundfile'...\")\n",
        "try:\n",
        "    wav, sr = torchaudio.load(test_file, backend=\"soundfile\")\n",
        "    print(f\"‚úÖ Success! Shape: {wav.shape}, SR: {sr}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå CRITICAL FAILURE (This is why your DB is empty):\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "\n",
        "    # Heuristic Help\n",
        "    if \"backend\" in str(e).lower():\n",
        "        print(\"\\nüí° HINT: You did not restart the runtime after 'pip install soundfile'.\")\n",
        "        print(\"   Action: Runtime > Restart Session\")\n",
        "    elif \"header\" in str(e).lower():\n",
        "        print(\"\\nüí° HINT: The file is likely corrupted or 0 bytes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49jSEZkmDuU3",
        "outputId": "773cea24-fe71-4960-c9e6-909aefc3a006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßê Inspecting: /content/data/eval/6c39a423-0487-443f-8d71-a6c92b29760b.wav\n",
            "   Size: 6038348 bytes\n",
            "\n",
            "üí• Attempting to load with backend='soundfile'...\n",
            "\n",
            "‚ùå CRITICAL FAILURE (This is why your DB is empty):\n",
            "Error Type: ImportError\n",
            "Error Message: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß™ BLOCK 6: V3 EVALUATION SYSTEM (MEMORY SAFE + SOUNDFILE)\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import os, glob, random, math\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from audiomentations import Compose, AddBackgroundNoise, PitchShift, TimeStretch, Gain\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EVAL_DIR = \"/content/data/eval\"\n",
        "NOISE_DIR = \"/content/data/noise_16k\"\n",
        "\n",
        "MODEL_BASE_DIR = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model\"\n",
        "MODEL_PATH = os.path.join(MODEL_BASE_DIR, \"best_spectrogram_model.pth\")\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "WIN_SEC = 3.0\n",
        "HOP_SEC = 1.5\n",
        "QUERY_LEN = 15\n",
        "\n",
        "TOLERANCE = 1.5\n",
        "SIGMA = 0.5\n",
        "SPREAD_FACTOR = 0.3\n",
        "INFERENCE_BATCH_SIZE = 32  # üîë NEW: Process song in chunks to save RAM\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. MODEL ARCHITECTURE\n",
        "# ------------------------------------------------------------------------------\n",
        "class AudioCRNN_Fast(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d((1, None))\n",
        "        )\n",
        "        self.lstm = nn.LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Sequential(nn.Linear(256, 64), nn.Tanh(), nn.Linear(64, 1))\n",
        "        self.fc = nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, embed_dim))\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.squeeze(2).permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "        attn = F.softmax(self.attention(x), dim=1)\n",
        "        x = torch.sum(x * attn, dim=1)\n",
        "        return F.normalize(self.fc(x), p=2, dim=1)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. AUGMENTATION\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_augmenter(level):\n",
        "    if level == \"soft\":\n",
        "        return Compose([\n",
        "            Gain(min_gain_db=-3, max_gain_db=3, p=0.5),\n",
        "            PitchShift(min_semitones=-1, max_semitones=1, p=0.3)\n",
        "        ])\n",
        "    if level == \"hard\":\n",
        "        return Compose([\n",
        "            AddBackgroundNoise(NOISE_DIR, min_snr_db=5, max_snr_db=15, p=1.0),\n",
        "            PitchShift(min_semitones=-2, max_semitones=2, p=0.8),\n",
        "            TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5)\n",
        "        ])\n",
        "    return None\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. HELPER: DIRECT LOADING\n",
        "# ------------------------------------------------------------------------------\n",
        "def robust_load(path, target_sr=16000):\n",
        "    try:\n",
        "        wav_np, sr = sf.read(path)\n",
        "        wav_np = wav_np.astype(np.float32)\n",
        "        wav = torch.from_numpy(wav_np)\n",
        "        if wav.ndim == 1:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        else:\n",
        "            wav = wav.t()\n",
        "        return wav, sr\n",
        "    except Exception as e:\n",
        "        return None, 0\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. AUDIO ‚Üí EMBEDDINGS (Memory Safe)\n",
        "# ------------------------------------------------------------------------------\n",
        "mel = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
        ").to(DEVICE)\n",
        "db = torchaudio.transforms.AmplitudeToDB().to(DEVICE)\n",
        "\n",
        "def audio_to_embedding(model, wav):\n",
        "    # üîë INPUT: wav is on CPU (1, T)\n",
        "    samples_win = int(SAMPLE_RATE * WIN_SEC)\n",
        "    samples_hop = int(SAMPLE_RATE * HOP_SEC)\n",
        "\n",
        "    if wav.shape[1] < samples_win:\n",
        "        wav = F.pad(wav, (0, samples_win - wav.shape[1]))\n",
        "\n",
        "    # Slice on CPU to avoid GPU OOM\n",
        "    windows = []\n",
        "    times = []\n",
        "\n",
        "    for i in range(0, wav.shape[1] - samples_win + 1, samples_hop):\n",
        "        windows.append(wav[:, i:i + samples_win])\n",
        "        times.append(i / SAMPLE_RATE)\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    # üîë PROCESS IN MINI-BATCHES\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(windows), INFERENCE_BATCH_SIZE):\n",
        "        # Create mini-batch\n",
        "        chunk = windows[i : i + INFERENCE_BATCH_SIZE]\n",
        "        batch = torch.stack(chunk).to(DEVICE) # Move only 32 items to GPU\n",
        "\n",
        "        # Transform\n",
        "        spec = mel(batch)\n",
        "        spec = db(spec)\n",
        "        spec = (spec - spec.mean(dim=(2,3), keepdim=True)) / (spec.std(dim=(2,3), keepdim=True) + 1e-6)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            emb = model.forward_one(spec)\n",
        "\n",
        "        # Move back to CPU immediately\n",
        "        all_embeddings.append(emb.cpu())\n",
        "\n",
        "    # Concatenate all CPU results\n",
        "    return torch.cat(all_embeddings), times\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. DATABASE BUILDER\n",
        "# ------------------------------------------------------------------------------\n",
        "class ReferenceDatabaseDataset(Dataset):\n",
        "    def __init__(self, file_paths, sample_rate):\n",
        "        self.file_paths = file_paths\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        name = os.path.basename(path)\n",
        "\n",
        "        wav, sr = robust_load(path, self.sample_rate)\n",
        "\n",
        "        if wav is None:\n",
        "            return torch.zeros(1, self.sample_rate), \"ERROR\"\n",
        "\n",
        "        if sr != self.sample_rate:\n",
        "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(0, keepdim=True)\n",
        "\n",
        "        return wav, name\n",
        "\n",
        "def build_database(model):\n",
        "    # üßπ CLEANUP BEFORE STARTING\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"üèóÔ∏è Building vector database (Memory Safe)...\")\n",
        "    files = glob.glob(os.path.join(EVAL_DIR, \"*.wav\"))\n",
        "    if not files:\n",
        "        print(\"‚ùå No .wav files found\")\n",
        "        return None, None\n",
        "\n",
        "    dataset = ReferenceDatabaseDataset(files, SAMPLE_RATE)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    vectors = []\n",
        "    metadata = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for wav, name in tqdm(loader):\n",
        "            if name[0] == \"ERROR\": continue\n",
        "\n",
        "            # üîë KEEP WAV ON CPU HERE\n",
        "            wav = wav.squeeze(0)\n",
        "\n",
        "            emb, times = audio_to_embedding(model, wav)\n",
        "            if emb is None: continue\n",
        "\n",
        "            for i, t in enumerate(times):\n",
        "                vectors.append(emb[i]) # Already on CPU\n",
        "                metadata.append({\"name\": name[0], \"offset\": t})\n",
        "\n",
        "    if not vectors:\n",
        "        print(\"‚ùå DB Build Failed.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"‚úÖ Indexed {len(metadata)} segments.\")\n",
        "    return torch.stack(vectors).to(DEVICE), metadata\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. SCORING & EVAL LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_v3_scores(matches):\n",
        "    song_scores = defaultdict(lambda: defaultdict(float))\n",
        "    for dist, meta, q_t in matches:\n",
        "        w = math.exp(-(dist ** 2) / (2 * SIGMA ** 2))\n",
        "        if w < 0.01: continue\n",
        "        delta = meta[\"offset\"] - q_t\n",
        "        b = int(round(delta / TOLERANCE))\n",
        "        song_scores[meta[\"name\"]][b] += w\n",
        "        song_scores[meta[\"name\"]][b-1] += w * SPREAD_FACTOR\n",
        "        song_scores[meta[\"name\"]][b+1] += w * SPREAD_FACTOR\n",
        "\n",
        "    ranked = []\n",
        "    for song, buckets in song_scores.items():\n",
        "        ranked.append((song, max(buckets.values())))\n",
        "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
        "    return ranked\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. SCORING & EVAL LOOP (FIXED DEVICE MISMATCH)\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_evaluation(model, db_vecs, db_meta, trials=100):\n",
        "    if db_vecs is None: return\n",
        "\n",
        "    # Ensure Database is on the correct device\n",
        "    db_vecs = db_vecs.to(DEVICE)\n",
        "\n",
        "    modes = [\"clean\", \"soft\", \"hard\"]\n",
        "    results = {m: {\"top1\":0,\"top5\":0,\"top10\":0} for m in modes}\n",
        "    songs = list(set(m[\"name\"] for m in db_meta))\n",
        "\n",
        "    print(f\"\\n‚ö° Starting Evaluation ({trials} trials per mode)...\")\n",
        "\n",
        "    for mode in modes:\n",
        "        print(f\"\\n‚ñ∂ MODE: {mode.upper()}\")\n",
        "        aug = get_augmenter(mode)\n",
        "\n",
        "        for _ in tqdm(range(trials)):\n",
        "            target = random.choice(songs)\n",
        "\n",
        "            # Load Audio\n",
        "            wav, sr = robust_load(os.path.join(EVAL_DIR, target))\n",
        "            if wav is None: continue\n",
        "\n",
        "            # Resample & Mix\n",
        "            if sr != SAMPLE_RATE:\n",
        "                wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wav)\n",
        "            if wav.shape[0] > 1:\n",
        "                wav = wav.mean(0, keepdim=True)\n",
        "\n",
        "            # Random Crop (Query Length)\n",
        "            max_len = int(QUERY_LEN * SAMPLE_RATE)\n",
        "            if wav.shape[1] > max_len:\n",
        "                s = random.randint(0, wav.shape[1] - max_len)\n",
        "                wav = wav[:, s:s + max_len]\n",
        "\n",
        "            # Augmentation\n",
        "            if aug:\n",
        "                try:\n",
        "                    wav_np = aug(samples=wav.squeeze(0).numpy(), sample_rate=SAMPLE_RATE)\n",
        "                    wav = torch.from_numpy(wav_np).unsqueeze(0)\n",
        "                except: pass\n",
        "\n",
        "            # Inference\n",
        "            # q_emb comes back on CPU (from our memory-safe function)\n",
        "            q_emb, q_times = audio_to_embedding(model, wav)\n",
        "            if q_emb is None: continue\n",
        "\n",
        "            # üîë FIX: Move Query to GPU for distance calculation\n",
        "            q_emb = q_emb.to(DEVICE)\n",
        "\n",
        "            # Distance Calculation\n",
        "            dists = torch.cdist(q_emb, db_vecs)\n",
        "            vals, idxs = torch.topk(dists, k=5, largest=False)\n",
        "\n",
        "            matches = []\n",
        "            for i in range(q_emb.shape[0]):\n",
        "                for k in range(5):\n",
        "                    matches.append((vals[i,k].item(), db_meta[idxs[i,k]], q_times[i]))\n",
        "\n",
        "            ranked = calculate_v3_scores(matches)\n",
        "            if not ranked: continue\n",
        "\n",
        "            names = [x[0] for x in ranked]\n",
        "            if target == names[0]: results[mode][\"top1\"] += 1\n",
        "            if target in names[:5]: results[mode][\"top5\"] += 1\n",
        "            if target in names[:10]: results[mode][\"top10\"] += 1\n",
        "\n",
        "    print(\"\\nüèÜ FINAL RESULTS\")\n",
        "    for m in modes:\n",
        "        print(f\"{m.upper():<6} | \"\n",
        "              f\"Top1: {results[m]['top1']/trials*100:.1f}% | \"\n",
        "              f\"Top5: {results[m]['top5']/trials*100:.1f}% | \"\n",
        "              f\"Top10: {results[m]['top10']/trials*100:.1f}%\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# RUN\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming model and db_vecs are already loaded/built from your previous cell\n",
        "    # If not, reload them:\n",
        "    if 'model' not in globals():\n",
        "        model = AudioCRNN_Fast(embed_dim=128).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "        model.eval()\n",
        "\n",
        "    if 'db_vecs' in globals() and db_vecs is not None:\n",
        "        run_evaluation(model, db_vecs, db_meta, trials=100)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Please run build_database() first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE2PiejbWYhW",
        "outputId": "36f0ca6d-6621-4302-9383-197c0752142f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö° Starting Evaluation (100 trials per mode)...\n",
            "\n",
            "‚ñ∂ MODE: CLEAN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂ MODE: SOFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09<00:00, 10.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂ MODE: HARD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16<00:00,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ FINAL RESULTS\n",
            "CLEAN  | Top1: 27.0% | Top5: 43.0% | Top10: 48.0%\n",
            "SOFT   | Top1: 25.0% | Top5: 32.0% | Top10: 40.0%\n",
            "HARD   | Top1: 4.0% | Top5: 13.0% | Top10: 20.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8QDxuOdWYib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL EVAL"
      ],
      "metadata": {
        "id": "5hRqVn360Zhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# üß™ FINAL SPECTROGRAM EVALUATION (SMART RESUME + ULTRA MODE)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "packages = [\"audiomentations\", \"torchaudio\"]\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "from audiomentations import Compose, AddBackgroundNoise, PitchShift, TimeStretch, Gain, PolarityInversion\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# ‚öôÔ∏è CONFIGURATION\n",
        "# ------------------------------------------------------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Input Zips (Drive)\n",
        "ZIP_SOURCE_DIR = '/content/drive/MyDrive/FINE_TUNE_V3'\n",
        "ZIP_EVAL_ORIGINALS = os.path.join(ZIP_SOURCE_DIR, 'eval_originals_300.zip')\n",
        "ZIP_TRAIN_COVERS = os.path.join(ZIP_SOURCE_DIR, 'train_covers_1300.zip')\n",
        "ZIP_NOISE = os.path.join(ZIP_SOURCE_DIR, 'noise_data_16k.zip')\n",
        "\n",
        "# Local Paths\n",
        "EVAL_DIR = \"/content/data/eval_combined\"\n",
        "NOISE_DIR = \"/content/data/noise_16k\"\n",
        "\n",
        "# Model Checkpoint\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN\"\n",
        "MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch_34.pth\")\n",
        "\n",
        "# Audio Params\n",
        "SAMPLE_RATE = 16000\n",
        "WIN_SEC = 3.0\n",
        "HOP_SEC = 1.5\n",
        "QUERY_LEN = 15\n",
        "INFERENCE_BATCH_SIZE = 64\n",
        "SIGMA = 0.5\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. DATA PREPARATION (SMART RESUME)\n",
        "# ------------------------------------------------------------------------------\n",
        "def cleanup_noise_mp3s(noise_dir):\n",
        "    \"\"\"Recursively finds MP3s, converts to WAV 16k, and deletes MP3.\"\"\"\n",
        "    mp3s = glob.glob(os.path.join(noise_dir, \"**/*.mp3\"), recursive=True)\n",
        "    if not mp3s:\n",
        "        return # Nothing to do\n",
        "\n",
        "    print(f\"üßπ Cleaning up {len(mp3s)} MP3 files in noise directory...\")\n",
        "    for m in tqdm(mp3s, desc=\"Converting MP3->WAV\"):\n",
        "        try:\n",
        "            w, sr = torchaudio.load(m)\n",
        "            if sr != SAMPLE_RATE:\n",
        "                w = torchaudio.functional.resample(w, sr, SAMPLE_RATE)\n",
        "            torchaudio.save(m.replace('.mp3','.wav'), w, SAMPLE_RATE)\n",
        "            os.remove(m)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to convert {os.path.basename(m)}: {e}\")\n",
        "\n",
        "def setup_data():\n",
        "    print(f\"\\nüöÄ SETTING UP DATA...\")\n",
        "    os.makedirs(EVAL_DIR, exist_ok=True)\n",
        "    os.makedirs(NOISE_DIR, exist_ok=True)\n",
        "\n",
        "    # --- A. NOISE SETUP ---\n",
        "    # Check if noise dir is populated (contains at least 5 files)\n",
        "    noise_files = glob.glob(os.path.join(NOISE_DIR, \"**/*.wav\"), recursive=True)\n",
        "    if len(noise_files) > 5:\n",
        "        print(f\"‚úÖ Noise directory seems populated ({len(noise_files)} files). Skipping Unzip.\")\n",
        "    else:\n",
        "        print(f\"üìÇ Unzipping Noise Data...\")\n",
        "        subprocess.run(f\"unzip -q -n '{ZIP_NOISE}' -d '{NOISE_DIR}'\", shell=True)\n",
        "\n",
        "    # Always run MP3 cleanup to be safe (it's fast if no MP3s exist)\n",
        "    cleanup_noise_mp3s(NOISE_DIR)\n",
        "\n",
        "    # --- B. EVALUATION DB SETUP ---\n",
        "    # Check if Eval dir is populated (contains > 100 wavs)\n",
        "    eval_files = glob.glob(os.path.join(EVAL_DIR, \"*.wav\"))\n",
        "    if len(eval_files) > 100:\n",
        "        print(f\"‚úÖ Eval directory seems populated ({len(eval_files)} files). Skipping Unzip.\")\n",
        "    else:\n",
        "        print(f\"üìÇ Unzipping Eval Originals (Targets)...\")\n",
        "        subprocess.run(f\"unzip -q -n '{ZIP_EVAL_ORIGINALS}' -d '{EVAL_DIR}'\", shell=True)\n",
        "\n",
        "        print(f\"üìÇ Unzipping Train Covers (Distractors)...\")\n",
        "        subprocess.run(f\"unzip -q -n '{ZIP_TRAIN_COVERS}' -d '{EVAL_DIR}'\", shell=True)\n",
        "\n",
        "        # Flatten directory (move subfolder contents to root)\n",
        "        print(\"   Flattening directory structure...\")\n",
        "        for root, dirs, files in os.walk(EVAL_DIR):\n",
        "            for file in files:\n",
        "                if file.endswith(\".wav\"):\n",
        "                    src = os.path.join(root, file)\n",
        "                    dst = os.path.join(EVAL_DIR, file)\n",
        "                    if src != dst:\n",
        "                        try: os.rename(src, dst)\n",
        "                        except: pass\n",
        "\n",
        "    final_count = len(glob.glob(os.path.join(EVAL_DIR, \"*.wav\")))\n",
        "    print(f\"üìä Final Database Count: {final_count} files in {EVAL_DIR}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. MODEL ARCHITECTURE (SHALLOW CNN)\n",
        "# ------------------------------------------------------------------------------\n",
        "class AudioSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.pool(x).view(x.size(0), -1)\n",
        "        return F.normalize(self.fc(x), p=2, dim=1)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. INFERENCE ENGINE\n",
        "# ------------------------------------------------------------------------------\n",
        "mel = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=512, n_mels=64\n",
        ").to(DEVICE)\n",
        "db_transform = torchaudio.transforms.AmplitudeToDB().to(DEVICE)\n",
        "\n",
        "def robust_load(path):\n",
        "    try:\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if sr != SAMPLE_RATE:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(0, keepdim=True)\n",
        "        return wav, SAMPLE_RATE\n",
        "    except: return None, 0\n",
        "\n",
        "def audio_to_embedding(model, wav):\n",
        "    samples_win = int(SAMPLE_RATE * WIN_SEC)\n",
        "    samples_hop = int(SAMPLE_RATE * HOP_SEC)\n",
        "\n",
        "    if wav.shape[1] < samples_win:\n",
        "        wav = F.pad(wav, (0, samples_win - wav.shape[1]))\n",
        "\n",
        "    windows = []\n",
        "    times = []\n",
        "    for i in range(0, wav.shape[1] - samples_win + 1, samples_hop):\n",
        "        windows.append(wav[:, i:i + samples_win])\n",
        "        times.append(i / SAMPLE_RATE)\n",
        "\n",
        "    if not windows: return None, None\n",
        "\n",
        "    all_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(windows), INFERENCE_BATCH_SIZE):\n",
        "            batch = torch.stack(windows[i : i + INFERENCE_BATCH_SIZE]).to(DEVICE)\n",
        "            spec = db_transform(mel(batch))\n",
        "            mean = spec.mean(dim=(2,3), keepdim=True)\n",
        "            std = spec.std(dim=(2,3), keepdim=True)\n",
        "            spec = (spec - mean) / (std + 1e-6)\n",
        "            emb = model.forward_one(spec)\n",
        "            all_embeds.append(emb.cpu())\n",
        "\n",
        "    return torch.cat(all_embeds), times\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. DATABASE BUILDER\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_database(model):\n",
        "    print(\"\\nüèóÔ∏è  Indexing Database (Originals + Covers)...\")\n",
        "    files = glob.glob(os.path.join(EVAL_DIR, \"*.wav\"))\n",
        "\n",
        "    db_vecs = []\n",
        "    db_meta = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for f in tqdm(files, desc=\"Indexing\"):\n",
        "        wav, _ = robust_load(f)\n",
        "        if wav is None: continue\n",
        "\n",
        "        emb, times = audio_to_embedding(model, wav)\n",
        "        if emb is None: continue\n",
        "\n",
        "        base_name = os.path.basename(f)\n",
        "\n",
        "        for i in range(len(times)):\n",
        "            db_vecs.append(emb[i])\n",
        "            db_meta.append({\"name\": base_name, \"offset\": times[i]})\n",
        "\n",
        "    if not db_vecs: return None, None\n",
        "    print(f\"‚úÖ Indexed {len(db_vecs)} vectors from {len(files)} songs.\")\n",
        "    return torch.stack(db_vecs).to(DEVICE), db_meta\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. EVALUATION LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_evaluation(model, db_vecs, db_meta, trials=200):\n",
        "    if db_vecs is None: return\n",
        "\n",
        "    augmenters = {\n",
        "        \"clean\": None,\n",
        "        \"soft\": Compose([Gain(min_gain_db=-3, max_gain_db=3, p=0.5)]),\n",
        "        \"hard\": Compose([\n",
        "            AddBackgroundNoise(sounds_path=NOISE_DIR, min_snr_db=5, max_snr_db=15, p=1.0),\n",
        "            PitchShift(min_semitones=-2, max_semitones=2, p=0.5)\n",
        "        ]),\n",
        "        \"ultra\": Compose([\n",
        "            AddBackgroundNoise(sounds_path=NOISE_DIR, min_snr_db=3, max_snr_db=10, p=1.0),\n",
        "            PitchShift(min_semitones=-3, max_semitones=3, p=0.8),\n",
        "            TimeStretch(min_rate=0.85, max_rate=1.15, p=0.5),\n",
        "            PolarityInversion(p=0.5)\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    unique_songs = list(set([m['name'] for m in db_meta]))\n",
        "\n",
        "    print(f\"\\n‚ö° STARTING EVALUATION ({trials} trials per mode)\")\n",
        "    print(f\"   Database Size: {len(unique_songs)} unique tracks\")\n",
        "\n",
        "    results = defaultdict(lambda: {\"top1\": 0, \"top5\": 0})\n",
        "\n",
        "    for mode, aug in augmenters.items():\n",
        "        print(f\"\\n‚ñ∂ MODE: {mode.upper()}\")\n",
        "\n",
        "        for _ in tqdm(range(trials)):\n",
        "            target_name = random.choice(unique_songs)\n",
        "            target_path = os.path.join(EVAL_DIR, target_name)\n",
        "\n",
        "            wav, _ = robust_load(target_path)\n",
        "            if wav is None: continue\n",
        "\n",
        "            req_samples = int(QUERY_LEN * SAMPLE_RATE)\n",
        "            if wav.shape[1] > req_samples:\n",
        "                start = random.randint(0, wav.shape[1] - req_samples)\n",
        "                query_wav = wav[:, start:start+req_samples]\n",
        "                query_offset = start / SAMPLE_RATE\n",
        "            else:\n",
        "                query_wav = wav\n",
        "                query_offset = 0.0\n",
        "\n",
        "            if aug:\n",
        "                try:\n",
        "                    q_np = query_wav.squeeze().numpy()\n",
        "                    q_aug = aug(samples=q_np, sample_rate=SAMPLE_RATE)\n",
        "                    query_wav = torch.from_numpy(q_aug).unsqueeze(0)\n",
        "                except: pass\n",
        "\n",
        "            q_emb, q_times = audio_to_embedding(model, query_wav)\n",
        "            if q_emb is None: continue\n",
        "            q_emb = q_emb.to(DEVICE)\n",
        "\n",
        "            dists = torch.cdist(q_emb, db_vecs)\n",
        "            scores = defaultdict(float)\n",
        "\n",
        "            topk_vals, topk_idxs = torch.topk(dists, k=10, largest=False)\n",
        "\n",
        "            for i in range(len(q_times)):\n",
        "                for k in range(10):\n",
        "                    match_idx = topk_idxs[i, k].item()\n",
        "                    match_dist = topk_vals[i, k].item()\n",
        "                    match_meta = db_meta[match_idx]\n",
        "                    weight = math.exp(-(match_dist**2) / (2 * SIGMA**2))\n",
        "                    scores[match_meta['name']] += weight\n",
        "\n",
        "            ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            if not ranked: continue\n",
        "\n",
        "            top1_name = ranked[0][0]\n",
        "            top5_names = [r[0] for r in ranked[:5]]\n",
        "\n",
        "            if top1_name == target_name: results[mode][\"top1\"] += 1\n",
        "            if target_name in top5_names: results[mode][\"top5\"] += 1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"üèÜ FINAL RESULTS (Epoch 34)\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{'MODE':<10} | {'TOP-1':<8} | {'TOP-5':<8}\")\n",
        "    print(\"-\" * 32)\n",
        "    for m in [\"clean\", \"soft\", \"hard\", \"ultra\"]:\n",
        "        t1 = results[m][\"top1\"] / trials * 100\n",
        "        t5 = results[m][\"top5\"] / trials * 100\n",
        "        print(f\"{m.upper():<10} | {t1:.1f}%     | {t5:.1f}%\")\n",
        "    print(\"-\" * 32)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    setup_data()\n",
        "\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        print(f\"üìÇ Loading Model: {MODEL_PATH}\")\n",
        "        model = AudioSiameseNet(embed_dim=128).to(DEVICE)\n",
        "        ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "\n",
        "        if \"model_state_dict\" in ckpt:\n",
        "            model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        else:\n",
        "            model.load_state_dict(ckpt)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        db_vecs, db_meta = build_database(model)\n",
        "        run_evaluation(model, db_vecs, db_meta, trials=200)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: Model not found at {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCS3yQgGWYje",
        "outputId": "f9d66a47-b1e4-4fd0-c2e6-3213c7730c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ SETTING UP DATA...\n",
            "‚úÖ Noise directory seems populated (2129 files). Skipping Unzip.\n",
            "üìÇ Unzipping Eval Originals (Targets)...\n",
            "üìÇ Unzipping Train Covers (Distractors)...\n",
            "   Flattening directory structure...\n",
            "üìä Final Database Count: 1612 files in /content/data/eval_combined\n",
            "üìÇ Loading Model: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\n",
            "\n",
            "üèóÔ∏è  Indexing Database (Originals + Covers)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1612/1612 [02:30<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Indexed 288216 vectors from 1612 songs.\n",
            "\n",
            "‚ö° STARTING EVALUATION (200 trials per mode)\n",
            "   Database Size: 1612 unique tracks\n",
            "\n",
            "‚ñ∂ MODE: CLEAN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:11<00:00, 16.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂ MODE: SOFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:13<00:00, 14.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂ MODE: HARD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:39<00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∂ MODE: ULTRA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:36<00:00,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "üèÜ FINAL RESULTS (Epoch 34)\n",
            "========================================\n",
            "MODE       | TOP-1    | TOP-5   \n",
            "--------------------------------\n",
            "CLEAN      | 91.5%     | 99.0%\n",
            "SOFT       | 93.5%     | 99.5%\n",
            "HARD       | 37.5%     | 54.5%\n",
            "ULTRA      | 22.0%     | 35.0%\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3cGsIFDWYkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMiMSYBbWYln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLLgzg2OWYml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kipl4XymWYn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CV6tUPPNWYo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwCVBwVZWYqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5q6NIB4WYrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0RYUP2GWYsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL best models to onnx conversion\n"
      ],
      "metadata": {
        "id": "L_B3NRGT1dBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvPVfc5e1w75",
        "outputId": "58e8362a-a03b-4b6e-a24f-f174a6afb847"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.15 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.16-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: onnx>=1.17 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (26.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17->onnxscript) (5.29.6)\n",
            "Downloading onnxscript-0.6.2-py3-none-any.whl (689 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.16-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx_ir, onnxscript\n",
            "Successfully installed onnx_ir-0.1.16 onnxscript-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. DEFINE ARCHITECTURES\n",
        "# ==========================================\n",
        "\n",
        "# --- PITCH MODEL (CRNN) ---\n",
        "# --- UPDATED PITCH MODEL (CRNN) ---\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(128, 256, 3, padding=1), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # ‚ùå REMOVED: self.lstm.flatten_parameters()\n",
        "        # (It breaks ONNX export by mutating state during forward pass)\n",
        "\n",
        "        out, _ = self.lstm(x)\n",
        "        out = torch.mean(out, dim=1)\n",
        "        return F.normalize(self.fc(out), p=2, dim=1)\n",
        "\n",
        "# --- SPECTROGRAM MODEL (Shallow CNN) ---\n",
        "class AudioSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    # Renamed from forward_one for ONNX tracing\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.pool(x).view(x.size(0), -1)\n",
        "        return F.normalize(self.fc(x), p=2, dim=1)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXPORT FUNCTION\n",
        "# ==========================================\n",
        "def export_to_onnx(model, weights_path, output_path, dummy_input, dynamic_axes):\n",
        "    print(f\"\\nüîÑ Loading weights from: {weights_path}\")\n",
        "\n",
        "    # Load weights safely\n",
        "    checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
        "    if \"model_state_dict\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"üì¶ Exporting to ONNX: {output_path}\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        output_path,\n",
        "        export_params=True,\n",
        "        opset_version=14,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes=dynamic_axes,\n",
        "        # üöÄ THE FIX: Tell PyTorch to use the stable TorchScript exporter, not Dynamo\n",
        "        dynamo=False\n",
        "    )\n",
        "    print(\"‚úÖ Export Successful!\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. EXECUTE EXPORTS\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure output directory exists\n",
        "    OUT_DIR = \"/content/drive/MyDrive/FIND_TUNE/onnx_models\"\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    # --- Export CRNN (Pitch) ---\n",
        "    crnn_model = CRNN(embed_dim=128)\n",
        "    crnn_weights = \"/content/drive/MyDrive/FIND_TUNE/pitch_based_model/finetuned_models(BEST)/FINETUNED_CRNN_Smooth.pth\"\n",
        "    crnn_out = os.path.join(OUT_DIR, \"pitch_crnn.onnx\")\n",
        "\n",
        "    # Dummy input: (Batch=1, Channels=1, Sequence Length=1000)\n",
        "    crnn_dummy = torch.randn(1, 1, 1000)\n",
        "    crnn_axes = {'input': {0: 'batch_size', 2: 'seq_length'}, 'output': {0: 'batch_size'}}\n",
        "\n",
        "    export_to_onnx(crnn_model, crnn_weights, crnn_out, crnn_dummy, crnn_axes)\n",
        "\n",
        "\n",
        "    # --- Export AudioSiameseNet (Spectrogram) ---\n",
        "    spec_model = AudioSiameseNet(embed_dim=128)\n",
        "    spec_weights = \"/content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\"\n",
        "    spec_out = os.path.join(OUT_DIR, \"spectrogram_cnn.onnx\")\n",
        "\n",
        "    # Dummy input: (Batch=1, Channels=1, Mels=64, Time_Frames=94)\n",
        "    spec_dummy = torch.randn(1, 1, 64, 94)\n",
        "    spec_axes = {'input': {0: 'batch_size', 3: 'time_frames'}, 'output': {0: 'batch_size'}}\n",
        "\n",
        "    export_to_onnx(spec_model, spec_weights, spec_out, spec_dummy, spec_axes)"
      ],
      "metadata": {
        "id": "0_ioMx5TWYtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbbb233-7349-4674-8c49-7a502ded5025"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Loading weights from: /content/drive/MyDrive/FIND_TUNE/pitch_based_model/finetuned_models(BEST)/FINETUNED_CRNN_Smooth.pth\n",
            "üì¶ Exporting to ONNX: /content/drive/MyDrive/FIND_TUNE/onnx_models/pitch_crnn.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-933335091.py:79: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:4247: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Export Successful!\n",
            "\n",
            "üîÑ Loading weights from: /content/drive/MyDrive/FIND_TUNE/spectrogram_based_model/shallow_CNN/checkpoint_epoch_34.pth\n",
            "üì¶ Exporting to ONNX: /content/drive/MyDrive/FIND_TUNE/onnx_models/spectrogram_cnn.onnx\n",
            "‚úÖ Export Successful!\n"
          ]
        }
      ]
    }
  ]
}