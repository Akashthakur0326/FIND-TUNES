{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Performance Analysis & Inferences\n",
        "1. The \"Smoothing\" Paradox\n",
        "\n",
        "        In CNNs: Smoothing helps \"Soft\" performance (Best 18 vs 21) but often hurts \"Hard\" performance. Why? Because a CNN is a pattern matcher; smoothing removes the high-frequency detail it uses to tell songs apart.\n",
        "\n",
        "\n",
        "        In LSTMs: Smoothing acts as a Stabilizer. Look at Best 19/20: the accuracy for Soft and Hard is almost identical. This means the LSTM doesn't care if the input is noisy or cleanâ€”it only cares about the melodic sequence.\n",
        "\n",
        "2. CNN vs. LSTM (Precision vs. Robustness)\n",
        "\n",
        "        The Deeper CNN (Best 18/21) is your \"High-Accuracy\" specialist. It hits 60% Top-1 on Soft Hum. It is excellent at recognizing clear, well-sung melodies.\n",
        "\n",
        "        The LSTM (Best 19/20) is your \"Tank.\" It is much more consistent. Notice that Best 19 has identical Top-1 scores (53.8%) for both Soft and Hard. It is completely unfazed by the noise and jitter of a \"Hard Hum.\"\n",
        "\n",
        "3. The \"Hard Hum\" Champions\n",
        "\n",
        "        Winner: Best 16 (Old + Smoothing) at 51.5% and Best 19 (LSTM + Smoothing) at 53.8%.\n",
        "\n",
        "        Loser: Best 15 (Old No Smoothing) at 11.33%.\n",
        "\n",
        "        Inference: Without smoothing or sequence modeling (LSTM), your model cannot \"see\" through the noise of a bad hum.\n",
        "\n",
        "\n",
        "TABLE\n",
        "\n",
        "        Model Style,Best Top-1 (Soft),Best Top-1 (Hard),Character\n",
        "        Deeper CNN (No Smooth),60.0%,48.8%,\"High Precision, Moderate Noise Resistance.\"\n",
        "        Deeper CNN (Smooth),60.2%,40.5%,\"Great for \"\"singing,\"\" bad for \"\"humming.\"\"\"\n",
        "        LSTM (Smooth),53.8%,53.8%,Rock Solid. Performance doesn't drop with noise.\n",
        "        Old Model (Smooth),53.2%,51.5%,\"Surprisingly decent, but limited growth.\""
      ],
      "metadata": {
        "id": "is124Ft-nUz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALL IN ONE\n"
      ],
      "metadata": {
        "id": "yi5d0fbG2dBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec\n",
        "!pip install torchcrepe"
      ],
      "metadata": {
        "id": "zoz5JbTN2zl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt_dlp"
      ],
      "metadata": {
        "id": "BTHYhCQ13CpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y aria2\n"
      ],
      "metadata": {
        "id": "ALtc00Qr7Wg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DAY 2"
      ],
      "metadata": {
        "id": "y85827DWmQ3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# folders used by your script\n",
        "folders_to_clean = [\n",
        "    \"/content/data_unique\",\n",
        "    \"/content/eval\",\n",
        "    \"/content/tmp_extract\"\n",
        "]\n",
        "\n",
        "for folder in folders_to_clean:\n",
        "    if os.path.exists(folder):\n",
        "        print(f\"ðŸ§¹ Removing: {folder}\")\n",
        "        shutil.rmtree(folder)\n",
        "    else:\n",
        "        print(f\"âœ” Skipped (not found): {folder}\")\n",
        "\n",
        "# recreate empty directories\n",
        "os.makedirs(\"/content/data_unique\", exist_ok=True)\n",
        "os.makedirs(\"/content/eval\", exist_ok=True)\n",
        "os.makedirs(\"/content/tmp_extract\", exist_ok=True)\n",
        "\n",
        "print(\"\\nâœ¨ Clean reset complete. Run your ZIP â†’ VALIDATE â†’ DEDUP script now.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcSrt300n8Ds",
        "outputId": "066e3000-7a5d-4b6a-e935-6ec2786743d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Removing: /content/data_unique\n",
            "ðŸ§¹ Removing: /content/eval\n",
            "ðŸ§¹ Removing: /content/tmp_extract\n",
            "\n",
            "âœ¨ Clean reset complete. Run your ZIP â†’ VALIDATE â†’ DEDUP script now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# ZIP -> EXTRACT -> VALIDATE -> DEDUP (with limit) -> SAVE UNIQUE\n",
        "# =============================================================\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "import csv\n",
        "import random\n",
        "import shutil\n",
        "from collections import defaultdict # <--- *** FIXED: ADDED THIS IMPORT ***\n",
        "\n",
        "# ---------- CONFIGURATION ----------\n",
        "ZIP_DIR = \"/content/zips\"\n",
        "EXTRACT_TMP = \"/content/tmp_extract\"\n",
        "OUTPUT_DIR = \"/content/data_unique\"  # unique, validated npy\n",
        "EVAL_DIR = \"/content/eval\"           # evaluation set\n",
        "METADATA_CSV = os.path.join(OUTPUT_DIR, \"metadata.csv\")\n",
        "\n",
        "# NEW CONFIG: Control how many duplicates of the exact same array content are kept\n",
        "MAX_DUPLICATE_COPIES = 1\n",
        "EVAL_N = 400\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(EXTRACT_TMP, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(EVAL_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- FIND ZIPs ----------\n",
        "zip_files = [f for f in os.listdir(ZIP_DIR) if f.endswith(\".zip\")]\n",
        "print(f\"ðŸ“¦ Found {len(zip_files)} ZIP files\")\n",
        "\n",
        "# ---------- VALIDATION ----------\n",
        "def is_valid_pitch(arr):\n",
        "    # Checks specific to pitch data (1D, non-empty, no junk values, not all zero)\n",
        "    if not isinstance(arr, np.ndarray): return False\n",
        "    if arr.size == 0: return False\n",
        "    if np.isnan(arr).any(): return False\n",
        "    if np.isinf(arr).any(): return False\n",
        "    if arr.ndim != 1: return False\n",
        "    if arr.sum() == 0: return False\n",
        "    return True\n",
        "\n",
        "# ---------- HASHING ----------\n",
        "def md5_of_array(arr):\n",
        "    m = hashlib.md5()\n",
        "    # Use raw bytes representation (ensure dtype & shape consistent)\n",
        "    m.update(arr.astype(np.float32).tobytes())\n",
        "    m.update(str(arr.shape).encode())\n",
        "    m.update(str(arr.dtype).encode())\n",
        "    return m.hexdigest()\n",
        "\n",
        "# Refined 'seen' tracks hash count: {md5: count}\n",
        "seen_hash_count = defaultdict(int)\n",
        "# Tracks hash -> first saved filename for reference\n",
        "seen_first_filename = {}\n",
        "\n",
        "duplicates_kept = 0\n",
        "duplicates_skipped = 0\n",
        "good_saved = 0\n",
        "bad_invalid = 0\n",
        "total_npy_seen = 0\n",
        "\n",
        "# Prepare metadata CSV\n",
        "meta_fields = [\"src_zip\", \"orig_path\", \"saved_name\", \"md5\", \"status\", \"note\"]\n",
        "meta_rows = []\n",
        "\n",
        "# ---------- PROCESS zips ----------\n",
        "for zip_name in tqdm(zip_files, desc=\"Processing ZIPs\"):\n",
        "    zip_path = os.path.join(ZIP_DIR, zip_name)\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall(EXTRACT_TMP)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not extract {zip_name}. Skipping. Error: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Walk extracted contents\n",
        "    for root, _, files in os.walk(EXTRACT_TMP):\n",
        "        for f in files:\n",
        "            if not f.endswith(\".npy\"):\n",
        "                continue\n",
        "\n",
        "            total_npy_seen += 1\n",
        "            npy_path = os.path.join(root, f)\n",
        "            relative_path = os.path.relpath(npy_path, EXTRACT_TMP)\n",
        "\n",
        "            arr = None\n",
        "            try:\n",
        "                arr = np.load(npy_path)\n",
        "            except Exception as e:\n",
        "                bad_invalid += 1\n",
        "                meta_rows.append([zip_name, relative_path, \"\", \"\", \"bad_load\", str(e)])\n",
        "                continue\n",
        "\n",
        "            if not is_valid_pitch(arr):\n",
        "                bad_invalid += 1\n",
        "                meta_rows.append([zip_name, relative_path, \"\", \"\", \"invalid\", \"empty/nan/inf/dim\"])\n",
        "                continue\n",
        "\n",
        "            h = md5_of_array(arr)\n",
        "\n",
        "            # --- DUPLICATE HANDLING LOGIC ---\n",
        "\n",
        "            is_duplicate = h in seen_hash_count\n",
        "\n",
        "            if is_duplicate and seen_hash_count[h] >= MAX_DUPLICATE_COPIES:\n",
        "                # We already have enough copies of this exact array content\n",
        "                duplicates_skipped += 1\n",
        "                meta_rows.append([zip_name, relative_path, seen_first_filename.get(h, \"\"), h, \"duplicate_skipped\", f\"max copies ({MAX_DUPLICATE_COPIES}) reached\"])\n",
        "                continue\n",
        "\n",
        "            # If not a duplicate, or if we need to keep more copies\n",
        "\n",
        "            # Handle potential filename collision in OUTPUT_DIR\n",
        "            base_name = os.path.splitext(f)[0]\n",
        "            save_name = f\"{base_name}.npy\"\n",
        "\n",
        "            # If MD5 is new, or we are keeping the duplicate\n",
        "            if not is_duplicate or seen_hash_count[h] < MAX_DUPLICATE_COPIES:\n",
        "\n",
        "                # Suffix file name if we're saving a duplicate copy\n",
        "                if is_duplicate:\n",
        "                    # e.g., song_a.npy -> song_a_copy1.npy\n",
        "                    save_name = f\"{base_name}_copy{seen_hash_count[h]}.npy\"\n",
        "                    duplicates_kept += 1\n",
        "                    status = \"duplicate_kept\"\n",
        "                    note = f\"copy {seen_hash_count[h]} saved\"\n",
        "                else:\n",
        "                    status = \"saved\"\n",
        "                    note = \"unique file saved\"\n",
        "\n",
        "                # Final check for filename collision (should be rare now)\n",
        "                if os.path.exists(os.path.join(OUTPUT_DIR, save_name)):\n",
        "                    save_name = f\"{base_name}_{h[:8]}.npy\"\n",
        "\n",
        "                save_path = os.path.join(OUTPUT_DIR, save_name)\n",
        "                np.save(save_path, arr.astype(np.float32))\n",
        "\n",
        "                # Update tracking\n",
        "                seen_hash_count[h] += 1\n",
        "                if not is_duplicate:\n",
        "                    seen_first_filename[h] = save_name\n",
        "\n",
        "                good_saved += 1\n",
        "                meta_rows.append([zip_name, relative_path, save_name, h, status, note])\n",
        "\n",
        "    # clear tmp (Using shutil.rmtree is safer and faster for cleanup)\n",
        "    shutil.rmtree(EXTRACT_TMP, ignore_errors=True)\n",
        "    os.makedirs(EXTRACT_TMP, exist_ok=True) # Recreate empty folder\n",
        "\n",
        "# ---------- WRITE METADATA ----------\n",
        "with open(METADATA_CSV, \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(meta_fields)\n",
        "    writer.writerows(meta_rows)\n",
        "\n",
        "# ---------- CREATE EVAL SET ----------\n",
        "# The unique source files are those that were saved (good_saved count)\n",
        "unique_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".npy\") and f != \"metadata.csv\"]\n",
        "random.shuffle(unique_files)\n",
        "\n",
        "# Delete old eval files first\n",
        "for f in os.listdir(EVAL_DIR):\n",
        "    os.remove(os.join(EVAL_DIR, f)) # Fixed os.path.join error\n",
        "\n",
        "eval_select = unique_files[:min(EVAL_N, len(unique_files))]\n",
        "for fn in eval_select:\n",
        "    src = os.path.join(OUTPUT_DIR, fn)\n",
        "    dst = os.path.join(EVAL_DIR, fn)\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "# ---------- SUMMARY ----------\n",
        "print(\"âœ… DONE!\")\n",
        "print(f\"ðŸ”¢ Total .npy encountered: {total_npy_seen}\")\n",
        "print(f\"ðŸ‘ Good files saved (including kept copies): {good_saved}\")\n",
        "print(f\"   -> Max copies per unique array: {MAX_DUPLICATE_COPIES}\")\n",
        "print(f\"   -> Duplicates Kept: {duplicates_kept}\")\n",
        "print(f\"ðŸ” Duplicates Skipped: {duplicates_skipped}\")\n",
        "print(f\"âŒ Bad/invalid files: {bad_invalid}\")\n",
        "print(f\"ðŸ“ Unique data saved to: {OUTPUT_DIR}\")\n",
        "print(f\"ðŸ“ Metadata CSV: {METADATA_CSV}\")\n",
        "print(f\"ðŸŽ¯ Eval set created: {len(eval_select)} files -> {EVAL_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7yy8kuYmQWI",
        "outputId": "6648ba4a-b3b4-42e2-ea13-ae5480bddcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Found 77 ZIP files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing ZIPs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:12<00:00,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DONE!\n",
            "ðŸ”¢ Total .npy encountered: 7050\n",
            "ðŸ‘ Good files saved (including kept copies): 4051\n",
            "   -> Max copies per unique array: 1\n",
            "   -> Duplicates Kept: 0\n",
            "ðŸ” Duplicates Skipped: 1525\n",
            "âŒ Bad/invalid files: 1474\n",
            "ðŸ“ Unique data saved to: /content/data_unique\n",
            "ðŸ“ Metadata CSV: /content/data_unique/metadata.csv\n",
            "ðŸŽ¯ Eval set created: 400 files -> /content/eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSZWu4sFmQSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD"
      ],
      "metadata": {
        "id": "jPGDK98BDDO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # <--- NEW IMPORT\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams (UPDATED)\n",
        "# -------------------------\n",
        "TARGET_LEN = 300 #frame size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100       # <--- REDUCED EPOCHS\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# UPDATED PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV1_old\" # Using the path provided by the user\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# Augment hum (strong) - NO CHANGE\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy().astype(np.float32)\n",
        "    pitch += np.random.normal(0, 0.06, size=len(pitch))\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "    if random.random() < 0.7:\n",
        "        rate = np.random.uniform(0.8, 1.25)\n",
        "        old_idx = np.arange(len(pitch))\n",
        "        new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "        pitch = np.interp(new_idx, old_idx, pitch)\n",
        "    try:\n",
        "        b, a = sg.butter(3, 0.15)\n",
        "        pitch = sg.filtfilt(b, a, pitch)\n",
        "    except Exception:\n",
        "        pass\n",
        "    pitch += np.random.normal(0, 0.04, size=len(pitch))\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: force equal length - NO CHANGE\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "    if len(arr) < target_len:\n",
        "        return np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# Dataset - NO CHANGE\n",
        "# -------------------------\n",
        "class PitchDatasetV3(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted([f for f in __import__(\"glob\").glob(os.path.join(pitch_dir, \"*.npy\"))])\n",
        "        self.target_len = target_len\n",
        "        print(f\"Loaded {len(self.files)} cleaned pitch contours from {pitch_dir}\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return force_length(arr, self.target_len)\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        anchor = self._random_crop(anchor_full)\n",
        "        positive_clean = self._random_crop(anchor_full)\n",
        "        positive_hum = augment_hum(positive_clean)\n",
        "        positive_hum = force_length(positive_hum, self.target_len)\n",
        "\n",
        "        negative = self._random_crop(neg_full)\n",
        "        negative = augment_hum(negative)\n",
        "        negative = force_length(negative, self.target_len)\n",
        "\n",
        "        anchor = force_length(anchor, self.target_len)\n",
        "        positive_clean = force_length(positive_clean, self.target_len)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(anchor).unsqueeze(0).float(),\n",
        "            torch.from_numpy(positive_clean).unsqueeze(0).float(),\n",
        "            torch.from_numpy(positive_hum).unsqueeze(0).float(),\n",
        "            torch.from_numpy(negative).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# Siamese Model - NO CHANGE\n",
        "# -------------------------\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop (FIXED num_workers)\n",
        "# -------------------------\n",
        "def train_v3_fixed():\n",
        "    print(f\"Training on: {DEVICE}\")\n",
        "\n",
        "    dataset = PitchDatasetV3(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    # FIX: Set num_workers=0 to prevent the AssertionErrors in the console\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchSiameseNet().to(DEVICE)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.8)\n",
        "\n",
        "    # Learning Rate Scheduler Initialization\n",
        "    # patience=8 means LR drops if loss doesn't improve for 8 epochs.\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for anchor, pos_clean, pos_hum, neg in loader:\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_clean = pos_clean.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg = neg.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            a = model.forward_one(anchor)\n",
        "            pc = model.forward_one(pos_clean)\n",
        "            ph = model.forward_one(pos_hum)\n",
        "            n = model.forward_one(neg)\n",
        "\n",
        "            # Dual Triplet Loss\n",
        "            loss = loss_fn(a, pc, n) + loss_fn(a, ph, n)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg:.4f} | LR: {optim.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(avg)\n",
        "\n",
        "        if avg < best_loss:\n",
        "            best_loss = avg\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New BEST saved: {BEST} (loss={best_loss:.4f})\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    print(f\"Best checkpoint: {BEST}\")\n",
        "    print(f\"Latest checkpoint: {LAST}\")\n",
        "\n",
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    train_v3_fixed()"
      ],
      "metadata": {
        "id": "xMzu-niumP2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90e078d-9dea-40cc-8e3f-bc9b28f37e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n",
            "Loaded 4051 cleaned pitch contours from /content/data_unique\n",
            "Epoch 1/100 | Loss: 0.6295 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.6295)\n",
            "Epoch 2/100 | Loss: 0.4605 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4605)\n",
            "Epoch 3/100 | Loss: 0.4470 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4470)\n",
            "Epoch 4/100 | Loss: 0.4216 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4216)\n",
            "Epoch 5/100 | Loss: 0.4287 | LR: 0.000100\n",
            "Epoch 6/100 | Loss: 0.4313 | LR: 0.000100\n",
            "Epoch 7/100 | Loss: 0.4126 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4126)\n",
            "Epoch 8/100 | Loss: 0.4093 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4093)\n",
            "Epoch 9/100 | Loss: 0.4029 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4029)\n",
            "Epoch 10/100 | Loss: 0.4012 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.4012)\n",
            "Epoch 11/100 | Loss: 0.3915 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3915)\n",
            "Epoch 12/100 | Loss: 0.3919 | LR: 0.000100\n",
            "Epoch 13/100 | Loss: 0.3718 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3718)\n",
            "Epoch 14/100 | Loss: 0.3604 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3604)\n",
            "Epoch 15/100 | Loss: 0.3712 | LR: 0.000100\n",
            "Epoch 16/100 | Loss: 0.3530 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3530)\n",
            "Epoch 17/100 | Loss: 0.3522 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3522)\n",
            "Epoch 18/100 | Loss: 0.3366 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3366)\n",
            "Epoch 19/100 | Loss: 0.3221 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3221)\n",
            "Epoch 20/100 | Loss: 0.3204 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.3204)\n",
            "Epoch 21/100 | Loss: 0.3501 | LR: 0.000100\n",
            "Epoch 22/100 | Loss: 0.3365 | LR: 0.000100\n",
            "Epoch 23/100 | Loss: 0.3260 | LR: 0.000100\n",
            "Epoch 24/100 | Loss: 0.3324 | LR: 0.000100\n",
            "Epoch 25/100 | Loss: 0.3379 | LR: 0.000100\n",
            "Epoch 26/100 | Loss: 0.2983 | LR: 0.000100\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2983)\n",
            "Epoch 27/100 | Loss: 0.3149 | LR: 0.000100\n",
            "Epoch 28/100 | Loss: 0.3285 | LR: 0.000100\n",
            "Epoch 29/100 | Loss: 0.3125 | LR: 0.000100\n",
            "Epoch 30/100 | Loss: 0.3430 | LR: 0.000100\n",
            "Epoch 31/100 | Loss: 0.3200 | LR: 0.000100\n",
            "Epoch 32/100 | Loss: 0.3213 | LR: 0.000100\n",
            "Epoch 33/100 | Loss: 0.3023 | LR: 0.000100\n",
            "Epoch 34/100 | Loss: 0.3236 | LR: 0.000100\n",
            "Epoch 35/100 | Loss: 0.3309 | LR: 0.000100\n",
            "Epoch 36/100 | Loss: 0.3027 | LR: 0.000050\n",
            "Epoch 37/100 | Loss: 0.3233 | LR: 0.000050\n",
            "Epoch 38/100 | Loss: 0.2976 | LR: 0.000050\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2976)\n",
            "Epoch 39/100 | Loss: 0.2918 | LR: 0.000050\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2918)\n",
            "Epoch 40/100 | Loss: 0.3149 | LR: 0.000050\n",
            "Epoch 41/100 | Loss: 0.3002 | LR: 0.000050\n",
            "Epoch 42/100 | Loss: 0.3008 | LR: 0.000050\n",
            "Epoch 43/100 | Loss: 0.2686 | LR: 0.000050\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2686)\n",
            "Epoch 44/100 | Loss: 0.2833 | LR: 0.000050\n",
            "Epoch 45/100 | Loss: 0.2747 | LR: 0.000050\n",
            "Epoch 46/100 | Loss: 0.2791 | LR: 0.000050\n",
            "Epoch 47/100 | Loss: 0.2940 | LR: 0.000050\n",
            "Epoch 48/100 | Loss: 0.3092 | LR: 0.000050\n",
            "Epoch 49/100 | Loss: 0.2842 | LR: 0.000050\n",
            "Epoch 50/100 | Loss: 0.2917 | LR: 0.000050\n",
            "Epoch 51/100 | Loss: 0.2573 | LR: 0.000050\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2573)\n",
            "Epoch 52/100 | Loss: 0.2798 | LR: 0.000050\n",
            "Epoch 53/100 | Loss: 0.2919 | LR: 0.000050\n",
            "Epoch 54/100 | Loss: 0.3263 | LR: 0.000050\n",
            "Epoch 55/100 | Loss: 0.2647 | LR: 0.000050\n",
            "Epoch 56/100 | Loss: 0.2596 | LR: 0.000050\n",
            "Epoch 57/100 | Loss: 0.2975 | LR: 0.000050\n",
            "Epoch 58/100 | Loss: 0.2832 | LR: 0.000050\n",
            "Epoch 59/100 | Loss: 0.2984 | LR: 0.000050\n",
            "Epoch 60/100 | Loss: 0.2755 | LR: 0.000050\n",
            "Epoch 61/100 | Loss: 0.2757 | LR: 0.000025\n",
            "Epoch 62/100 | Loss: 0.2970 | LR: 0.000025\n",
            "Epoch 63/100 | Loss: 0.2673 | LR: 0.000025\n",
            "Epoch 64/100 | Loss: 0.2538 | LR: 0.000025\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2538)\n",
            "Epoch 65/100 | Loss: 0.2362 | LR: 0.000025\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2362)\n",
            "Epoch 66/100 | Loss: 0.2642 | LR: 0.000025\n",
            "Epoch 67/100 | Loss: 0.2766 | LR: 0.000025\n",
            "Epoch 68/100 | Loss: 0.2545 | LR: 0.000025\n",
            "Epoch 69/100 | Loss: 0.2779 | LR: 0.000025\n",
            "Epoch 70/100 | Loss: 0.2922 | LR: 0.000025\n",
            "Epoch 71/100 | Loss: 0.2748 | LR: 0.000025\n",
            "Epoch 72/100 | Loss: 0.2914 | LR: 0.000025\n",
            "Epoch 73/100 | Loss: 0.2654 | LR: 0.000025\n",
            "Epoch 74/100 | Loss: 0.2789 | LR: 0.000025\n",
            "Epoch 75/100 | Loss: 0.2956 | LR: 0.000013\n",
            "Epoch 76/100 | Loss: 0.2099 | LR: 0.000013\n",
            " â­ New BEST saved: /content/pitch_modelV1_old/best.pth (loss=0.2099)\n",
            "Epoch 77/100 | Loss: 0.2893 | LR: 0.000013\n",
            "Epoch 78/100 | Loss: 0.2893 | LR: 0.000013\n",
            "Epoch 79/100 | Loss: 0.2799 | LR: 0.000013\n",
            "Epoch 80/100 | Loss: 0.2494 | LR: 0.000013\n",
            "Epoch 81/100 | Loss: 0.2450 | LR: 0.000013\n",
            "Epoch 82/100 | Loss: 0.2296 | LR: 0.000013\n",
            "Epoch 83/100 | Loss: 0.2269 | LR: 0.000013\n",
            "Epoch 84/100 | Loss: 0.2464 | LR: 0.000013\n",
            "Epoch 85/100 | Loss: 0.2802 | LR: 0.000013\n",
            "Epoch 86/100 | Loss: 0.2775 | LR: 0.000006\n",
            "Epoch 87/100 | Loss: 0.2673 | LR: 0.000006\n",
            "Epoch 88/100 | Loss: 0.2837 | LR: 0.000006\n",
            "Epoch 89/100 | Loss: 0.2272 | LR: 0.000006\n",
            "Epoch 90/100 | Loss: 0.2734 | LR: 0.000006\n",
            "Epoch 91/100 | Loss: 0.2462 | LR: 0.000006\n",
            "Epoch 92/100 | Loss: 0.2576 | LR: 0.000006\n",
            "Epoch 93/100 | Loss: 0.2729 | LR: 0.000006\n",
            "Epoch 94/100 | Loss: 0.2871 | LR: 0.000006\n",
            "Epoch 95/100 | Loss: 0.2594 | LR: 0.000003\n",
            "Epoch 96/100 | Loss: 0.2674 | LR: 0.000003\n",
            "Epoch 97/100 | Loss: 0.2670 | LR: 0.000003\n",
            "Epoch 98/100 | Loss: 0.2959 | LR: 0.000003\n",
            "Epoch 99/100 | Loss: 0.2591 | LR: 0.000003\n",
            "Epoch 100/100 | Loss: 0.2504 | LR: 0.000003\n",
            "Training finished.\n",
            "Best checkpoint: /content/pitch_modelV1_old/best.pth\n",
            "Latest checkpoint: /content/pitch_modelV1_old/last.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD EVAL"
      ],
      "metadata": {
        "id": "wEz475DhHUvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import scipy.signal as sg\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"                  # UPDATED: Points to your 400 .npy files\n",
        "MODEL_PATH = \"/content/pitch_modelV1_old/best.pth\" # Your trained model\n",
        "\n",
        "# Windowing Params\n",
        "WIN_LEN = 300      # 3 seconds (at 100Hz)\n",
        "HOP_LEN = 150      # 1.5 seconds overlap\n",
        "TOLERANCE = 1.0    # Time bucket tolerance in seconds (for voting)\n",
        "TOP_K_MATCHES = 20 # How many candidates to check per window\n",
        "\n",
        "# Eval Params\n",
        "NUM_TRIALS = 150   # How many random songs to test\n",
        "SEGMENT_LEN = 1500 # 15 seconds of hum (approx)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL ARCHITECTURE (Must match training)\n",
        "# ======================================================\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "# Load Model\n",
        "print(f\"â³ Loading model from {MODEL_PATH}...\")\n",
        "model = PitchSiameseNet(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    # Handle if state dict is nested or direct\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 2. AUGMENTATION UTILS (Soft & Hard)\n",
        "# ======================================================\n",
        "\n",
        "def augment_soft(pitch):\n",
        "    \"\"\"Simulates a good singer (slight pitch wobble)\"\"\"\n",
        "    arr = pitch.copy().astype(np.float32)\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr)) # Light noise\n",
        "    return arr\n",
        "\n",
        "def augment_hard(pitch):\n",
        "    \"\"\"Simulates a hum: Key shift, Time stretch, Jitter\"\"\"\n",
        "    arr = pitch.copy().astype(np.float32)\n",
        "\n",
        "    # 1. Jitter\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    # 2. Key Shift (Simulated by adding semitones if log-scale/cents, or linear shift)\n",
        "    # Assuming standard augmentation logic from training\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Time Warp (Linear interpolation)\n",
        "    if random.random() < 0.8:\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "    # Force back to original length (crop or pad)\n",
        "    target = len(pitch)\n",
        "    if len(arr) < target:\n",
        "        arr = np.pad(arr, (0, target - len(arr)), mode='constant')\n",
        "    else:\n",
        "        start = (len(arr) - target) // 2\n",
        "        arr = arr[start:start+target]\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. EMBEDDING UTILS\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"\n",
        "    Takes a full pitch array (Song or Hum).\n",
        "    Returns: Tensor of embeddings, List of timestamps (offsets in seconds)\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "\n",
        "        # Skip if mostly silence (optional, keeps DB clean)\n",
        "        if np.mean(crop > 0) < 0.1:\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0) # Assuming 100Hz sample rate = 0.01s per frame\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    # Batch process for speed\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE) # (B, 1, 300)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.forward_one(windows_tensor)\n",
        "\n",
        "    return embeddings, offsets # (B, 128), List[float]\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 4. BUILD DATABASE (FLATTENED)\n",
        "# ======================================================\n",
        "def build_flat_database():\n",
        "    \"\"\"\n",
        "    Creates a massive tensor of all windows from all songs.\n",
        "    Returns:\n",
        "       all_embeds: Tensor (Total_Windows, 128)\n",
        "       metadata: List of (SongName, Offset_Seconds)\n",
        "    \"\"\"\n",
        "    # 1. Load files from the CORRECT directory\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "\n",
        "    # NOTE: The limit files[:250] has been REMOVED to use all files.\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Database from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    # Stack into one giant tensor for matrix multiplication\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} total windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 5. GEOMETRIC SCORING (The \"Magic\")\n",
        "# ======================================================\n",
        "def query_database_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    \"\"\"\n",
        "    1. Compares Query Windows vs ALL DB Windows.\n",
        "    2. Filters Top-K matches per window.\n",
        "    3. Aligns them using Delta T (Projected Start Time).\n",
        "    4. Votes for the best song.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate Distance Matrix (Query_Size x DB_Size)\n",
        "    # Using CDIST (Euclidean)\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Get Top K matches for each query window\n",
        "    # values: (Q, K), indices: (Q, K)\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    # 3. Voting Containers\n",
        "    # Key: (SongID, BucketIndex) -> Value: Score\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4 # Avoid div by zero\n",
        "\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            # Retrieve DB Info\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # Calculate Projected Start Time (The \"Alignment\")\n",
        "            # If match is true: MatchTime - QueryTime should be constant (the song start)\n",
        "            projected_start = match_time - q_time\n",
        "\n",
        "            # Quantize into Buckets (Rounding to nearest tolerance)\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score Weighting\n",
        "            # Closer vectors = Higher Score\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Aggregate Scores per Song\n",
        "    # We take the MAX bucket score for each song (best alignment)\n",
        "    song_final_scores = defaultdict(float)\n",
        "\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    # 5. Sort Results\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs] # Return list of song IDs\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 6. MAIN EVALUATION LOOP\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Build DB once\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting Evaluation: {NUM_TRIALS} Trials\")\n",
        "    print(f\"   Using {TOP_K_MATCHES} neighbors per window with Geometric Scoring.\")\n",
        "\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        # Pick Random Target\n",
        "        target_song = random.choice(song_list)\n",
        "\n",
        "        # Load Full File\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "        if len(full_arr) < SEGMENT_LEN: continue # Skip if too short\n",
        "\n",
        "        # Create Random Crop (The \"Truth\")\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "        clean_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # -----------------------------\n",
        "        # TEST 1: SOFT AUGMENTATION\n",
        "        # -----------------------------\n",
        "        soft_hum = augment_soft(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_database_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "\n",
        "            if len(ranked) > 0:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1\n",
        "\n",
        "        # -----------------------------\n",
        "        # TEST 2: HARD AUGMENTATION\n",
        "        # -----------------------------\n",
        "        hard_hum = augment_hard(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_database_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "\n",
        "            if len(ranked) > 0:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"ðŸ“Š FINAL EVALUATION RESULTS\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    print(f\"\\nðŸŽ¤ SOFT AUGMENTATION (Good Singing)\")\n",
        "    print(f\"   Top-1 Accuracy:  {results['Soft']['top1']/NUM_TRIALS:.2%}\")\n",
        "    print(f\"   Top-5 Accuracy:  {results['Soft']['top5']/NUM_TRIALS:.2%}\")\n",
        "    print(f\"   Top-10 Accuracy: {results['Soft']['top10']/NUM_TRIALS:.2%}\")\n",
        "\n",
        "    print(f\"\\nðŸ”¥ HARD AUGMENTATION (Humming/Noise)\")\n",
        "    print(f\"   Top-1 Accuracy:  {results['Hard']['top1']/NUM_TRIALS:.2%}\")\n",
        "    print(f\"   Top-5 Accuracy:  {results['Hard']['top5']/NUM_TRIALS:.2%}\")\n",
        "    print(f\"   Top-10 Accuracy: {results['Hard']['top10']/NUM_TRIALS:.2%}\")\n",
        "    print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soz-nExpHUbu",
        "outputId": "e284ef88-7bd7-4fd0-f213-1638897f65cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading model from /content/pitch_modelV1_old/best.pth...\n",
            "âœ… Model loaded successfully.\n",
            "ðŸ—ï¸ Building Database from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:01<00:00, 337.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 total windows across 400 songs.\n",
            "\n",
            "ðŸš€ Starting Evaluation: 150 Trials\n",
            "   Using 20 neighbors per window with Geometric Scoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 158.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "ðŸ“Š FINAL EVALUATION RESULTS\n",
            "========================================\n",
            "\n",
            "ðŸŽ¤ SOFT AUGMENTATION (Good Singing)\n",
            "   Top-1 Accuracy:  45.33%\n",
            "   Top-5 Accuracy:  54.00%\n",
            "   Top-10 Accuracy: 56.00%\n",
            "\n",
            "ðŸ”¥ HARD AUGMENTATION (Humming/Noise)\n",
            "   Top-1 Accuracy:  11.33%\n",
            "   Top-5 Accuracy:  18.67%\n",
            "   Top-10 Accuracy: 25.33%\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¤ SOFT AUGMENTATION (Good Singing)\n",
        "          Top-1 Accuracy:  45.33%\n",
        "          Top-5 Accuracy:  54.00%\n",
        "          Top-10 Accuracy: 56.00%\n",
        "\n",
        "ðŸ”¥ HARD AUGMENTATION (Humming/Noise)\n",
        "          Top-1 Accuracy:  11.33%\n",
        "          Top-5 Accuracy:  18.67%\n",
        "          Top-10 Accuracy: 25.33%\n"
      ],
      "metadata": {
        "id": "S_iotuJJ9Ivo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+SMOOTHING"
      ],
      "metadata": {
        "id": "xCu_6JoyIaWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREPE is accurate but \"jittery.\" It often produces tiny, instantaneous spikes (octave errors or noise) that aren't actually part of the melody.\n",
        "\n",
        "The Fix: A Median Filter smoothes out these jagged edges.\n",
        "\n",
        "The Impact: Your model stops learning \"This song has a weird spike at frame 50\" (which is an artifact) and starts learning \"This song goes up and then down\" (the actual melody). This makes the embeddings much cleaner\n",
        "\n",
        "      Layer,Operation,Output Shape,What it does\n",
        "      1,\"Conv1d(1, 32, k=5)\",\"(Batch, 32, 300)\",Finds local slopes (is pitch rising?)\n",
        "      2,MaxPool1d(2),\"(Batch, 32, 150)\",Downsamples (ignores minor timing errors)\n",
        "      3,\"Conv1d(32, 64, k=5)\",\"(Batch, 64, 150)\",\"Finds patterns (vibrato, trills)\"\n",
        "      4,MaxPool1d(2),\"(Batch, 64, 75)\",Downsamples again\n",
        "      5,\"Conv1d(64, 128, k=3)\",\"(Batch, 128, 75)\",Finds longer phrases\n",
        "      6,AdaptiveAvgPool1d(1),\"(Batch, 128, 1)\",The Bottleneck: Squashes time dimension completely.\n",
        "      7,\"Linear(128, 128)\",\"(Batch, 128)\",Refines the features.\n",
        "      8,\"Linear(128, 128)\",\"(Batch, 128)\",Final Embedding Vector."
      ],
      "metadata": {
        "id": "0VjbY8Y2am0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PITCH MODEL V4: ORIGINAL CNN + SMOOTHING + ADAPTIVE LR\n",
        "# Trains on: /content/data_unique\n",
        "# Saves to:  /content/pitch_modelV1_oldplussmoothing\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # <--- Adaptive LR\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams\n",
        "# -------------------------\n",
        "TARGET_LEN = 300   # 3 seconds (approx)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 110       # <--- Updated to 110\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV1_oldplussmoothing\" # Final save location\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# 1. SMOOTHING HELPER\n",
        "# -------------------------\n",
        "def smooth_pitch(pitch):\n",
        "    \"\"\"\n",
        "    Global smoothing: Median filter to remove jagged tracking errors.\n",
        "    \"\"\"\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# 2. AUGMENTATION\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy()\n",
        "\n",
        "    # 1. More Noise (Harder to see the line)\n",
        "    pitch += np.random.normal(0, 0.1, size=len(pitch))\n",
        "\n",
        "    # 2. Key Shift (Unchanged)\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Aggressive Time Warp (ALWAYS HAPPENS)\n",
        "    # Range 0.7 to 1.4 makes it stretch/squash significantly\n",
        "    rate = np.random.uniform(0.7, 1.4)\n",
        "    old_idx = np.arange(len(pitch))\n",
        "    new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "    pitch = np.interp(new_idx, old_idx, pitch)\n",
        "\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: Pad/Crop\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None or len(arr) == 0:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "\n",
        "    if len(arr) < target_len:\n",
        "        pad_amt = target_len - len(arr)\n",
        "        return np.pad(arr, (0, pad_amt), mode='constant')\n",
        "\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# 3. DATASET\n",
        "# -------------------------\n",
        "class PitchDatasetV4(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted(glob.glob(os.path.join(pitch_dir, \"*.npy\")))\n",
        "        self.target_len = target_len\n",
        "        print(f\"âœ… Loaded {len(self.files)} files from {pitch_dir}\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return arr\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        # 1. CROP\n",
        "        anchor_raw = self._random_crop(anchor_full)\n",
        "        neg_raw = self._random_crop(neg_full)\n",
        "\n",
        "        # 2. GLOBAL SMOOTHING (Apply to everything)\n",
        "        anchor_clean = smooth_pitch(anchor_raw)\n",
        "        neg_clean = smooth_pitch(neg_raw)\n",
        "\n",
        "        # 3. AUGMENT\n",
        "        # Create positive from the smoothed anchor\n",
        "        pos_hum = augment_hum(anchor_clean)\n",
        "\n",
        "        # 4. FINALIZE\n",
        "        anchor_out = force_length(anchor_clean, self.target_len)\n",
        "        pos_hum_out = force_length(pos_hum, self.target_len)\n",
        "        neg_out = force_length(neg_clean, self.target_len)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(anchor_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(pos_hum_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(neg_out).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# 4. MODEL (ORIGINAL CNN)\n",
        "# -------------------------\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        # Original 3-layer architecture\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1) # L2 normalization\n",
        "\n",
        "# -------------------------\n",
        "# 5. TRAINING LOOP\n",
        "# -------------------------\n",
        "def train_v4():\n",
        "    print(f\"ðŸš€ Training V4 (Original CNN + Smooth + AdaptiveLR) on: {DEVICE}\")\n",
        "    print(f\"ðŸ“‚ Data: {PITCH_DIR}\")\n",
        "    print(f\"ðŸ”§ Epochs: {EPOCHS} | Margin: 0.85\")\n",
        "\n",
        "    dataset = PitchDatasetV4(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchSiameseNet().to(DEVICE)\n",
        "    initial_lr = 0.0001\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
        "\n",
        "    # ADAPTIVE LR SCHEDULER (verbose=True added)\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.85, p=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"Starting LR: {initial_lr}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for anchor, pos_hum, neg in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg = neg.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            a_emb = model.forward_one(anchor)\n",
        "            p_emb = model.forward_one(pos_hum)\n",
        "            n_emb = model.forward_one(neg)\n",
        "\n",
        "            loss = loss_fn(a_emb, p_emb, n_emb)\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "        print(f\"\\nSummary | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Step the scheduler, observing the current average loss\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New Best: {best_loss:.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Training finished.\")\n",
        "    print(f\"Best checkpoint: {BEST}\")\n",
        "    print(f\"Latest checkpoint: {LAST}\")\n",
        "\n",
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    train_v4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku22055OIQqF",
        "outputId": "258df347-6247-4975-f73b-56f371b5ec9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training V4 (Original CNN + Smooth + AdaptiveLR) on: cuda\n",
            "ðŸ“‚ Data: /content/data_unique\n",
            "ðŸ”§ Epochs: 110 | Margin: 0.85\n",
            "âœ… Loaded 4051 files from /content/data_unique\n",
            "Starting LR: 0.0001\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:08<00:00, 15.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.3026 | LR: 0.000100\n",
            " â­ New Best: 0.3026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:06<00:00, 18.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1840 | LR: 0.000100\n",
            " â­ New Best: 0.1840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1435 | LR: 0.000100\n",
            " â­ New Best: 0.1435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1303 | LR: 0.000100\n",
            " â­ New Best: 0.1303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1313 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1203 | LR: 0.000100\n",
            " â­ New Best: 0.1203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1060 | LR: 0.000100\n",
            " â­ New Best: 0.1060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1118 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1012 | LR: 0.000100\n",
            " â­ New Best: 0.1012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0906 | LR: 0.000100\n",
            " â­ New Best: 0.0906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.1021 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0875 | LR: 0.000100\n",
            " â­ New Best: 0.0875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 31.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0913 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0955 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0889 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0841 | LR: 0.000100\n",
            " â­ New Best: 0.0841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0796 | LR: 0.000100\n",
            " â­ New Best: 0.0796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0803 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0788 | LR: 0.000100\n",
            " â­ New Best: 0.0788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0739 | LR: 0.000100\n",
            " â­ New Best: 0.0739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0755 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0687 | LR: 0.000100\n",
            " â­ New Best: 0.0687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0748 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0620 | LR: 0.000100\n",
            " â­ New Best: 0.0620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0729 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0831 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0771 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0706 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0647 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0726 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0647 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 31.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0647 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0747 | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0666 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0625 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0645 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0698 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0718 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0558 | LR: 0.000050\n",
            " â­ New Best: 0.0558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0606 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0540 | LR: 0.000050\n",
            " â­ New Best: 0.0540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0661 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0576 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0652 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0467 | LR: 0.000050\n",
            " â­ New Best: 0.0467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0608 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0533 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0521 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0488 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0521 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0620 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0595 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0601 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 54/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0525 | LR: 0.000050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 55/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0504 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0601 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0454 | LR: 0.000025\n",
            " â­ New Best: 0.0454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 58/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0529 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 59/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0445 | LR: 0.000025\n",
            " â­ New Best: 0.0445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 60/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0531 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0594 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 62/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0531 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 63/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0485 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 64/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0481 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 65/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0500 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0643 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 67/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0527 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 31.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0526 | LR: 0.000025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0516 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0541 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0426 | LR: 0.000013\n",
            " â­ New Best: 0.0426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 72/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0481 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 73/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0575 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 74/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0505 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:05<00:00, 23.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0511 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0490 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 77/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0687 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 78/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0475 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 30.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0514 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0518 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0381 | LR: 0.000006\n",
            " â­ New Best: 0.0381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 82/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0434 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0489 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0424 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0448 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0429 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 87/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0445 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 88/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0379 | LR: 0.000006\n",
            " â­ New Best: 0.0379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 89/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 31.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0385 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0446 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0506 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 92/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 31.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0481 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 93/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0454 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 94/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0421 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 95/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 31.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0500 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0484 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 97/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0481 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 98/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0515 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 99/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 28.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0470 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0511 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 101/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0434 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 102/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0421 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 103/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0467 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 104/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0436 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 105/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0434 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 106/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0509 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 107/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 33.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0497 | LR: 0.000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 108/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:04<00:00, 27.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0438 | LR: 0.000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 109/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0481 | LR: 0.000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 110/110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127/127 [00:03<00:00, 32.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary | Loss: 0.0460 | LR: 0.000002\n",
            "----------------------------------------\n",
            "Training finished.\n",
            "Best checkpoint: /content/pitch_modelV1_oldplussmoothing/best.pth\n",
            "Latest checkpoint: /content/pitch_modelV1_oldplussmoothing/last.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+SMOOTHING EVAL"
      ],
      "metadata": {
        "id": "HSJs6oXBMHxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVAL V4: GEOMETRIC SCORING + SMOOTHING (Corrected for /content/eval)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION (UPDATED)\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"  # <--- CORRECTED: Using the folder with 400 unseen files\n",
        "MODEL_PATH = \"/content/pitch_modelV1_oldplussmoothing/best.pth\" # Points to V4 model\n",
        "\n",
        "# Params\n",
        "WIN_LEN = 300\n",
        "HOP_LEN = 150\n",
        "TOLERANCE = 1.0    # Time bucket tolerance\n",
        "TOP_K_MATCHES = 20\n",
        "NUM_TRIALS = 400   # <--- INCREASED: Matching the number of songs for full coverage\n",
        "SEGMENT_LEN = 1500 # 15 seconds\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL (V4 Architecture)\n",
        "# ======================================================\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "print(f\"â³ Loading model from {MODEL_PATH}...\")\n",
        "model = PitchSiameseNet(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "# ======================================================\n",
        "# 2. SMOOTHING (Must match Training V4)\n",
        "# ======================================================\n",
        "def smooth_pitch(pitch):\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# ======================================================\n",
        "# 3. AUGMENTATION\n",
        "# ======================================================\n",
        "def humify_soft(arr):\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr))\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def humify_hard(arr):\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # --- ADDED: Time Warp Logic for Consistency ---\n",
        "    if random.random() < 0.8:\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "        # Force back to original length (SEGMENT_LEN=1500)\n",
        "        target = 1500  # Based on SEGMENT_LEN\n",
        "        if len(arr) < target:\n",
        "            arr = np.pad(arr, (0, target - len(arr)), mode='constant')\n",
        "        else:\n",
        "            start = (len(arr) - target) // 2\n",
        "            arr = arr[start:start+target]\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "# ======================================================\n",
        "# 4. EMBEDDING + DB BUILDER (FLAT)\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"Returns embeddings and time offsets.\"\"\"\n",
        "    # 1. APPLY GLOBAL SMOOTHING FIRST\n",
        "    arr = smooth_pitch(arr)\n",
        "\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "        if np.mean(crop > 0) < 0.1: # Skip silence\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0)\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.forward_one(windows_tensor)\n",
        "\n",
        "    return embeddings, offsets\n",
        "\n",
        "def build_flat_database():\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "    # FILE LIMIT REMOVED: All found files are used for the database\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Geometric DB from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        # All DB files are smoothed and windowed\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "# ======================================================\n",
        "# 5. GEOMETRIC SCORING\n",
        "# ======================================================\n",
        "def query_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    # 1. Distance Matrix\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Top-K\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4\n",
        "\n",
        "    #\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # 3. Geometric Alignment (Projected Start)\n",
        "            # This aligns all time windows to a single projected start time for the song\n",
        "            projected_start = match_time - q_time\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score weighted by inverse distance\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Max Score per Song (Max vote over all possible start alignments)\n",
        "    song_final_scores = defaultdict(float)\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs]\n",
        "\n",
        "# ======================================================\n",
        "# 6. RUN EVAL\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    # Added \"top10\" to results dictionary\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸš€ Running {NUM_TRIALS} Trials with Geometric Scoring...\")\n",
        "\n",
        "    # Total successful trials counter\n",
        "    effective_trials = {\"Soft\": 0, \"Hard\": 0}\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        target_song = random.choice(song_list)\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "\n",
        "        if len(full_arr) < SEGMENT_LEN: continue\n",
        "\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "        clean_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # --- Test Soft ---\n",
        "        soft_hum_clip = humify_soft(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Soft\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1 # <--- Added Top 10 check\n",
        "\n",
        "        # --- Test Hard ---\n",
        "        hard_hum_clip = humify_hard(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Hard\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1 # <--- Added Top 10 check\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š V4 FINAL RESULTS (Using 400 Unseen Files)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    def calc_acc(res, key, total):\n",
        "        return res[key] / total if total > 0 else 0\n",
        "\n",
        "    print(f\"Total Effective Soft Trials: {effective_trials['Soft']}\")\n",
        "    print(f\"Total Effective Hard Trials: {effective_trials['Hard']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"ðŸŽ¤ Soft Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Soft'], 'top1', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Soft'], 'top5', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Soft'], 'top10', effective_trials['Soft']):.1%}\") # <--- Added Top 10 Report\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Hard Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Hard'], 'top1', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Hard'], 'top5', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Hard'], 'top10', effective_trials['Hard']):.1%}\") # <--- Added Top 10 Report\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQWgz73ELtRP",
        "outputId": "16cd135b-4d0f-4eeb-f7e6-d5f4bd6f549d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading model from /content/pitch_modelV1_oldplussmoothing/best.pth...\n",
            "âœ… Model loaded.\n",
            "ðŸ—ï¸ Building Geometric DB from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:01<00:00, 299.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 windows across 400 songs.\n",
            "\n",
            "ðŸš€ Running 400 Trials with Geometric Scoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 152.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š V4 FINAL RESULTS (Using 400 Unseen Files)\n",
            "==================================================\n",
            "Total Effective Soft Trials: 400\n",
            "Total Effective Hard Trials: 400\n",
            "--------------------------------------------------\n",
            "ðŸŽ¤ Soft Hum:\n",
            "   Top-1:  54.2%\n",
            "   Top-5:  60.0%\n",
            "   Top-10: 62.5%\n",
            "\n",
            "ðŸ”¥ Hard Hum:\n",
            "   Top-1:  31.0%\n",
            "   Top-5:  42.8%\n",
            "   Top-10: 47.5%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------\n",
        "ðŸŽ¤ Soft Hum:\n",
        "   Top-1:  54.2%\n",
        "   Top-5:  60.0%\n",
        "   Top-10: 62.5%\n",
        "\n",
        "ðŸ”¥ Hard Hum:\n",
        "   Top-1:  31.0%\n",
        "   Top-5:  42.8%\n",
        "   Top-10: 47.5%\n"
      ],
      "metadata": {
        "id": "COf_wBiu9WxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+deeper CNN"
      ],
      "metadata": {
        "id": "3zPDv7vE8J71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Change: You added a 4th Convolutional Layer and increased the channel depth to 256.\n",
        "\n",
        "Old (V3/V4): Conv(32) -> Conv(64) -> Conv(128) -> Output\n",
        "\n",
        "New (V3.6): Conv(32) -> Conv(64) -> Conv(128) -> **Conv(256)** -> Output\n",
        "\n",
        "Shallow networks (3 layers) learn simple shapes (lines going up/down). Deeper networks (4+ layers) can learn complex patterns of patterns (e.g., a specific vibrato style or a repeating melodic motif)\n",
        "\n",
        "SMOOTHING REMOVED IN THIS VERSION"
      ],
      "metadata": {
        "id": "xpvX_TgCcEzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V3.6: DEEPER CNN (4-LAYER) | NO SMOOTHING | ADAPTIVE LR | 100 EPOCHS\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams\n",
        "# -------------------------\n",
        "TARGET_LEN = 300   # 3 seconds\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100       # 100 Epochs\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV3_6_nosmoothdeepcnn\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# 1. AUGMENTATION\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy().astype(np.float32)\n",
        "\n",
        "    # 1. Noise\n",
        "    pitch += np.random.normal(0, 0.06, size=len(pitch))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Time Warp\n",
        "    if random.random() < 0.7:\n",
        "        rate = np.random.uniform(0.8, 1.25)\n",
        "        old_idx = np.arange(len(pitch))\n",
        "        new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "        pitch = np.interp(new_idx, old_idx, pitch)\n",
        "\n",
        "    # 4. Breath Noise\n",
        "    pitch += np.random.normal(0, 0.04, size=len(pitch))\n",
        "\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: Pad/Crop\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None or len(arr) == 0:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "    if len(arr) < target_len:\n",
        "        pad_amt = target_len - len(arr)\n",
        "        return np.pad(arr, (0, pad_amt), mode='constant')\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# 2. DATASET (NO SMOOTHING)\n",
        "# -------------------------\n",
        "class PitchDatasetV3(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted(glob.glob(os.path.join(pitch_dir, \"*.npy\")))\n",
        "        self.target_len = target_len\n",
        "        print(f\"âœ… Loaded {len(self.files)} files\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return arr\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Raw Data\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        # --- NO SMOOTHING APPLIED HERE ---\n",
        "\n",
        "        # 1. CROP\n",
        "        anchor_clean = self._random_crop(anchor_full)\n",
        "        neg_clean = self._random_crop(neg_full)\n",
        "\n",
        "        # 2. AUGMENT\n",
        "        positive_hum = augment_hum(anchor_clean)\n",
        "        negative_hum = augment_hum(neg_clean)\n",
        "\n",
        "        # 3. PAD/TRUNCATE\n",
        "        a_out = force_length(anchor_clean, self.target_len)\n",
        "        ph_out = force_length(positive_hum, self.target_len)\n",
        "        n_out = force_length(negative_hum, self.target_len)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(a_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(ph_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(n_out).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# 3. MODEL: DEEPER 4-LAYER CNN\n",
        "# -------------------------\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32), nn.ReLU(),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            # Layer 3\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "\n",
        "            # Layer 4 (The \"Deep\" Part)\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), # Input 256 matches CNN output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)   # (B, 256)\n",
        "        x = self.fc(x)               # (B, 128)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "# -------------------------\n",
        "# 4. TRAINING LOOP\n",
        "# -------------------------\n",
        "def train_deeper_nosmooth():\n",
        "    print(f\"ðŸš€ Training V3.6: DEEPER CNN (4-Layer) | NO SMOOTHING | {DEVICE}\")\n",
        "    print(f\"ðŸ“‚ Data: {PITCH_DIR}\")\n",
        "    print(f\"ðŸ”§ Epochs: {EPOCHS} | Adaptive LR: ON | Margin: 0.85\")\n",
        "\n",
        "    dataset = PitchDatasetV3(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchSiameseNet().to(DEVICE)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # ADAPTIVE LR SCHEDULER\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    # SINGLE ROBUST TRIPLET LOSS\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.85, p=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for anchor, pos_hum, neg_hum in loader:\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg_hum = neg_hum.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            a = model.forward_one(anchor)\n",
        "            ph = model.forward_one(pos_hum)\n",
        "            n = model.forward_one(neg_hum)\n",
        "\n",
        "            loss = loss_fn(a, ph, n)\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New Best: {best_loss:.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"âœ… Training Complete.\")\n",
        "    print(f\"Best Model Saved to: {BEST}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_deeper_nosmooth()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj8zJMTe8HZE",
        "outputId": "9f215e80-9746-43f9-9314-63043e0f6e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training V3.6: DEEPER CNN (4-Layer) | NO SMOOTHING | cuda\n",
            "ðŸ“‚ Data: /content/data_unique\n",
            "ðŸ”§ Epochs: 100 | Adaptive LR: ON | Margin: 0.85\n",
            "âœ… Loaded 4051 files\n",
            "Epoch 1/100 | Avg Loss: 0.2489 | LR: 0.000100\n",
            " â­ New Best: 0.2489\n",
            "Epoch 2/100 | Avg Loss: 0.1496 | LR: 0.000100\n",
            " â­ New Best: 0.1496\n",
            "Epoch 3/100 | Avg Loss: 0.1125 | LR: 0.000100\n",
            " â­ New Best: 0.1125\n",
            "Epoch 4/100 | Avg Loss: 0.1030 | LR: 0.000100\n",
            " â­ New Best: 0.1030\n",
            "Epoch 5/100 | Avg Loss: 0.0904 | LR: 0.000100\n",
            " â­ New Best: 0.0904\n",
            "Epoch 6/100 | Avg Loss: 0.1015 | LR: 0.000100\n",
            "Epoch 7/100 | Avg Loss: 0.0781 | LR: 0.000100\n",
            " â­ New Best: 0.0781\n",
            "Epoch 8/100 | Avg Loss: 0.0794 | LR: 0.000100\n",
            "Epoch 9/100 | Avg Loss: 0.0793 | LR: 0.000100\n",
            "Epoch 10/100 | Avg Loss: 0.0601 | LR: 0.000100\n",
            " â­ New Best: 0.0601\n",
            "Epoch 11/100 | Avg Loss: 0.0747 | LR: 0.000100\n",
            "Epoch 12/100 | Avg Loss: 0.0709 | LR: 0.000100\n",
            "Epoch 13/100 | Avg Loss: 0.0586 | LR: 0.000100\n",
            " â­ New Best: 0.0586\n",
            "Epoch 14/100 | Avg Loss: 0.0576 | LR: 0.000100\n",
            " â­ New Best: 0.0576\n",
            "Epoch 15/100 | Avg Loss: 0.0600 | LR: 0.000100\n",
            "Epoch 16/100 | Avg Loss: 0.0588 | LR: 0.000100\n",
            "Epoch 17/100 | Avg Loss: 0.0504 | LR: 0.000100\n",
            " â­ New Best: 0.0504\n",
            "Epoch 18/100 | Avg Loss: 0.0413 | LR: 0.000100\n",
            " â­ New Best: 0.0413\n",
            "Epoch 19/100 | Avg Loss: 0.0435 | LR: 0.000100\n",
            "Epoch 20/100 | Avg Loss: 0.0512 | LR: 0.000100\n",
            "Epoch 21/100 | Avg Loss: 0.0567 | LR: 0.000100\n",
            "Epoch 22/100 | Avg Loss: 0.0587 | LR: 0.000100\n",
            "Epoch 23/100 | Avg Loss: 0.0442 | LR: 0.000100\n",
            "Epoch 24/100 | Avg Loss: 0.0562 | LR: 0.000100\n",
            "Epoch 25/100 | Avg Loss: 0.0485 | LR: 0.000100\n",
            "Epoch 26/100 | Avg Loss: 0.0524 | LR: 0.000100\n",
            "Epoch 27/100 | Avg Loss: 0.0486 | LR: 0.000100\n",
            "Epoch 28/100 | Avg Loss: 0.0384 | LR: 0.000050\n",
            " â­ New Best: 0.0384\n",
            "Epoch 29/100 | Avg Loss: 0.0502 | LR: 0.000050\n",
            "Epoch 30/100 | Avg Loss: 0.0343 | LR: 0.000050\n",
            " â­ New Best: 0.0343\n",
            "Epoch 31/100 | Avg Loss: 0.0461 | LR: 0.000050\n",
            "Epoch 32/100 | Avg Loss: 0.0448 | LR: 0.000050\n",
            "Epoch 33/100 | Avg Loss: 0.0487 | LR: 0.000050\n",
            "Epoch 34/100 | Avg Loss: 0.0434 | LR: 0.000050\n",
            "Epoch 35/100 | Avg Loss: 0.0364 | LR: 0.000050\n",
            "Epoch 36/100 | Avg Loss: 0.0495 | LR: 0.000050\n",
            "Epoch 37/100 | Avg Loss: 0.0397 | LR: 0.000050\n",
            "Epoch 38/100 | Avg Loss: 0.0348 | LR: 0.000050\n",
            "Epoch 39/100 | Avg Loss: 0.0446 | LR: 0.000050\n",
            "Epoch 40/100 | Avg Loss: 0.0375 | LR: 0.000025\n",
            "Epoch 41/100 | Avg Loss: 0.0310 | LR: 0.000025\n",
            " â­ New Best: 0.0310\n",
            "Epoch 42/100 | Avg Loss: 0.0296 | LR: 0.000025\n",
            " â­ New Best: 0.0296\n",
            "Epoch 43/100 | Avg Loss: 0.0294 | LR: 0.000025\n",
            " â­ New Best: 0.0294\n",
            "Epoch 44/100 | Avg Loss: 0.0370 | LR: 0.000025\n",
            "Epoch 45/100 | Avg Loss: 0.0245 | LR: 0.000025\n",
            " â­ New Best: 0.0245\n",
            "Epoch 46/100 | Avg Loss: 0.0268 | LR: 0.000025\n",
            "Epoch 47/100 | Avg Loss: 0.0385 | LR: 0.000025\n",
            "Epoch 48/100 | Avg Loss: 0.0368 | LR: 0.000025\n",
            "Epoch 49/100 | Avg Loss: 0.0308 | LR: 0.000025\n",
            "Epoch 50/100 | Avg Loss: 0.0387 | LR: 0.000025\n",
            "Epoch 51/100 | Avg Loss: 0.0364 | LR: 0.000025\n",
            "Epoch 52/100 | Avg Loss: 0.0344 | LR: 0.000025\n",
            "Epoch 53/100 | Avg Loss: 0.0362 | LR: 0.000025\n",
            "Epoch 54/100 | Avg Loss: 0.0296 | LR: 0.000025\n",
            "Epoch 55/100 | Avg Loss: 0.0287 | LR: 0.000013\n",
            "Epoch 56/100 | Avg Loss: 0.0305 | LR: 0.000013\n",
            "Epoch 57/100 | Avg Loss: 0.0342 | LR: 0.000013\n",
            "Epoch 58/100 | Avg Loss: 0.0347 | LR: 0.000013\n",
            "Epoch 59/100 | Avg Loss: 0.0243 | LR: 0.000013\n",
            " â­ New Best: 0.0243\n",
            "Epoch 60/100 | Avg Loss: 0.0310 | LR: 0.000013\n",
            "Epoch 61/100 | Avg Loss: 0.0288 | LR: 0.000013\n",
            "Epoch 62/100 | Avg Loss: 0.0316 | LR: 0.000013\n",
            "Epoch 63/100 | Avg Loss: 0.0294 | LR: 0.000013\n",
            "Epoch 64/100 | Avg Loss: 0.0292 | LR: 0.000013\n",
            "Epoch 65/100 | Avg Loss: 0.0271 | LR: 0.000013\n",
            "Epoch 66/100 | Avg Loss: 0.0231 | LR: 0.000013\n",
            " â­ New Best: 0.0231\n",
            "Epoch 67/100 | Avg Loss: 0.0217 | LR: 0.000013\n",
            " â­ New Best: 0.0217\n",
            "Epoch 68/100 | Avg Loss: 0.0238 | LR: 0.000013\n",
            "Epoch 69/100 | Avg Loss: 0.0341 | LR: 0.000013\n",
            "Epoch 70/100 | Avg Loss: 0.0311 | LR: 0.000013\n",
            "Epoch 71/100 | Avg Loss: 0.0280 | LR: 0.000013\n",
            "Epoch 72/100 | Avg Loss: 0.0247 | LR: 0.000013\n",
            "Epoch 73/100 | Avg Loss: 0.0222 | LR: 0.000013\n",
            "Epoch 74/100 | Avg Loss: 0.0311 | LR: 0.000013\n",
            "Epoch 75/100 | Avg Loss: 0.0270 | LR: 0.000013\n",
            "Epoch 76/100 | Avg Loss: 0.0298 | LR: 0.000013\n",
            "Epoch 77/100 | Avg Loss: 0.0419 | LR: 0.000006\n",
            "Epoch 78/100 | Avg Loss: 0.0250 | LR: 0.000006\n",
            "Epoch 79/100 | Avg Loss: 0.0246 | LR: 0.000006\n",
            "Epoch 80/100 | Avg Loss: 0.0277 | LR: 0.000006\n",
            "Epoch 81/100 | Avg Loss: 0.0257 | LR: 0.000006\n",
            "Epoch 82/100 | Avg Loss: 0.0251 | LR: 0.000006\n",
            "Epoch 83/100 | Avg Loss: 0.0278 | LR: 0.000006\n",
            "Epoch 84/100 | Avg Loss: 0.0307 | LR: 0.000006\n",
            "Epoch 85/100 | Avg Loss: 0.0314 | LR: 0.000006\n",
            "Epoch 86/100 | Avg Loss: 0.0230 | LR: 0.000003\n",
            "Epoch 87/100 | Avg Loss: 0.0316 | LR: 0.000003\n",
            "Epoch 88/100 | Avg Loss: 0.0280 | LR: 0.000003\n",
            "Epoch 89/100 | Avg Loss: 0.0208 | LR: 0.000003\n",
            " â­ New Best: 0.0208\n",
            "Epoch 90/100 | Avg Loss: 0.0278 | LR: 0.000003\n",
            "Epoch 91/100 | Avg Loss: 0.0294 | LR: 0.000003\n",
            "Epoch 92/100 | Avg Loss: 0.0242 | LR: 0.000003\n",
            "Epoch 93/100 | Avg Loss: 0.0190 | LR: 0.000003\n",
            " â­ New Best: 0.0190\n",
            "Epoch 94/100 | Avg Loss: 0.0278 | LR: 0.000003\n",
            "Epoch 95/100 | Avg Loss: 0.0323 | LR: 0.000003\n",
            "Epoch 96/100 | Avg Loss: 0.0251 | LR: 0.000003\n",
            "Epoch 97/100 | Avg Loss: 0.0275 | LR: 0.000003\n",
            "Epoch 98/100 | Avg Loss: 0.0284 | LR: 0.000003\n",
            "Epoch 99/100 | Avg Loss: 0.0217 | LR: 0.000003\n",
            "Epoch 100/100 | Avg Loss: 0.0234 | LR: 0.000003\n",
            "âœ… Training Complete.\n",
            "Best Model Saved to: /content/pitch_modelV3_6_nosmoothdeepcnn/best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVAL deepCNN + no SMOOTHING\n"
      ],
      "metadata": {
        "id": "c6xv-P6cui-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVAL V3.6: DEEP CNN (NO SMOOTHING) + GEOMETRIC SCORING\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"  # 400 unseen files\n",
        "# Pointing to the specific checkpoint you requested\n",
        "MODEL_PATH = \"/content/pitch_modelV3_6_nosmoothdeepcnn/best.pth\"\n",
        "\n",
        "# Params\n",
        "WIN_LEN = 300\n",
        "HOP_LEN = 150\n",
        "TOLERANCE = 1.0    # Time bucket tolerance\n",
        "TOP_K_MATCHES = 20\n",
        "NUM_TRIALS = 400   # Full coverage\n",
        "SEGMENT_LEN = 1500 # 15 seconds\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL ARCHITECTURE (V3.6 DEEP CNN)\n",
        "# ======================================================\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32), nn.ReLU(),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            # Layer 3\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "\n",
        "            # Layer 4 (The \"Deep\" Part)\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), # Input matches CNN output (256)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "print(f\"â³ Loading V3.6 Model from {MODEL_PATH}...\")\n",
        "model = PitchSiameseNet(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "# ======================================================\n",
        "# 2. AUGMENTATION (Proper Soft & Hard with Time Warp)\n",
        "# ======================================================\n",
        "def humify_soft(arr):\n",
        "    \"\"\"Soft Hum: Light Noise only.\"\"\"\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr))\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def humify_hard(arr):\n",
        "    \"\"\"Hard Hum: Noise + Key Shift + Time Warping\"\"\"\n",
        "    arr = arr.copy()\n",
        "\n",
        "    # 1. Jitter\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. TIME WARP (Crucial for robustness test)\n",
        "    target_len = SEGMENT_LEN # 1500\n",
        "    if random.random() < 0.8: # 80% chance\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "        # Force back to target length\n",
        "        if len(arr) < target_len:\n",
        "            arr = np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
        "        else:\n",
        "            start = (len(arr) - target_len) // 2\n",
        "            arr = arr[start:start+target_len]\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. EMBEDDING + DB BUILDER (NO SMOOTHING)\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"\n",
        "    Returns embeddings and time offsets.\n",
        "    NO SMOOTHING applied here (Matches training V3.6 No-Smooth).\n",
        "    Uses mini-batches to prevent OOM.\n",
        "    \"\"\"\n",
        "\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "        if np.mean(crop > 0) < 0.1: # Skip silence\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0)\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    # --- BATCH PROCESSING ---\n",
        "    batch_size = 64\n",
        "    embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for k in range(0, len(windows_tensor), batch_size):\n",
        "            batch = windows_tensor[k : k + batch_size]\n",
        "            emb_batch = model.forward_one(batch)\n",
        "            embeddings_list.append(emb_batch)\n",
        "\n",
        "    embeddings = torch.cat(embeddings_list, dim=0)\n",
        "\n",
        "    return embeddings, offsets\n",
        "\n",
        "def build_flat_database():\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Geometric DB (RAW PITCH) from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        # Raw pitch goes directly into embedding (No Smoothing)\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "# ======================================================\n",
        "# 4. GEOMETRIC SCORING\n",
        "# ======================================================\n",
        "def query_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    # 1. Distance Matrix\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Top-K\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4\n",
        "\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # 3. Geometric Alignment\n",
        "            projected_start = match_time - q_time\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Max Score per Song\n",
        "    song_final_scores = defaultdict(float)\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs]\n",
        "\n",
        "# ======================================================\n",
        "# 5. RUN EVAL\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸš€ Running {NUM_TRIALS} Trials with Geometric Scoring...\")\n",
        "\n",
        "    effective_trials = {\"Soft\": 0, \"Hard\": 0}\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        target_song = random.choice(song_list)\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "\n",
        "        if len(full_arr) < SEGMENT_LEN: continue\n",
        "\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "\n",
        "        # BASE: Raw, unsmoothed clip (Matches V3.6 No-Smooth training)\n",
        "        raw_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # --- Test Soft ---\n",
        "        soft_hum_clip = humify_soft(raw_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Soft\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1\n",
        "\n",
        "        # --- Test Hard ---\n",
        "        hard_hum_clip = humify_hard(raw_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Hard\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    def calc_acc(res, key, total):\n",
        "        return res[key] / total if total > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š V3.6 DEEP CNN (NO SMOOTHING) RESULTS (400 Files)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Effective Soft Trials: {effective_trials['Soft']}\")\n",
        "    print(f\"Total Effective Hard Trials: {effective_trials['Hard']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"ðŸŽ¤ Soft Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Soft'], 'top1', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Soft'], 'top5', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Soft'], 'top10', effective_trials['Soft']):.1%}\")\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Hard Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Hard'], 'top1', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Hard'], 'top5', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Hard'], 'top10', effective_trials['Hard']):.1%}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk1yLtFQup43",
        "outputId": "44edeae8-dfc5-4075-d02c-b251ac174e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading V3.6 Model from /content/pitch_modelV3_6_nosmoothdeepcnn/best.pth...\n",
            "âœ… Model loaded successfully.\n",
            "ðŸ—ï¸ Building Geometric DB (RAW PITCH) from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 156.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 windows across 400 songs.\n",
            "\n",
            "ðŸš€ Running 400 Trials with Geometric Scoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 146.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š V3.6 DEEP CNN (NO SMOOTHING) RESULTS (400 Files)\n",
            "==================================================\n",
            "Total Effective Soft Trials: 400\n",
            "Total Effective Hard Trials: 400\n",
            "--------------------------------------------------\n",
            "ðŸŽ¤ Soft Hum:\n",
            "   Top-1:  63.2%\n",
            "   Top-5:  67.2%\n",
            "   Top-10: 68.8%\n",
            "\n",
            "ðŸ”¥ Hard Hum:\n",
            "   Top-1:  49.2%\n",
            "   Top-5:  58.5%\n",
            "   Top-10: 61.8%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¤ Soft Hum:\n",
        "   Top-1:  63.2%\n",
        "   Top-5:  67.2%\n",
        "   Top-10: 68.8%\n",
        "\n",
        "ðŸ”¥ Hard Hum:\n",
        "   Top-1:  49.2%\n",
        "   Top-5:  58.5%\n",
        "   Top-10: 61.8%"
      ],
      "metadata": {
        "id": "640SK1dO9eIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+deeper CNN+smoothing"
      ],
      "metadata": {
        "id": "tjVsIhCL9bTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V3.6: DEEPER CNN + GLOBAL SMOOTHING + MARGIN 0.85 + ADAPTIVE LR\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams\n",
        "# -------------------------\n",
        "TARGET_LEN = 300   # 3 seconds (approx)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 120       # <--- 120 Epochs\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV1_deepCNNplussmoothing\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# 1. GLOBAL SMOOTHING HELPER\n",
        "# -------------------------\n",
        "def smooth_pitch(pitch):\n",
        "    \"\"\"\n",
        "    Applies Median Filter (k=5) to remove jagged tracking errors.\n",
        "    \"\"\"\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# 2. AUGMENTATION\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy().astype(np.float32)\n",
        "\n",
        "    # 1. Noise\n",
        "    pitch += np.random.normal(0, 0.06, size=len(pitch))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Time Warp\n",
        "    if random.random() < 0.7:\n",
        "        rate = np.random.uniform(0.8, 1.25)\n",
        "        old_idx = np.arange(len(pitch))\n",
        "        new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "        pitch = np.interp(new_idx, old_idx, pitch)\n",
        "\n",
        "    # 4. Breath Noise\n",
        "    pitch += np.random.normal(0, 0.04, size=len(pitch))\n",
        "\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: Pad/Crop\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None or len(arr) == 0:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "\n",
        "    if len(arr) < target_len:\n",
        "        pad_amt = target_len - len(arr)\n",
        "        return np.pad(arr, (0, pad_amt), mode='constant')\n",
        "\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# 3. DATASET (WITH GLOBAL SMOOTHING)\n",
        "# -------------------------\n",
        "class PitchDatasetV3(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted(glob.glob(os.path.join(pitch_dir, \"*.npy\")))\n",
        "        self.target_len = target_len\n",
        "        print(f\"âœ… Loaded {len(self.files)} files from {pitch_dir}\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return arr\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Raw\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        # --- APPLY GLOBAL SMOOTHING ---\n",
        "        anchor_full = smooth_pitch(anchor_full)\n",
        "        neg_full = smooth_pitch(neg_full)\n",
        "        # ------------------------------\n",
        "\n",
        "        # 1. CROP (Now cropping from Cleaned data)\n",
        "        anchor_clean = self._random_crop(anchor_full)\n",
        "        neg_clean = self._random_crop(neg_full)\n",
        "\n",
        "        # 2. AUGMENT\n",
        "        # Create positive hum from the smoothed positive crop\n",
        "        positive_hum = augment_hum(anchor_clean)\n",
        "\n",
        "        # Apply augmentation to negative as well (Harder Negative)\n",
        "        negative_hum = augment_hum(neg_clean)\n",
        "\n",
        "        # 3. FINALIZE\n",
        "        anchor_out = force_length(anchor_clean, self.target_len)\n",
        "        pos_hum_out = force_length(positive_hum, self.target_len)\n",
        "        neg_out = force_length(negative_hum, self.target_len)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(anchor_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(pos_hum_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(neg_out).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# 4. MODEL: DEEPER 4-LAYER CNN\n",
        "# -------------------------\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32), nn.ReLU(),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            # Layer 3\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "\n",
        "            # Layer 4 (The \"Deeper\" part)\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), # Input 256 matches CNN output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)   # (B, 256)\n",
        "        x = self.fc(x)               # (B, 128)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "# -------------------------\n",
        "# 5. TRAINING LOOP\n",
        "# -------------------------\n",
        "def train_deeper_smooth():\n",
        "    print(f\"ðŸš€ Training Deeper CNN (4-Layer) on: {DEVICE}\")\n",
        "    print(f\"ðŸ“‚ Data: {PITCH_DIR}\")\n",
        "    print(f\"ðŸ”§ Margin: 0.85 | Smoothing: ON | Adaptive LR: ON\")\n",
        "\n",
        "    dataset = PitchDatasetV3(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    # Using num_workers=0 for safety in Colab/Notebooks\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchSiameseNet().to(DEVICE)\n",
        "    initial_lr = 0.0001\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
        "\n",
        "    # ADAPTIVE LR SCHEDULER (Reduced Factor=0.5, Patience=8)\n",
        "    # verbose=True removed for compatibility with newer PyTorch\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    # FIXED MARGIN 0.85\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.85, p=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"Starting LR: {initial_lr}\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "\n",
        "        for anchor, pos_hum, neg_hum in loader:\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg_hum = neg_hum.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            a = model.forward_one(anchor)\n",
        "            ph = model.forward_one(pos_hum)\n",
        "            n = model.forward_one(neg_hum)\n",
        "\n",
        "            # Single Loss: Anchor vs Hummed Positive vs Hummed Negative\n",
        "            loss = loss_fn(a, ph, n)\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            # pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New Best: {best_loss:.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"âœ… Training Complete.\")\n",
        "    print(f\"Best Model Saved to: {BEST}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_deeper_smooth()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv8icxmt9bAq",
        "outputId": "d6004d1b-bebd-4c91-fa2e-3eab0f54d0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training Deeper CNN (4-Layer) on: cuda\n",
            "ðŸ“‚ Data: /content/data_unique\n",
            "ðŸ”§ Margin: 0.85 | Smoothing: ON | Adaptive LR: ON\n",
            "âœ… Loaded 4051 files from /content/data_unique\n",
            "Starting LR: 0.0001\n",
            "Epoch 1/120 | Avg Loss: 0.2640 | LR: 0.000100\n",
            " â­ New Best: 0.2640\n",
            "Epoch 2/120 | Avg Loss: 0.1497 | LR: 0.000100\n",
            " â­ New Best: 0.1497\n",
            "Epoch 3/120 | Avg Loss: 0.1091 | LR: 0.000100\n",
            " â­ New Best: 0.1091\n",
            "Epoch 4/120 | Avg Loss: 0.1038 | LR: 0.000100\n",
            " â­ New Best: 0.1038\n",
            "Epoch 5/120 | Avg Loss: 0.1022 | LR: 0.000100\n",
            " â­ New Best: 0.1022\n",
            "Epoch 6/120 | Avg Loss: 0.0912 | LR: 0.000100\n",
            " â­ New Best: 0.0912\n",
            "Epoch 7/120 | Avg Loss: 0.0747 | LR: 0.000100\n",
            " â­ New Best: 0.0747\n",
            "Epoch 8/120 | Avg Loss: 0.0729 | LR: 0.000100\n",
            " â­ New Best: 0.0729\n",
            "Epoch 9/120 | Avg Loss: 0.0822 | LR: 0.000100\n",
            "Epoch 10/120 | Avg Loss: 0.0824 | LR: 0.000100\n",
            "Epoch 11/120 | Avg Loss: 0.0705 | LR: 0.000100\n",
            " â­ New Best: 0.0705\n",
            "Epoch 12/120 | Avg Loss: 0.0770 | LR: 0.000100\n",
            "Epoch 13/120 | Avg Loss: 0.0667 | LR: 0.000100\n",
            " â­ New Best: 0.0667\n",
            "Epoch 14/120 | Avg Loss: 0.0625 | LR: 0.000100\n",
            " â­ New Best: 0.0625\n",
            "Epoch 15/120 | Avg Loss: 0.0663 | LR: 0.000100\n",
            "Epoch 16/120 | Avg Loss: 0.0584 | LR: 0.000100\n",
            " â­ New Best: 0.0584\n",
            "Epoch 17/120 | Avg Loss: 0.0624 | LR: 0.000100\n",
            "Epoch 18/120 | Avg Loss: 0.0557 | LR: 0.000100\n",
            " â­ New Best: 0.0557\n",
            "Epoch 19/120 | Avg Loss: 0.0678 | LR: 0.000100\n",
            "Epoch 20/120 | Avg Loss: 0.0696 | LR: 0.000100\n",
            "Epoch 21/120 | Avg Loss: 0.0516 | LR: 0.000100\n",
            " â­ New Best: 0.0516\n",
            "Epoch 22/120 | Avg Loss: 0.0516 | LR: 0.000100\n",
            " â­ New Best: 0.0516\n",
            "Epoch 23/120 | Avg Loss: 0.0483 | LR: 0.000100\n",
            " â­ New Best: 0.0483\n",
            "Epoch 24/120 | Avg Loss: 0.0463 | LR: 0.000100\n",
            " â­ New Best: 0.0463\n",
            "Epoch 25/120 | Avg Loss: 0.0500 | LR: 0.000100\n",
            "Epoch 26/120 | Avg Loss: 0.0535 | LR: 0.000100\n",
            "Epoch 27/120 | Avg Loss: 0.0566 | LR: 0.000100\n",
            "Epoch 28/120 | Avg Loss: 0.0541 | LR: 0.000100\n",
            "Epoch 29/120 | Avg Loss: 0.0495 | LR: 0.000100\n",
            "Epoch 30/120 | Avg Loss: 0.0470 | LR: 0.000100\n",
            "Epoch 31/120 | Avg Loss: 0.0427 | LR: 0.000100\n",
            " â­ New Best: 0.0427\n",
            "Epoch 32/120 | Avg Loss: 0.0456 | LR: 0.000100\n",
            "Epoch 33/120 | Avg Loss: 0.0474 | LR: 0.000100\n",
            "Epoch 34/120 | Avg Loss: 0.0425 | LR: 0.000100\n",
            " â­ New Best: 0.0425\n",
            "Epoch 35/120 | Avg Loss: 0.0517 | LR: 0.000100\n",
            "Epoch 36/120 | Avg Loss: 0.0429 | LR: 0.000100\n",
            "Epoch 37/120 | Avg Loss: 0.0416 | LR: 0.000100\n",
            " â­ New Best: 0.0416\n",
            "Epoch 38/120 | Avg Loss: 0.0408 | LR: 0.000100\n",
            " â­ New Best: 0.0408\n",
            "Epoch 39/120 | Avg Loss: 0.0500 | LR: 0.000100\n",
            "Epoch 40/120 | Avg Loss: 0.0492 | LR: 0.000100\n",
            "Epoch 41/120 | Avg Loss: 0.0519 | LR: 0.000100\n",
            "Epoch 42/120 | Avg Loss: 0.0476 | LR: 0.000100\n",
            "Epoch 43/120 | Avg Loss: 0.0365 | LR: 0.000100\n",
            " â­ New Best: 0.0365\n",
            "Epoch 44/120 | Avg Loss: 0.0400 | LR: 0.000100\n",
            "Epoch 45/120 | Avg Loss: 0.0449 | LR: 0.000100\n",
            "Epoch 46/120 | Avg Loss: 0.0463 | LR: 0.000100\n",
            "Epoch 47/120 | Avg Loss: 0.0503 | LR: 0.000100\n",
            "Epoch 48/120 | Avg Loss: 0.0405 | LR: 0.000100\n",
            "Epoch 49/120 | Avg Loss: 0.0407 | LR: 0.000100\n",
            "Epoch 50/120 | Avg Loss: 0.0485 | LR: 0.000100\n",
            "Epoch 51/120 | Avg Loss: 0.0378 | LR: 0.000100\n",
            "Epoch 52/120 | Avg Loss: 0.0298 | LR: 0.000100\n",
            " â­ New Best: 0.0298\n",
            "Epoch 53/120 | Avg Loss: 0.0456 | LR: 0.000100\n",
            "Epoch 54/120 | Avg Loss: 0.0379 | LR: 0.000100\n",
            "Epoch 55/120 | Avg Loss: 0.0345 | LR: 0.000100\n",
            "Epoch 56/120 | Avg Loss: 0.0427 | LR: 0.000100\n",
            "Epoch 57/120 | Avg Loss: 0.0478 | LR: 0.000100\n",
            "Epoch 58/120 | Avg Loss: 0.0305 | LR: 0.000100\n",
            "Epoch 59/120 | Avg Loss: 0.0374 | LR: 0.000100\n",
            "Epoch 60/120 | Avg Loss: 0.0320 | LR: 0.000100\n",
            "Epoch 61/120 | Avg Loss: 0.0438 | LR: 0.000100\n",
            "Epoch 62/120 | Avg Loss: 0.0255 | LR: 0.000050\n",
            " â­ New Best: 0.0255\n",
            "Epoch 63/120 | Avg Loss: 0.0308 | LR: 0.000050\n",
            "Epoch 64/120 | Avg Loss: 0.0362 | LR: 0.000050\n",
            "Epoch 65/120 | Avg Loss: 0.0327 | LR: 0.000050\n",
            "Epoch 66/120 | Avg Loss: 0.0370 | LR: 0.000050\n",
            "Epoch 67/120 | Avg Loss: 0.0340 | LR: 0.000050\n",
            "Epoch 68/120 | Avg Loss: 0.0283 | LR: 0.000050\n",
            "Epoch 69/120 | Avg Loss: 0.0253 | LR: 0.000050\n",
            " â­ New Best: 0.0253\n",
            "Epoch 70/120 | Avg Loss: 0.0274 | LR: 0.000050\n",
            "Epoch 71/120 | Avg Loss: 0.0214 | LR: 0.000050\n",
            " â­ New Best: 0.0214\n",
            "Epoch 72/120 | Avg Loss: 0.0398 | LR: 0.000050\n",
            "Epoch 73/120 | Avg Loss: 0.0256 | LR: 0.000050\n",
            "Epoch 74/120 | Avg Loss: 0.0341 | LR: 0.000050\n",
            "Epoch 75/120 | Avg Loss: 0.0324 | LR: 0.000050\n",
            "Epoch 76/120 | Avg Loss: 0.0279 | LR: 0.000050\n",
            "Epoch 77/120 | Avg Loss: 0.0318 | LR: 0.000050\n",
            "Epoch 78/120 | Avg Loss: 0.0303 | LR: 0.000050\n",
            "Epoch 79/120 | Avg Loss: 0.0306 | LR: 0.000050\n",
            "Epoch 80/120 | Avg Loss: 0.0293 | LR: 0.000050\n",
            "Epoch 81/120 | Avg Loss: 0.0289 | LR: 0.000025\n",
            "Epoch 82/120 | Avg Loss: 0.0223 | LR: 0.000025\n",
            "Epoch 83/120 | Avg Loss: 0.0230 | LR: 0.000025\n",
            "Epoch 84/120 | Avg Loss: 0.0280 | LR: 0.000025\n",
            "Epoch 85/120 | Avg Loss: 0.0314 | LR: 0.000025\n",
            "Epoch 86/120 | Avg Loss: 0.0277 | LR: 0.000025\n",
            "Epoch 87/120 | Avg Loss: 0.0308 | LR: 0.000025\n",
            "Epoch 88/120 | Avg Loss: 0.0271 | LR: 0.000025\n",
            "Epoch 89/120 | Avg Loss: 0.0256 | LR: 0.000025\n",
            "Epoch 90/120 | Avg Loss: 0.0296 | LR: 0.000013\n",
            "Epoch 91/120 | Avg Loss: 0.0219 | LR: 0.000013\n",
            "Epoch 92/120 | Avg Loss: 0.0292 | LR: 0.000013\n",
            "Epoch 93/120 | Avg Loss: 0.0211 | LR: 0.000013\n",
            " â­ New Best: 0.0211\n",
            "Epoch 94/120 | Avg Loss: 0.0216 | LR: 0.000013\n",
            "Epoch 95/120 | Avg Loss: 0.0257 | LR: 0.000013\n",
            "Epoch 96/120 | Avg Loss: 0.0240 | LR: 0.000013\n",
            "Epoch 97/120 | Avg Loss: 0.0261 | LR: 0.000013\n",
            "Epoch 98/120 | Avg Loss: 0.0238 | LR: 0.000013\n",
            "Epoch 99/120 | Avg Loss: 0.0282 | LR: 0.000013\n",
            "Epoch 100/120 | Avg Loss: 0.0203 | LR: 0.000013\n",
            " â­ New Best: 0.0203\n",
            "Epoch 101/120 | Avg Loss: 0.0237 | LR: 0.000013\n",
            "Epoch 102/120 | Avg Loss: 0.0230 | LR: 0.000013\n",
            "Epoch 103/120 | Avg Loss: 0.0266 | LR: 0.000013\n",
            "Epoch 104/120 | Avg Loss: 0.0189 | LR: 0.000013\n",
            " â­ New Best: 0.0189\n",
            "Epoch 105/120 | Avg Loss: 0.0219 | LR: 0.000013\n",
            "Epoch 106/120 | Avg Loss: 0.0220 | LR: 0.000013\n",
            "Epoch 107/120 | Avg Loss: 0.0192 | LR: 0.000013\n",
            "Epoch 108/120 | Avg Loss: 0.0251 | LR: 0.000013\n",
            "Epoch 109/120 | Avg Loss: 0.0248 | LR: 0.000013\n",
            "Epoch 110/120 | Avg Loss: 0.0252 | LR: 0.000013\n",
            "Epoch 111/120 | Avg Loss: 0.0216 | LR: 0.000013\n",
            "Epoch 112/120 | Avg Loss: 0.0178 | LR: 0.000013\n",
            " â­ New Best: 0.0178\n",
            "Epoch 113/120 | Avg Loss: 0.0206 | LR: 0.000013\n",
            "Epoch 114/120 | Avg Loss: 0.0182 | LR: 0.000013\n",
            "Epoch 115/120 | Avg Loss: 0.0275 | LR: 0.000013\n",
            "Epoch 116/120 | Avg Loss: 0.0269 | LR: 0.000013\n",
            "Epoch 117/120 | Avg Loss: 0.0286 | LR: 0.000013\n",
            "Epoch 118/120 | Avg Loss: 0.0289 | LR: 0.000013\n",
            "Epoch 119/120 | Avg Loss: 0.0210 | LR: 0.000013\n",
            "Epoch 120/120 | Avg Loss: 0.0216 | LR: 0.000013\n",
            "âœ… Training Complete.\n",
            "Best Model Saved to: /content/pitch_modelV1_deepCNNplussmoothing/best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVAL DEEP_CNN+SMOOTHING"
      ],
      "metadata": {
        "id": "-03BDfsxcG1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It combines the Global Smoothing from V4 (to clean up the input) with the Deeper Architecture from V3.6 (to learn complex patterns).\n",
        "\n",
        "      Layer,Input Channels,Output Channels,What it learns\n",
        "      1. Conv1d,1 (Pitch),32,Slopes: Is the pitch going up or down?\n",
        "      2. Conv1d,32,64,Curves: Simple vibrato or note transitions.\n",
        "      3. Conv1d,64,128,\"Motifs: Short musical ideas (e.g., a specific riff).\"\n",
        "      4. Conv1d,128,256,Phrasing: Long-term melodic structure.\n",
        "      FC Layer,256,256,Feature Mixing: Combining these patterns.\n",
        "      Output,256,128,Fingerprint: The final embedding."
      ],
      "metadata": {
        "id": "J6cxDvMcdHJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVAL V3.6/V4: DEEP CNN + SMOOTHING + GEOMETRIC SCORING (HARD AUG FIXED)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"  # 400 unseen files\n",
        "# Pointing to the Deep CNN model checkpoint (V3.6/V4)\n",
        "MODEL_PATH = \"/content/pitch_modelV1_deepCNNplussmoothing/best.pth\"\n",
        "\n",
        "# Params\n",
        "WIN_LEN = 300\n",
        "HOP_LEN = 150\n",
        "TOLERANCE = 1.0    # Time bucket tolerance\n",
        "TOP_K_MATCHES = 20\n",
        "NUM_TRIALS = 400   # Full coverage\n",
        "SEGMENT_LEN = 1500 # 15 seconds\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL ARCHITECTURE (DEEP CNN V3.6)\n",
        "# ======================================================\n",
        "class PitchSiameseNet(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32), nn.ReLU(),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            # Layer 3\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "\n",
        "            # Layer 4 (The \"Deep\" Part)\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), # Input matches CNN output (256)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.cnn(x).squeeze(-1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)\n",
        "\n",
        "print(f\"â³ Loading Deep CNN Model from {MODEL_PATH}...\")\n",
        "model = PitchSiameseNet(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "# ======================================================\n",
        "# 2. SMOOTHING (MATCHES TRAINING)\n",
        "# ======================================================\n",
        "def smooth_pitch(pitch):\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# ======================================================\n",
        "# 3. AUGMENTATION (FIXED)\n",
        "# ======================================================\n",
        "def humify_soft(arr):\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr))\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def humify_hard(arr):\n",
        "    \"\"\"Hard Hum Augmentation (NOW INCLUDES TIME WARPING)\"\"\"\n",
        "    arr = arr.copy()\n",
        "\n",
        "    # 1. Jitter\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. TIME WARP (FIXED: Added back logic to match Training V4)\n",
        "    target_len = SEGMENT_LEN # 1500\n",
        "    if random.random() < 0.8: # 80% chance of warping\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "        # Force back to target length (Cropping or Padding)\n",
        "        if len(arr) < target_len:\n",
        "            arr = np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
        "        else:\n",
        "            start = (len(arr) - target_len) // 2\n",
        "            arr = arr[start:start+target_len]\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "# ======================================================\n",
        "# 4. EMBEDDING + DB BUILDER\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"\n",
        "    Returns embeddings and time offsets.\n",
        "    NOTE: Processes in mini-batches to prevent CUDA OutOfMemoryError.\n",
        "    \"\"\"\n",
        "    # 1. APPLY GLOBAL SMOOTHING FIRST\n",
        "    arr = smooth_pitch(arr)\n",
        "\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "        if np.mean(crop > 0) < 0.1: # Skip silence\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0)\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    # Stack all windows into one large tensor\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    # --- BATCH PROCESSING FIX (prevents OOM) ---\n",
        "    batch_size = 64  # Process 64 windows at a time\n",
        "    embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for k in range(0, len(windows_tensor), batch_size):\n",
        "            # Slice the batch\n",
        "            batch = windows_tensor[k : k + batch_size]\n",
        "            # Forward pass\n",
        "            emb_batch = model.forward_one(batch)\n",
        "            # Store result\n",
        "            embeddings_list.append(emb_batch)\n",
        "\n",
        "    # Concatenate all batch results back into one tensor\n",
        "    embeddings = torch.cat(embeddings_list, dim=0)\n",
        "\n",
        "    return embeddings, offsets\n",
        "\n",
        "def build_flat_database():\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Geometric DB from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        # Process full song (Smoothing happens inside function)\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "# ======================================================\n",
        "# 5. GEOMETRIC SCORING\n",
        "# ======================================================\n",
        "def query_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    # 1. Distance Matrix\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Top-K\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4\n",
        "\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # 3. Geometric Alignment (Projected Start)\n",
        "            projected_start = match_time - q_time\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Max Score per Song\n",
        "    song_final_scores = defaultdict(float)\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs]\n",
        "\n",
        "# ======================================================\n",
        "# 6. RUN EVAL\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸš€ Running {NUM_TRIALS} Trials with Geometric Scoring...\")\n",
        "\n",
        "    effective_trials = {\"Soft\": 0, \"Hard\": 0}\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        target_song = random.choice(song_list)\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "\n",
        "        if len(full_arr) < SEGMENT_LEN: continue\n",
        "\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "        clean_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # --- Test Soft ---\n",
        "        soft_hum_clip = humify_soft(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Soft\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1\n",
        "\n",
        "        # --- Test Hard ---\n",
        "        hard_hum_clip = humify_hard(clean_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Hard\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    def calc_acc(res, key, total):\n",
        "        return res[key] / total if total > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š V3.6 DEEP CNN RESULTS (400 Files) - HARD AUG FIXED\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Effective Soft Trials: {effective_trials['Soft']}\")\n",
        "    print(f\"Total Effective Hard Trials: {effective_trials['Hard']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"ðŸŽ¤ Soft Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Soft'], 'top1', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Soft'], 'top5', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Soft'], 'top10', effective_trials['Soft']):.1%}\")\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Hard Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Hard'], 'top1', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Hard'], 'top5', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Hard'], 'top10', effective_trials['Hard']):.1%}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7w9fOz6cGby",
        "outputId": "4c32aa5f-6002-405e-fad7-5f14be137e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading Deep CNN Model from /content/pitch_modelV1_deepCNNplussmoothing/best.pth...\n",
            "âœ… Model loaded successfully.\n",
            "ðŸ—ï¸ Building Geometric DB from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 184.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 windows across 400 songs.\n",
            "\n",
            "ðŸš€ Running 400 Trials with Geometric Scoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 143.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š V3.6 DEEP CNN RESULTS (400 Files) - HARD AUG FIXED\n",
            "==================================================\n",
            "Total Effective Soft Trials: 400\n",
            "Total Effective Hard Trials: 400\n",
            "--------------------------------------------------\n",
            "ðŸŽ¤ Soft Hum:\n",
            "   Top-1:  58.0%\n",
            "   Top-5:  62.3%\n",
            "   Top-10: 64.0%\n",
            "\n",
            "ðŸ”¥ Hard Hum:\n",
            "   Top-1:  37.5%\n",
            "   Top-5:  49.2%\n",
            "   Top-10: 54.5%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¤ Soft Hum:\n",
        "   Top-1:  58.0%\n",
        "   Top-5:  62.3%\n",
        "   Top-10: 64.0%\n",
        "\n",
        "ðŸ”¥ Hard Hum:\n",
        "   Top-1:  37.5%\n",
        "   Top-5:  49.2%\n",
        "   Top-10: 54.5%"
      ],
      "metadata": {
        "id": "YsSa3d6e9lU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+LSTN+smoothing"
      ],
      "metadata": {
        "id": "jnR77Df7JiP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-Directional: This is key. The model reads the melody forwards (start to end) AND backwards (end to start).\n",
        "\n",
        "      Smoothing: Removes the jitter.\n",
        "\n",
        "      CNN: Extracts clean features.\n",
        "\n",
        "      LSTM: Analyzes the clean sequence.\n",
        "\n",
        "Why? In music, context matters.\n",
        "\n",
        "      Forward: \"I heard a C, then an E, so the next note is probably a G.\"\n",
        "\n",
        "      Backward: \"I see a G at the end, which confirms the C at the start was part of a C-Major chord.\"\n",
        "\n",
        "      Memory: Unlike CNNs, LSTMs have \"memory cells.\" They can remember a key change that happened 2 seconds ago and use that info to interpret the current note.\n",
        "\n",
        "BiLSTM attaches Musical Context (Phrasing & Key) to the notes based on the melody structure."
      ],
      "metadata": {
        "id": "pQwQcnmKdhgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V5: CRNN (DEEP CNN + 2-LAYER BiLSTM) + GLOBAL SMOOTHING + ADAPTIVE LR\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams\n",
        "# -------------------------\n",
        "TARGET_LEN = 300   # 3 seconds\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 120       # <--- Updated to 120\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV1_lstmplussmoothing\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# 1. GLOBAL SMOOTHING HELPER\n",
        "# -------------------------\n",
        "def smooth_pitch(pitch):\n",
        "    \"\"\"\n",
        "    Applies Median Filter (k=5) to remove jagged tracking errors.\n",
        "    Crucial for stabilizing the LSTM input.\n",
        "    \"\"\"\n",
        "    #\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# 2. AUGMENTATION\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy().astype(np.float32)\n",
        "\n",
        "    # 1. Noise\n",
        "    pitch += np.random.normal(0, 0.06, size=len(pitch))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Time Warp\n",
        "    if random.random() < 0.7:\n",
        "        rate = np.random.uniform(0.8, 1.25)\n",
        "        old_idx = np.arange(len(pitch))\n",
        "        new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "        pitch = np.interp(new_idx, old_idx, pitch)\n",
        "\n",
        "    # 4. Breath Noise (Simulating air in the mic)\n",
        "    pitch += np.random.normal(0, 0.04, size=len(pitch))\n",
        "\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: Pad/Crop\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None or len(arr) == 0:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "    if len(arr) < target_len:\n",
        "        pad_amt = target_len - len(arr)\n",
        "        return np.pad(arr, (0, pad_amt), mode='constant')\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# 3. DATASET\n",
        "# -------------------------\n",
        "class PitchDatasetV5(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted(glob.glob(os.path.join(pitch_dir, \"*.npy\")))\n",
        "        self.target_len = target_len\n",
        "        print(f\"âœ… Loaded {len(self.files)} files\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return arr\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Raw Data\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        # --- APPLY GLOBAL SMOOTHING ---\n",
        "        # This is the V4/V5 upgrade: Clean inputs before cropping\n",
        "        anchor_full = smooth_pitch(anchor_full)\n",
        "        neg_full = smooth_pitch(neg_full)\n",
        "        # ------------------------------\n",
        "\n",
        "        # 1. CROP\n",
        "        anchor_clean = self._random_crop(anchor_full)\n",
        "        neg_clean = self._random_crop(neg_full)\n",
        "\n",
        "        # 2. AUGMENT\n",
        "        # Positive is the HUMMED version of the anchor\n",
        "        positive_hum = augment_hum(anchor_clean)\n",
        "        # Negative is the HUMMED version of the negative (Harder Negative)\n",
        "        negative_hum = augment_hum(neg_clean)\n",
        "\n",
        "        # 3. PAD/TRUNCATE\n",
        "        a_out = force_length(anchor_clean, self.target_len)\n",
        "        ph_out = force_length(positive_hum, self.target_len)\n",
        "        n_out = force_length(negative_hum, self.target_len)\n",
        "\n",
        "        # Return only the 3 necessary tensors for Triplet Loss\n",
        "        return (\n",
        "            torch.from_numpy(a_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(ph_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(n_out).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# 4. MODEL: CRNN (CNN + BiLSTM)\n",
        "# -------------------------\n",
        "class PitchCRNN(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # A. CNN Feature Extractor\n",
        "        # Reduces length 300 -> 75, Increases features 1 -> 256\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 300 -> 150\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 150 -> 75\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            # No pooling here, preserving sequence length (75) for LSTM\n",
        "        )\n",
        "\n",
        "        # B. Deep Bidirectional LSTM\n",
        "        # Input: 256 features (from CNN)\n",
        "        # Hidden: 128 features per direction = 256 total output\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=256,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,        # Deep LSTM (2 stacked layers)\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # C. Projection Head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256), # 256 matches BiLSTM output (128*2)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # x shape: (Batch, 1, 300)\n",
        "\n",
        "        # 1. CNN Forward\n",
        "        x = self.cnn(x)  # Output: (Batch, 256, 75)\n",
        "\n",
        "        # 2. Prepare for LSTM\n",
        "        # LSTM expects (Batch, Sequence, Features)\n",
        "        x = x.permute(0, 2, 1) # Output: (Batch, 75, 256)\n",
        "\n",
        "        # 3. LSTM Forward\n",
        "        # out shape: (Batch, 75, 256)\n",
        "        self.lstm.flatten_parameters() # Optimize memory for CUDA\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # 4. Global Average Pooling (Over time dimension)\n",
        "        # We average the 75 time steps to get one vector per song\n",
        "        out = torch.mean(out, dim=1) # Output: (Batch, 256)\n",
        "\n",
        "        # 5. Projection\n",
        "        out = self.fc(out) # Output: (Batch, 128)\n",
        "\n",
        "        return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "# -------------------------\n",
        "# 5. TRAINING LOOP\n",
        "# -------------------------\n",
        "def train_crnn_v5():\n",
        "    print(f\"ðŸš€ Training V5: CRNN (CNN + BiLSTM) on: {DEVICE}\")\n",
        "    print(f\"ðŸ“‚ Data: {PITCH_DIR}\")\n",
        "    print(f\"ðŸ”§ Margin: 0.85 | Smoothing: ON | Epochs: {EPOCHS}\")\n",
        "\n",
        "    dataset = PitchDatasetV5(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    # num_workers=0 avoids multiprocessing errors in notebooks\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchCRNN().to(DEVICE)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # ADAPTIVE LR SCHEDULER\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.85, p=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    print(f\"Starting LR: 0.0001\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Using simple iterator to keep log clean\n",
        "        # pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for anchor, pos_hum, neg_hum in loader:\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg_hum = neg_hum.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            a = model.forward_one(anchor)\n",
        "            ph = model.forward_one(pos_hum)\n",
        "            n = model.forward_one(neg_hum)\n",
        "\n",
        "            # Single robust loss\n",
        "            loss = loss_fn(a, ph, n)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping (Important for LSTMs stability)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New Best: {best_loss:.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"âœ… Training Complete.\")\n",
        "    print(f\"Best Model Saved to: {BEST}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_crnn_v5()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zno8g1kOJhwx",
        "outputId": "1b7ca93e-b161-4c74-ad98-45667e4bb5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training V5: CRNN (CNN + BiLSTM) on: cuda\n",
            "ðŸ“‚ Data: /content/data_unique\n",
            "ðŸ”§ Margin: 0.85 | Smoothing: ON | Epochs: 120\n",
            "âœ… Loaded 4051 files\n",
            "Starting LR: 0.0001\n",
            "Epoch 1/120 | Avg Loss: 0.2638 | LR: 0.000100\n",
            " â­ New Best: 0.2638\n",
            "Epoch 2/120 | Avg Loss: 0.1283 | LR: 0.000100\n",
            " â­ New Best: 0.1283\n",
            "Epoch 3/120 | Avg Loss: 0.1201 | LR: 0.000100\n",
            " â­ New Best: 0.1201\n",
            "Epoch 4/120 | Avg Loss: 0.0915 | LR: 0.000100\n",
            " â­ New Best: 0.0915\n",
            "Epoch 5/120 | Avg Loss: 0.0789 | LR: 0.000100\n",
            " â­ New Best: 0.0789\n",
            "Epoch 6/120 | Avg Loss: 0.0759 | LR: 0.000100\n",
            " â­ New Best: 0.0759\n",
            "Epoch 7/120 | Avg Loss: 0.0713 | LR: 0.000100\n",
            " â­ New Best: 0.0713\n",
            "Epoch 8/120 | Avg Loss: 0.0641 | LR: 0.000100\n",
            " â­ New Best: 0.0641\n",
            "Epoch 9/120 | Avg Loss: 0.0599 | LR: 0.000100\n",
            " â­ New Best: 0.0599\n",
            "Epoch 10/120 | Avg Loss: 0.0546 | LR: 0.000100\n",
            " â­ New Best: 0.0546\n",
            "Epoch 11/120 | Avg Loss: 0.0622 | LR: 0.000100\n",
            "Epoch 12/120 | Avg Loss: 0.0755 | LR: 0.000100\n",
            "Epoch 13/120 | Avg Loss: 0.0600 | LR: 0.000100\n",
            "Epoch 14/120 | Avg Loss: 0.0488 | LR: 0.000100\n",
            " â­ New Best: 0.0488\n",
            "Epoch 15/120 | Avg Loss: 0.0515 | LR: 0.000100\n",
            "Epoch 16/120 | Avg Loss: 0.0509 | LR: 0.000100\n",
            "Epoch 17/120 | Avg Loss: 0.0499 | LR: 0.000100\n",
            "Epoch 18/120 | Avg Loss: 0.0653 | LR: 0.000100\n",
            "Epoch 19/120 | Avg Loss: 0.0549 | LR: 0.000100\n",
            "Epoch 20/120 | Avg Loss: 0.0460 | LR: 0.000100\n",
            " â­ New Best: 0.0460\n",
            "Epoch 21/120 | Avg Loss: 0.0457 | LR: 0.000100\n",
            " â­ New Best: 0.0457\n",
            "Epoch 22/120 | Avg Loss: 0.0457 | LR: 0.000100\n",
            " â­ New Best: 0.0457\n",
            "Epoch 23/120 | Avg Loss: 0.0459 | LR: 0.000100\n",
            "Epoch 24/120 | Avg Loss: 0.0453 | LR: 0.000100\n",
            " â­ New Best: 0.0453\n",
            "Epoch 25/120 | Avg Loss: 0.0387 | LR: 0.000100\n",
            " â­ New Best: 0.0387\n",
            "Epoch 26/120 | Avg Loss: 0.0370 | LR: 0.000100\n",
            " â­ New Best: 0.0370\n",
            "Epoch 27/120 | Avg Loss: 0.0471 | LR: 0.000100\n",
            "Epoch 28/120 | Avg Loss: 0.0340 | LR: 0.000100\n",
            " â­ New Best: 0.0340\n",
            "Epoch 29/120 | Avg Loss: 0.0373 | LR: 0.000100\n",
            "Epoch 30/120 | Avg Loss: 0.0454 | LR: 0.000100\n",
            "Epoch 31/120 | Avg Loss: 0.0377 | LR: 0.000100\n",
            "Epoch 32/120 | Avg Loss: 0.0326 | LR: 0.000100\n",
            " â­ New Best: 0.0326\n",
            "Epoch 33/120 | Avg Loss: 0.0345 | LR: 0.000100\n",
            "Epoch 34/120 | Avg Loss: 0.0406 | LR: 0.000100\n",
            "Epoch 35/120 | Avg Loss: 0.0406 | LR: 0.000100\n",
            "Epoch 36/120 | Avg Loss: 0.0318 | LR: 0.000100\n",
            " â­ New Best: 0.0318\n",
            "Epoch 37/120 | Avg Loss: 0.0389 | LR: 0.000100\n",
            "Epoch 38/120 | Avg Loss: 0.0338 | LR: 0.000100\n",
            "Epoch 39/120 | Avg Loss: 0.0272 | LR: 0.000100\n",
            " â­ New Best: 0.0272\n",
            "Epoch 40/120 | Avg Loss: 0.0258 | LR: 0.000100\n",
            " â­ New Best: 0.0258\n",
            "Epoch 41/120 | Avg Loss: 0.0324 | LR: 0.000100\n",
            "Epoch 42/120 | Avg Loss: 0.0301 | LR: 0.000100\n",
            "Epoch 43/120 | Avg Loss: 0.0226 | LR: 0.000100\n",
            " â­ New Best: 0.0226\n",
            "Epoch 44/120 | Avg Loss: 0.0288 | LR: 0.000100\n",
            "Epoch 45/120 | Avg Loss: 0.0285 | LR: 0.000100\n",
            "Epoch 46/120 | Avg Loss: 0.0307 | LR: 0.000100\n",
            "Epoch 47/120 | Avg Loss: 0.0252 | LR: 0.000100\n",
            "Epoch 48/120 | Avg Loss: 0.0258 | LR: 0.000100\n",
            "Epoch 49/120 | Avg Loss: 0.0339 | LR: 0.000100\n",
            "Epoch 50/120 | Avg Loss: 0.0301 | LR: 0.000100\n",
            "Epoch 51/120 | Avg Loss: 0.0227 | LR: 0.000100\n",
            "Epoch 52/120 | Avg Loss: 0.0245 | LR: 0.000100\n",
            "Epoch 53/120 | Avg Loss: 0.0236 | LR: 0.000050\n",
            "Epoch 54/120 | Avg Loss: 0.0204 | LR: 0.000050\n",
            " â­ New Best: 0.0204\n",
            "Epoch 55/120 | Avg Loss: 0.0182 | LR: 0.000050\n",
            " â­ New Best: 0.0182\n",
            "Epoch 56/120 | Avg Loss: 0.0242 | LR: 0.000050\n",
            "Epoch 57/120 | Avg Loss: 0.0221 | LR: 0.000050\n",
            "Epoch 58/120 | Avg Loss: 0.0213 | LR: 0.000050\n",
            "Epoch 59/120 | Avg Loss: 0.0258 | LR: 0.000050\n",
            "Epoch 60/120 | Avg Loss: 0.0188 | LR: 0.000050\n",
            "Epoch 61/120 | Avg Loss: 0.0257 | LR: 0.000050\n",
            "Epoch 62/120 | Avg Loss: 0.0191 | LR: 0.000050\n",
            "Epoch 63/120 | Avg Loss: 0.0223 | LR: 0.000050\n",
            "Epoch 64/120 | Avg Loss: 0.0241 | LR: 0.000050\n",
            "Epoch 65/120 | Avg Loss: 0.0258 | LR: 0.000025\n",
            "Epoch 66/120 | Avg Loss: 0.0206 | LR: 0.000025\n",
            "Epoch 67/120 | Avg Loss: 0.0203 | LR: 0.000025\n",
            "Epoch 68/120 | Avg Loss: 0.0194 | LR: 0.000025\n",
            "Epoch 69/120 | Avg Loss: 0.0165 | LR: 0.000025\n",
            " â­ New Best: 0.0165\n",
            "Epoch 70/120 | Avg Loss: 0.0190 | LR: 0.000025\n",
            "Epoch 71/120 | Avg Loss: 0.0201 | LR: 0.000025\n",
            "Epoch 72/120 | Avg Loss: 0.0185 | LR: 0.000025\n",
            "Epoch 73/120 | Avg Loss: 0.0175 | LR: 0.000025\n",
            "Epoch 74/120 | Avg Loss: 0.0138 | LR: 0.000025\n",
            " â­ New Best: 0.0138\n",
            "Epoch 75/120 | Avg Loss: 0.0201 | LR: 0.000025\n",
            "Epoch 76/120 | Avg Loss: 0.0210 | LR: 0.000025\n",
            "Epoch 77/120 | Avg Loss: 0.0214 | LR: 0.000025\n",
            "Epoch 78/120 | Avg Loss: 0.0222 | LR: 0.000025\n",
            "Epoch 79/120 | Avg Loss: 0.0146 | LR: 0.000025\n",
            "Epoch 80/120 | Avg Loss: 0.0161 | LR: 0.000025\n",
            "Epoch 81/120 | Avg Loss: 0.0170 | LR: 0.000025\n",
            "Epoch 82/120 | Avg Loss: 0.0194 | LR: 0.000025\n",
            "Epoch 83/120 | Avg Loss: 0.0147 | LR: 0.000025\n",
            "Epoch 84/120 | Avg Loss: 0.0143 | LR: 0.000013\n",
            "Epoch 85/120 | Avg Loss: 0.0231 | LR: 0.000013\n",
            "Epoch 86/120 | Avg Loss: 0.0211 | LR: 0.000013\n",
            "Epoch 87/120 | Avg Loss: 0.0226 | LR: 0.000013\n",
            "Epoch 88/120 | Avg Loss: 0.0134 | LR: 0.000013\n",
            " â­ New Best: 0.0134\n",
            "Epoch 89/120 | Avg Loss: 0.0173 | LR: 0.000013\n",
            "Epoch 90/120 | Avg Loss: 0.0153 | LR: 0.000013\n",
            "Epoch 91/120 | Avg Loss: 0.0186 | LR: 0.000013\n",
            "Epoch 92/120 | Avg Loss: 0.0149 | LR: 0.000013\n",
            "Epoch 93/120 | Avg Loss: 0.0136 | LR: 0.000013\n",
            "Epoch 94/120 | Avg Loss: 0.0145 | LR: 0.000013\n",
            "Epoch 95/120 | Avg Loss: 0.0144 | LR: 0.000013\n",
            "Epoch 96/120 | Avg Loss: 0.0105 | LR: 0.000013\n",
            " â­ New Best: 0.0105\n",
            "Epoch 97/120 | Avg Loss: 0.0193 | LR: 0.000013\n",
            "Epoch 98/120 | Avg Loss: 0.0233 | LR: 0.000013\n",
            "Epoch 99/120 | Avg Loss: 0.0118 | LR: 0.000013\n",
            "Epoch 100/120 | Avg Loss: 0.0171 | LR: 0.000013\n",
            "Epoch 101/120 | Avg Loss: 0.0195 | LR: 0.000013\n",
            "Epoch 102/120 | Avg Loss: 0.0137 | LR: 0.000013\n",
            "Epoch 103/120 | Avg Loss: 0.0141 | LR: 0.000013\n",
            "Epoch 104/120 | Avg Loss: 0.0131 | LR: 0.000013\n",
            "Epoch 105/120 | Avg Loss: 0.0124 | LR: 0.000013\n",
            "Epoch 106/120 | Avg Loss: 0.0194 | LR: 0.000006\n",
            "Epoch 107/120 | Avg Loss: 0.0140 | LR: 0.000006\n",
            "Epoch 108/120 | Avg Loss: 0.0173 | LR: 0.000006\n",
            "Epoch 109/120 | Avg Loss: 0.0168 | LR: 0.000006\n",
            "Epoch 110/120 | Avg Loss: 0.0161 | LR: 0.000006\n",
            "Epoch 111/120 | Avg Loss: 0.0178 | LR: 0.000006\n",
            "Epoch 112/120 | Avg Loss: 0.0188 | LR: 0.000006\n",
            "Epoch 113/120 | Avg Loss: 0.0151 | LR: 0.000006\n",
            "Epoch 114/120 | Avg Loss: 0.0149 | LR: 0.000006\n",
            "Epoch 115/120 | Avg Loss: 0.0131 | LR: 0.000003\n",
            "Epoch 116/120 | Avg Loss: 0.0196 | LR: 0.000003\n",
            "Epoch 117/120 | Avg Loss: 0.0112 | LR: 0.000003\n",
            "Epoch 118/120 | Avg Loss: 0.0091 | LR: 0.000003\n",
            " â­ New Best: 0.0091\n",
            "Epoch 119/120 | Avg Loss: 0.0135 | LR: 0.000003\n",
            "Epoch 120/120 | Avg Loss: 0.0173 | LR: 0.000003\n",
            "âœ… Training Complete.\n",
            "Best Model Saved to: /content/pitch_modelV1_lstmplussmoothing/best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval OLD+LSTN+smoothing"
      ],
      "metadata": {
        "id": "VeBvIsOq9E70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVAL V5: CRNN (CNN+LSTM) + SMOOTHING + GEOMETRIC SCORING (HARD AUG FIXED)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"  # 400 unseen files\n",
        "# Pointing to the new CRNN model checkpoint\n",
        "MODEL_PATH = \"/content/pitch_modelV1_lstmplussmoothing/best.pth\"\n",
        "\n",
        "# Params\n",
        "WIN_LEN = 300\n",
        "HOP_LEN = 150\n",
        "TOLERANCE = 1.0    # Time bucket tolerance\n",
        "TOP_K_MATCHES = 20\n",
        "NUM_TRIALS = 400   # Full coverage\n",
        "SEGMENT_LEN = 1500 # 15 seconds\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL (V5 Architecture)\n",
        "# ======================================================\n",
        "class PitchCRNN(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. CNN Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 300 -> 150\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 150 -> 75\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # 2. Deep Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=256,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # 3. Projection Head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # A. CNN Forward\n",
        "        x = self.cnn(x)  # (Batch, 256, 75)\n",
        "\n",
        "        # B. Prepare for LSTM (Permute to Batch, Seq, Feat)\n",
        "        x = x.permute(0, 2, 1) # (Batch, 75, 256)\n",
        "\n",
        "        # C. LSTM Forward\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # D. Global Average Pooling (Over time dimension)\n",
        "        out = torch.mean(out, dim=1) # (Batch, 256)\n",
        "\n",
        "        # E. Projection\n",
        "        out = self.fc(out) # (Batch, 128)\n",
        "\n",
        "        return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "print(f\"â³ Loading V5 CRNN Model from {MODEL_PATH}...\")\n",
        "model = PitchCRNN(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "# ======================================================\n",
        "# 2. SMOOTHING (MATCHES TRAINING)\n",
        "# ======================================================\n",
        "def smooth_pitch(pitch):\n",
        "    return sg.medfilt(pitch, kernel_size=5).astype(np.float32)\n",
        "\n",
        "# ======================================================\n",
        "# 3. AUGMENTATION (FIXED)\n",
        "# ======================================================\n",
        "def humify_soft(arr):\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr))\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def humify_hard(arr):\n",
        "    \"\"\"Hard Hum Augmentation (NOW INCLUDES TIME WARPING)\"\"\"\n",
        "    # NOTE: This function assumes the input 'arr' is already the clean_base clip (1500 frames)\n",
        "    arr = arr.copy()\n",
        "\n",
        "    # 1. Jitter\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. TIME WARP (FIXED: Added back the crucial tempo distortion)\n",
        "    target_len = SEGMENT_LEN # 1500\n",
        "    if random.random() < 0.8: # 80% chance of warping\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "        # Force back to target length (Cropping or Padding)\n",
        "        if len(arr) < target_len:\n",
        "            arr = np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
        "        else:\n",
        "            start = (len(arr) - target_len) // 2\n",
        "            arr = arr[start:start+target_len]\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 4. EMBEDDING + DB BUILDER\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"\n",
        "    Returns embeddings and time offsets.\n",
        "    Processes windows in mini-batches to prevent OOM errors.\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "        if np.mean(crop > 0) < 0.1: # Skip silence\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0)\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    # Stack all windows\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    # --- BATCH PROCESSING FIX ---\n",
        "    batch_size = 64  # Safe batch size for inference\n",
        "    embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for k in range(0, len(windows_tensor), batch_size):\n",
        "            batch = windows_tensor[k : k + batch_size]\n",
        "            emb_batch = model.forward_one(batch)\n",
        "            embeddings_list.append(emb_batch)\n",
        "\n",
        "    # Concatenate all batch results back into one tensor\n",
        "    embeddings = torch.cat(embeddings_list, dim=0)\n",
        "\n",
        "    return embeddings, offsets\n",
        "\n",
        "def build_flat_database():\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Geometric DB from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        # V5 LOGIC: Smooth the DB tracks (Anchors)\n",
        "        arr = smooth_pitch(arr)\n",
        "\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "# ======================================================\n",
        "# 5. GEOMETRIC SCORING\n",
        "# ======================================================\n",
        "def query_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    # 1. Distance Matrix\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Top-K\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4\n",
        "\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # 3. Geometric Alignment\n",
        "            projected_start = match_time - q_time\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Max Score per Song\n",
        "    song_final_scores = defaultdict(float)\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs]\n",
        "\n",
        "# ======================================================\n",
        "# 6. RUN EVAL\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸš€ Running {NUM_TRIALS} Trials with Geometric Scoring...\")\n",
        "\n",
        "    effective_trials = {\"Soft\": 0, \"Hard\": 0}\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        target_song = random.choice(song_list)\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "\n",
        "        if len(full_arr) < SEGMENT_LEN: continue\n",
        "\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "        raw_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # V5 LOGIC:\n",
        "        # 1. Smooth the raw clip (Simulates the Clean Anchor base)\n",
        "        clean_base = smooth_pitch(raw_clip)\n",
        "\n",
        "        # 2. Augment the Smoothed Base (Matches Training: augment(smooth(anchor)))\n",
        "\n",
        "        # --- Test Soft ---\n",
        "        soft_hum_clip = humify_soft(clean_base)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Soft\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1\n",
        "\n",
        "        # --- Test Hard ---\n",
        "        hard_hum_clip = humify_hard(clean_base)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Hard\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    def calc_acc(res, key, total):\n",
        "        return res[key] / total if total > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š V5 CRNN (CNN+LSTM) RESULTS (400 Files)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Effective Soft Trials: {effective_trials['Soft']}\")\n",
        "    print(f\"Total Effective Hard Trials: {effective_trials['Hard']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"ðŸŽ¤ Soft Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Soft'], 'top1', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Soft'], 'top5', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Soft'], 'top10', effective_trials['Soft']):.1%}\")\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Hard Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Hard'], 'top1', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Hard'], 'top5', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Hard'], 'top10', effective_trials['Hard']):.1%}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW0AZ0q29Ean",
        "outputId": "e5ff1fa5-f4e0-46df-9a53-bc7848524f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading V5 CRNN Model from /content/pitch_modelV1_lstmplussmoothing/best.pth...\n",
            "âœ… Model loaded successfully.\n",
            "ðŸ—ï¸ Building Geometric DB from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:05<00:00, 69.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 windows across 400 songs.\n",
            "\n",
            "ðŸš€ Running 400 Trials with Geometric Scoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:05<00:00, 78.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š V5 CRNN (CNN+LSTM) RESULTS (400 Files)\n",
            "==================================================\n",
            "Total Effective Soft Trials: 400\n",
            "Total Effective Hard Trials: 400\n",
            "--------------------------------------------------\n",
            "ðŸŽ¤ Soft Hum:\n",
            "   Top-1:  54.8%\n",
            "   Top-5:  60.0%\n",
            "   Top-10: 60.8%\n",
            "\n",
            "ðŸ”¥ Hard Hum:\n",
            "   Top-1:  54.8%\n",
            "   Top-5:  60.2%\n",
            "   Top-10: 61.8%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¤ Soft Hum:\n",
        "   Top-1:  54.8%\n",
        "   Top-5:  60.0%\n",
        "   Top-10: 60.8%\n",
        "\n",
        "ðŸ”¥ Hard Hum:\n",
        "   Top-1:  54.8%\n",
        "   Top-5:  60.2%\n",
        "   Top-10: 61.8%"
      ],
      "metadata": {
        "id": "9gOrq2yE9rkh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlKNxJ2grSMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RJveVkRrSJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD+LSTM"
      ],
      "metadata": {
        "id": "H1j956T8Ppaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTMs are good at learning \"temporal noise.\" You are betting that the LSTM can learn to ignore the specific \"jitter pattern\" of CREPE artifacts on its own, potentially preserving subtle vibrato details that the Median Filter might have accidentally smoothed away."
      ],
      "metadata": {
        "id": "YhXsBh_EfOnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# V5: CRNN (CNN + BiLSTM) | NO SMOOTHING | ADAPTIVE LR | 100 EPOCHS\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparams\n",
        "# -------------------------\n",
        "TARGET_LEN = 300   # 3 seconds\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100       # 100 Epochs\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# PATHS\n",
        "# -------------------------\n",
        "PITCH_DIR = \"/content/data_unique\"\n",
        "CKPT_DIR = \"/content/pitch_modelV5_nosmooth\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "BEST = f\"{CKPT_DIR}/best.pth\"\n",
        "LAST = f\"{CKPT_DIR}/last.pth\"\n",
        "\n",
        "# -------------------------\n",
        "# 1. AUGMENTATION\n",
        "# -------------------------\n",
        "def augment_hum(pitch):\n",
        "    pitch = pitch.copy().astype(np.float32)\n",
        "\n",
        "    # 1. Noise\n",
        "    pitch += np.random.normal(0, 0.06, size=len(pitch))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-5, 5)\n",
        "    pitch[pitch > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. Time Warp\n",
        "    if random.random() < 0.7:\n",
        "        rate = np.random.uniform(0.8, 1.25)\n",
        "        old_idx = np.arange(len(pitch))\n",
        "        new_idx = np.linspace(0, len(pitch)-1, max(2, int(len(pitch)*rate)))\n",
        "        pitch = np.interp(new_idx, old_idx, pitch)\n",
        "\n",
        "    # 4. Breath Noise\n",
        "    pitch += np.random.normal(0, 0.04, size=len(pitch))\n",
        "\n",
        "    return pitch.astype(np.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Helper: Pad/Crop\n",
        "# -------------------------\n",
        "def force_length(arr, target_len=TARGET_LEN):\n",
        "    if arr is None or len(arr) == 0:\n",
        "        return np.zeros(target_len, dtype=np.float32)\n",
        "    if len(arr) < target_len:\n",
        "        pad_amt = target_len - len(arr)\n",
        "        return np.pad(arr, (0, pad_amt), mode='constant')\n",
        "    elif len(arr) > target_len:\n",
        "        start = random.randint(0, len(arr) - target_len)\n",
        "        return arr[start:start + target_len]\n",
        "    return arr\n",
        "\n",
        "# -------------------------\n",
        "# 2. DATASET (NO SMOOTHING)\n",
        "# -------------------------\n",
        "class PitchDatasetV5(Dataset):\n",
        "    def __init__(self, pitch_dir, target_len=TARGET_LEN):\n",
        "        self.files = sorted(glob.glob(os.path.join(pitch_dir, \"*.npy\")))\n",
        "        self.target_len = target_len\n",
        "        print(f\"âœ… Loaded {len(self.files)} files\")\n",
        "\n",
        "    def _random_crop(self, arr):\n",
        "        if len(arr) <= self.target_len:\n",
        "            return arr\n",
        "        start = random.randint(0, len(arr) - self.target_len)\n",
        "        return arr[start:start + self.target_len]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Raw Data\n",
        "        anchor_path = self.files[idx]\n",
        "        anchor_full = np.load(anchor_path)\n",
        "\n",
        "        neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        while neg_idx == idx:\n",
        "            neg_idx = random.randint(0, len(self.files) - 1)\n",
        "        neg_full = np.load(self.files[neg_idx])\n",
        "\n",
        "        # --- NO SMOOTHING APPLIED HERE ---\n",
        "\n",
        "        # 1. CROP\n",
        "        anchor_clean = self._random_crop(anchor_full)\n",
        "        neg_clean = self._random_crop(neg_full)\n",
        "\n",
        "        # 2. AUGMENT\n",
        "        positive_hum = augment_hum(anchor_clean)\n",
        "        negative_hum = augment_hum(neg_clean)\n",
        "\n",
        "        # 3. PAD/TRUNCATE\n",
        "        a_out = force_length(anchor_clean, self.target_len)\n",
        "        ph_out = force_length(positive_hum, self.target_len)\n",
        "        n_out = force_length(negative_hum, self.target_len)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(a_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(ph_out).unsqueeze(0).float(),\n",
        "            torch.from_numpy(n_out).unsqueeze(0).float(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# -------------------------\n",
        "# 3. MODEL: CRNN (CNN + BiLSTM)\n",
        "# -------------------------\n",
        "class PitchCRNN(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # A. CNN Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 300 -> 150\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 150 -> 75\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # B. Deep Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=256,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # C. Projection Head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # 1. CNN Forward\n",
        "        x = self.cnn(x)  # (Batch, 256, 75)\n",
        "\n",
        "        # 2. Prepare for LSTM (Batch, Seq, Feat)\n",
        "        x = x.permute(0, 2, 1) # (Batch, 75, 256)\n",
        "\n",
        "        # 3. LSTM Forward\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # 4. Global Average Pooling\n",
        "        out = torch.mean(out, dim=1) # (Batch, 256)\n",
        "\n",
        "        # 5. Projection\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "# -------------------------\n",
        "# 4. TRAINING LOOP\n",
        "# -------------------------\n",
        "def train_crnn_nosmooth():\n",
        "    print(f\"ðŸš€ Training V5: CRNN (CNN + BiLSTM) | NO SMOOTHING | {DEVICE}\")\n",
        "    print(f\"ðŸ“‚ Data: {PITCH_DIR}\")\n",
        "    print(f\"ðŸ”§ Epochs: {EPOCHS} | Adaptive LR: ON\")\n",
        "\n",
        "    dataset = PitchDatasetV5(PITCH_DIR, target_len=TARGET_LEN)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    model = PitchCRNN().to(DEVICE)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # ADAPTIVE LR SCHEDULER\n",
        "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=8)\n",
        "\n",
        "    loss_fn = nn.TripletMarginLoss(margin=0.85, p=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for anchor, pos_hum, neg_hum in loader:\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            pos_hum = pos_hum.to(DEVICE)\n",
        "            neg_hum = neg_hum.to(DEVICE)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            a = model.forward_one(anchor)\n",
        "            ph = model.forward_one(pos_hum)\n",
        "            n = model.forward_one(neg_hum)\n",
        "\n",
        "            loss = loss_fn(a, ph, n)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip Gradients for LSTM stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), BEST)\n",
        "            print(f\" â­ New Best: {best_loss:.4f}\")\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optim.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "        }, LAST)\n",
        "\n",
        "    print(\"âœ… Training Complete.\")\n",
        "    print(f\"Best Model Saved to: {BEST}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_crnn_nosmooth()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efk-TmGVppX6",
        "outputId": "79903199-a34f-4021-c9fa-794d134dbc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Training V5: CRNN (CNN + BiLSTM) | NO SMOOTHING | cuda\n",
            "ðŸ“‚ Data: /content/data_unique\n",
            "ðŸ”§ Epochs: 100 | Adaptive LR: ON\n",
            "âœ… Loaded 4051 files\n",
            "Epoch 1/100 | Avg Loss: 0.2556 | LR: 0.000100\n",
            " â­ New Best: 0.2556\n",
            "Epoch 2/100 | Avg Loss: 0.1378 | LR: 0.000100\n",
            " â­ New Best: 0.1378\n",
            "Epoch 3/100 | Avg Loss: 0.1092 | LR: 0.000100\n",
            " â­ New Best: 0.1092\n",
            "Epoch 4/100 | Avg Loss: 0.0908 | LR: 0.000100\n",
            " â­ New Best: 0.0908\n",
            "Epoch 5/100 | Avg Loss: 0.0907 | LR: 0.000100\n",
            " â­ New Best: 0.0907\n",
            "Epoch 6/100 | Avg Loss: 0.0754 | LR: 0.000100\n",
            " â­ New Best: 0.0754\n",
            "Epoch 7/100 | Avg Loss: 0.0673 | LR: 0.000100\n",
            " â­ New Best: 0.0673\n",
            "Epoch 8/100 | Avg Loss: 0.0633 | LR: 0.000100\n",
            " â­ New Best: 0.0633\n",
            "Epoch 9/100 | Avg Loss: 0.0596 | LR: 0.000100\n",
            " â­ New Best: 0.0596\n",
            "Epoch 10/100 | Avg Loss: 0.0603 | LR: 0.000100\n",
            "Epoch 11/100 | Avg Loss: 0.0587 | LR: 0.000100\n",
            " â­ New Best: 0.0587\n",
            "Epoch 12/100 | Avg Loss: 0.0544 | LR: 0.000100\n",
            " â­ New Best: 0.0544\n",
            "Epoch 13/100 | Avg Loss: 0.0636 | LR: 0.000100\n",
            "Epoch 14/100 | Avg Loss: 0.0507 | LR: 0.000100\n",
            " â­ New Best: 0.0507\n",
            "Epoch 15/100 | Avg Loss: 0.0478 | LR: 0.000100\n",
            " â­ New Best: 0.0478\n",
            "Epoch 16/100 | Avg Loss: 0.0457 | LR: 0.000100\n",
            " â­ New Best: 0.0457\n",
            "Epoch 17/100 | Avg Loss: 0.0617 | LR: 0.000100\n",
            "Epoch 18/100 | Avg Loss: 0.0541 | LR: 0.000100\n",
            "Epoch 19/100 | Avg Loss: 0.0494 | LR: 0.000100\n",
            "Epoch 20/100 | Avg Loss: 0.0435 | LR: 0.000100\n",
            " â­ New Best: 0.0435\n",
            "Epoch 21/100 | Avg Loss: 0.0428 | LR: 0.000100\n",
            " â­ New Best: 0.0428\n",
            "Epoch 22/100 | Avg Loss: 0.0487 | LR: 0.000100\n",
            "Epoch 23/100 | Avg Loss: 0.0448 | LR: 0.000100\n",
            "Epoch 24/100 | Avg Loss: 0.0455 | LR: 0.000100\n",
            "Epoch 25/100 | Avg Loss: 0.0416 | LR: 0.000100\n",
            " â­ New Best: 0.0416\n",
            "Epoch 26/100 | Avg Loss: 0.0468 | LR: 0.000100\n",
            "Epoch 27/100 | Avg Loss: 0.0306 | LR: 0.000100\n",
            " â­ New Best: 0.0306\n",
            "Epoch 28/100 | Avg Loss: 0.0504 | LR: 0.000100\n",
            "Epoch 29/100 | Avg Loss: 0.0425 | LR: 0.000100\n",
            "Epoch 30/100 | Avg Loss: 0.0403 | LR: 0.000100\n",
            "Epoch 31/100 | Avg Loss: 0.0364 | LR: 0.000100\n",
            "Epoch 32/100 | Avg Loss: 0.0459 | LR: 0.000100\n",
            "Epoch 33/100 | Avg Loss: 0.0398 | LR: 0.000100\n",
            "Epoch 34/100 | Avg Loss: 0.0460 | LR: 0.000100\n",
            "Epoch 35/100 | Avg Loss: 0.0355 | LR: 0.000100\n",
            "Epoch 36/100 | Avg Loss: 0.0400 | LR: 0.000100\n",
            "Epoch 37/100 | Avg Loss: 0.0309 | LR: 0.000050\n",
            "Epoch 38/100 | Avg Loss: 0.0365 | LR: 0.000050\n",
            "Epoch 39/100 | Avg Loss: 0.0341 | LR: 0.000050\n",
            "Epoch 40/100 | Avg Loss: 0.0309 | LR: 0.000050\n",
            "Epoch 41/100 | Avg Loss: 0.0260 | LR: 0.000050\n",
            " â­ New Best: 0.0260\n",
            "Epoch 42/100 | Avg Loss: 0.0301 | LR: 0.000050\n",
            "Epoch 43/100 | Avg Loss: 0.0280 | LR: 0.000050\n",
            "Epoch 44/100 | Avg Loss: 0.0286 | LR: 0.000050\n",
            "Epoch 45/100 | Avg Loss: 0.0264 | LR: 0.000050\n",
            "Epoch 46/100 | Avg Loss: 0.0367 | LR: 0.000050\n",
            "Epoch 47/100 | Avg Loss: 0.0282 | LR: 0.000050\n",
            "Epoch 48/100 | Avg Loss: 0.0235 | LR: 0.000050\n",
            " â­ New Best: 0.0235\n",
            "Epoch 49/100 | Avg Loss: 0.0234 | LR: 0.000050\n",
            " â­ New Best: 0.0234\n",
            "Epoch 50/100 | Avg Loss: 0.0311 | LR: 0.000050\n",
            "Epoch 51/100 | Avg Loss: 0.0327 | LR: 0.000050\n",
            "Epoch 52/100 | Avg Loss: 0.0266 | LR: 0.000050\n",
            "Epoch 53/100 | Avg Loss: 0.0229 | LR: 0.000050\n",
            " â­ New Best: 0.0229\n",
            "Epoch 54/100 | Avg Loss: 0.0266 | LR: 0.000050\n",
            "Epoch 55/100 | Avg Loss: 0.0274 | LR: 0.000050\n",
            "Epoch 56/100 | Avg Loss: 0.0317 | LR: 0.000050\n",
            "Epoch 57/100 | Avg Loss: 0.0198 | LR: 0.000050\n",
            " â­ New Best: 0.0198\n",
            "Epoch 58/100 | Avg Loss: 0.0288 | LR: 0.000050\n",
            "Epoch 59/100 | Avg Loss: 0.0317 | LR: 0.000050\n",
            "Epoch 60/100 | Avg Loss: 0.0205 | LR: 0.000050\n",
            "Epoch 61/100 | Avg Loss: 0.0264 | LR: 0.000050\n",
            "Epoch 62/100 | Avg Loss: 0.0245 | LR: 0.000050\n",
            "Epoch 63/100 | Avg Loss: 0.0230 | LR: 0.000050\n",
            "Epoch 64/100 | Avg Loss: 0.0291 | LR: 0.000050\n",
            "Epoch 65/100 | Avg Loss: 0.0231 | LR: 0.000050\n",
            "Epoch 66/100 | Avg Loss: 0.0148 | LR: 0.000050\n",
            " â­ New Best: 0.0148\n",
            "Epoch 67/100 | Avg Loss: 0.0249 | LR: 0.000050\n",
            "Epoch 68/100 | Avg Loss: 0.0234 | LR: 0.000050\n",
            "Epoch 69/100 | Avg Loss: 0.0208 | LR: 0.000050\n",
            "Epoch 70/100 | Avg Loss: 0.0255 | LR: 0.000050\n",
            "Epoch 71/100 | Avg Loss: 0.0262 | LR: 0.000050\n",
            "Epoch 72/100 | Avg Loss: 0.0289 | LR: 0.000050\n",
            "Epoch 73/100 | Avg Loss: 0.0197 | LR: 0.000050\n",
            "Epoch 74/100 | Avg Loss: 0.0204 | LR: 0.000050\n",
            "Epoch 75/100 | Avg Loss: 0.0212 | LR: 0.000050\n",
            "Epoch 76/100 | Avg Loss: 0.0189 | LR: 0.000025\n",
            "Epoch 77/100 | Avg Loss: 0.0228 | LR: 0.000025\n",
            "Epoch 78/100 | Avg Loss: 0.0145 | LR: 0.000025\n",
            " â­ New Best: 0.0145\n",
            "Epoch 79/100 | Avg Loss: 0.0229 | LR: 0.000025\n",
            "Epoch 80/100 | Avg Loss: 0.0151 | LR: 0.000025\n",
            "Epoch 81/100 | Avg Loss: 0.0194 | LR: 0.000025\n",
            "Epoch 82/100 | Avg Loss: 0.0230 | LR: 0.000025\n",
            "Epoch 83/100 | Avg Loss: 0.0191 | LR: 0.000025\n",
            "Epoch 84/100 | Avg Loss: 0.0185 | LR: 0.000025\n",
            "Epoch 85/100 | Avg Loss: 0.0278 | LR: 0.000025\n",
            "Epoch 86/100 | Avg Loss: 0.0174 | LR: 0.000025\n",
            "Epoch 87/100 | Avg Loss: 0.0151 | LR: 0.000025\n",
            "Epoch 88/100 | Avg Loss: 0.0169 | LR: 0.000013\n",
            "Epoch 89/100 | Avg Loss: 0.0121 | LR: 0.000013\n",
            " â­ New Best: 0.0121\n",
            "Epoch 90/100 | Avg Loss: 0.0138 | LR: 0.000013\n",
            "Epoch 91/100 | Avg Loss: 0.0164 | LR: 0.000013\n",
            "Epoch 92/100 | Avg Loss: 0.0108 | LR: 0.000013\n",
            " â­ New Best: 0.0108\n",
            "Epoch 93/100 | Avg Loss: 0.0164 | LR: 0.000013\n",
            "Epoch 94/100 | Avg Loss: 0.0251 | LR: 0.000013\n",
            "Epoch 95/100 | Avg Loss: 0.0130 | LR: 0.000013\n",
            "Epoch 96/100 | Avg Loss: 0.0177 | LR: 0.000013\n",
            "Epoch 97/100 | Avg Loss: 0.0188 | LR: 0.000013\n",
            "Epoch 98/100 | Avg Loss: 0.0120 | LR: 0.000013\n",
            "Epoch 99/100 | Avg Loss: 0.0170 | LR: 0.000013\n",
            "Epoch 100/100 | Avg Loss: 0.0191 | LR: 0.000013\n",
            "âœ… Training Complete.\n",
            "Best Model Saved to: /content/pitch_modelV5_nosmooth/best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EVAL V5: CRNN (NO SMOOTHING) + GEOMETRIC SCORING\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.signal as sg\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Data Paths\n",
        "VAL_DIR = \"/content/eval\"  # 400 unseen files\n",
        "# Pointing to the NO SMOOTHING checkpoint\n",
        "MODEL_PATH = \"/content/pitch_modelV5_nosmooth/best.pth\"\n",
        "\n",
        "# Params\n",
        "WIN_LEN = 300\n",
        "HOP_LEN = 150\n",
        "TOLERANCE = 1.0    # Time bucket tolerance\n",
        "TOP_K_MATCHES = 20\n",
        "NUM_TRIALS = 400   # Full coverage\n",
        "SEGMENT_LEN = 1500 # 15 seconds\n",
        "\n",
        "# ======================================================\n",
        "# 1. MODEL ARCHITECTURE (MUST MATCH V5 TRAINING)\n",
        "# ======================================================\n",
        "class PitchCRNN(nn.Module):\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. CNN Feature Extractor\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 300 -> 150\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.MaxPool1d(2), # 150 -> 75\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # 2. Deep Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=256,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # 3. Projection Head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # A. CNN Forward\n",
        "        x = self.cnn(x)  # (Batch, 256, 75)\n",
        "\n",
        "        # B. Prepare for LSTM (Permute to Batch, Seq, Feat)\n",
        "        x = x.permute(0, 2, 1) # (Batch, 75, 256)\n",
        "\n",
        "        # C. LSTM Forward\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x)\n",
        "\n",
        "        # D. Global Average Pooling (Over time dimension)\n",
        "        out = torch.mean(out, dim=1) # (Batch, 256)\n",
        "\n",
        "        # E. Projection\n",
        "        out = self.fc(out) # (Batch, 128)\n",
        "\n",
        "        return F.normalize(out, p=2, dim=1)\n",
        "\n",
        "print(f\"â³ Loading V5 CRNN (No Smooth) Model from {MODEL_PATH}...\")\n",
        "model = PitchCRNN(embed_dim=128).to(DEVICE)\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "    if \"model\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    exit()\n",
        "model.eval()\n",
        "\n",
        "# ======================================================\n",
        "# 2. AUGMENTATION (Proper Soft & Hard)\n",
        "# ======================================================\n",
        "def humify_soft(arr):\n",
        "    \"\"\"Soft Hum: Light Noise only.\"\"\"\n",
        "    arr = arr.copy()\n",
        "    arr += np.random.normal(0, 0.02, size=len(arr))\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "def humify_hard(arr):\n",
        "    \"\"\"Hard Hum: Noise + Key Shift + Time Warping\"\"\"\n",
        "    arr = arr.copy()\n",
        "\n",
        "    # 1. Jitter\n",
        "    arr += np.random.normal(0, 0.06, size=len(arr))\n",
        "\n",
        "    # 2. Key Shift\n",
        "    semitones = np.random.uniform(-3, 3)\n",
        "    arr[arr > 0] += semitones * 0.057\n",
        "\n",
        "    # 3. TIME WARP (Crucial for testing LSTM robustness)\n",
        "    target_len = SEGMENT_LEN # 1500\n",
        "    if random.random() < 0.8: # 80% chance\n",
        "        rate = np.random.uniform(0.85, 1.15)\n",
        "        old_idx = np.arange(len(arr))\n",
        "        new_len = int(len(arr) * rate)\n",
        "        new_idx = np.linspace(0, len(arr)-1, new_len)\n",
        "        arr = np.interp(new_idx, old_idx, arr)\n",
        "\n",
        "        # Force back to target length\n",
        "        if len(arr) < target_len:\n",
        "            arr = np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
        "        else:\n",
        "            start = (len(arr) - target_len) // 2\n",
        "            arr = arr[start:start+target_len]\n",
        "\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. EMBEDDING + DB BUILDER (No Smoothing applied here)\n",
        "# ======================================================\n",
        "def process_sequence_to_embeddings(arr):\n",
        "    \"\"\"Returns embeddings and time offsets.\n",
        "    NOTE: Processes in mini-batches to prevent OOM.\"\"\"\n",
        "\n",
        "    # NO SMOOTHING CALL HERE (Matches Training V5 No-Smooth)\n",
        "\n",
        "    windows = []\n",
        "    offsets = []\n",
        "\n",
        "    i = 0\n",
        "    while i + WIN_LEN <= len(arr):\n",
        "        crop = arr[i : i + WIN_LEN]\n",
        "        if np.mean(crop > 0) < 0.1: # Skip silence\n",
        "            i += HOP_LEN\n",
        "            continue\n",
        "        windows.append(crop)\n",
        "        offsets.append(i / 100.0)\n",
        "        i += HOP_LEN\n",
        "\n",
        "    if not windows:\n",
        "        return None, None\n",
        "\n",
        "    windows_np = np.stack(windows)\n",
        "    windows_tensor = torch.from_numpy(windows_np).float().unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    # --- BATCH PROCESSING (Prevents OOM) ---\n",
        "    batch_size = 64\n",
        "    embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for k in range(0, len(windows_tensor), batch_size):\n",
        "            batch = windows_tensor[k : k + batch_size]\n",
        "            emb_batch = model.forward_one(batch)\n",
        "            embeddings_list.append(emb_batch)\n",
        "\n",
        "    embeddings = torch.cat(embeddings_list, dim=0)\n",
        "\n",
        "    return embeddings, offsets\n",
        "\n",
        "def build_flat_database():\n",
        "    files = sorted([f for f in os.listdir(VAL_DIR) if f.endswith(\".npy\")])\n",
        "\n",
        "    all_embeds_list = []\n",
        "    metadata = []\n",
        "\n",
        "    print(f\"ðŸ—ï¸ Building Geometric DB (RAW PITCH) from {len(files)} songs in {VAL_DIR}...\")\n",
        "\n",
        "    for f_name in tqdm(files):\n",
        "        path = os.path.join(VAL_DIR, f_name)\n",
        "        arr = np.load(path)\n",
        "\n",
        "        # Raw pitch goes directly into embedding\n",
        "        embeds, offsets = process_sequence_to_embeddings(arr)\n",
        "        if embeds is None: continue\n",
        "\n",
        "        all_embeds_list.append(embeds)\n",
        "        song_id = f_name.replace(\".npy\", \"\")\n",
        "\n",
        "        for t in offsets:\n",
        "            metadata.append((song_id, t))\n",
        "\n",
        "    full_db_tensor = torch.cat(all_embeds_list, dim=0)\n",
        "    print(f\"âœ… DB Built: {full_db_tensor.shape[0]} windows across {len(files)} songs.\")\n",
        "    return full_db_tensor, metadata\n",
        "\n",
        "# ======================================================\n",
        "# 4. GEOMETRIC SCORING\n",
        "# ======================================================\n",
        "def query_geometric(query_embeds, query_offsets, db_tensor, db_metadata):\n",
        "    # 1. Distance Matrix\n",
        "    dists = torch.cdist(query_embeds, db_tensor, p=2)\n",
        "\n",
        "    # 2. Top-K\n",
        "    top_vals, top_inds = torch.topk(dists, k=TOP_K_MATCHES, dim=1, largest=False)\n",
        "    top_vals = top_vals.cpu().numpy()\n",
        "    top_inds = top_inds.cpu().numpy()\n",
        "\n",
        "    vote_buckets = defaultdict(float)\n",
        "    epsilon = 1e-4\n",
        "\n",
        "    for q_idx, q_time in enumerate(query_offsets):\n",
        "        for k in range(TOP_K_MATCHES):\n",
        "            match_idx = top_inds[q_idx, k]\n",
        "            dist = top_vals[q_idx, k]\n",
        "\n",
        "            match_song, match_time = db_metadata[match_idx]\n",
        "\n",
        "            # 3. Geometric Alignment\n",
        "            projected_start = match_time - q_time\n",
        "            bucket = int(round(projected_start / TOLERANCE))\n",
        "\n",
        "            # Score\n",
        "            score = 1.0 / (dist + epsilon)\n",
        "            vote_buckets[(match_song, bucket)] += score\n",
        "\n",
        "    # 4. Max Score per Song\n",
        "    song_final_scores = defaultdict(float)\n",
        "    for (song, bucket), score in vote_buckets.items():\n",
        "        if score > song_final_scores[song]:\n",
        "            song_final_scores[song] = score\n",
        "\n",
        "    ranked_songs = sorted(song_final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in ranked_songs]\n",
        "\n",
        "# ======================================================\n",
        "# 5. RUN EVAL\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "    db_tensor, db_metadata = build_flat_database()\n",
        "    song_list = list(set([m[0] for m in db_metadata]))\n",
        "\n",
        "    results = {\n",
        "        \"Soft\": {\"top1\": 0, \"top5\": 0, \"top10\": 0},\n",
        "        \"Hard\": {\"top1\": 0, \"top5\": 0, \"top10\": 0}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸš€ Running {NUM_TRIALS} Trials with Geometric Scoring...\")\n",
        "\n",
        "    effective_trials = {\"Soft\": 0, \"Hard\": 0}\n",
        "\n",
        "    for _ in tqdm(range(NUM_TRIALS)):\n",
        "        target_song = random.choice(song_list)\n",
        "        full_arr = np.load(os.path.join(VAL_DIR, f\"{target_song}.npy\"))\n",
        "\n",
        "        if len(full_arr) < SEGMENT_LEN: continue\n",
        "\n",
        "        start_idx = np.random.randint(0, len(full_arr) - SEGMENT_LEN)\n",
        "\n",
        "        # BASE: Raw, unsmoothed clip (Matches V5 No-Smooth training)\n",
        "        raw_clip = full_arr[start_idx : start_idx + SEGMENT_LEN]\n",
        "\n",
        "        # --- Test Soft ---\n",
        "        soft_hum_clip = humify_soft(raw_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(soft_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Soft\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Soft\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Soft\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Soft\"][\"top10\"] += 1\n",
        "\n",
        "        # --- Test Hard ---\n",
        "        hard_hum_clip = humify_hard(raw_clip)\n",
        "        q_emb, q_off = process_sequence_to_embeddings(hard_hum_clip)\n",
        "\n",
        "        if q_emb is not None:\n",
        "            ranked = query_geometric(q_emb, q_off, db_tensor, db_metadata)\n",
        "            effective_trials[\"Hard\"] += 1\n",
        "            if ranked:\n",
        "                if ranked[0] == target_song: results[\"Hard\"][\"top1\"] += 1\n",
        "                if target_song in ranked[:5]: results[\"Hard\"][\"top5\"] += 1\n",
        "                if target_song in ranked[:10]: results[\"Hard\"][\"top10\"] += 1\n",
        "\n",
        "    # ======================================================\n",
        "    # FINAL REPORT\n",
        "    # ======================================================\n",
        "    def calc_acc(res, key, total):\n",
        "        return res[key] / total if total > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š V5 CRNN (NO SMOOTHING) RESULTS (400 Files)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Effective Soft Trials: {effective_trials['Soft']}\")\n",
        "    print(f\"Total Effective Hard Trials: {effective_trials['Hard']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"ðŸŽ¤ Soft Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Soft'], 'top1', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Soft'], 'top5', effective_trials['Soft']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Soft'], 'top10', effective_trials['Soft']):.1%}\")\n",
        "\n",
        "    print(f\"\\nðŸ”¥ Hard Hum:\")\n",
        "    print(f\"   Top-1:  {calc_acc(results['Hard'], 'top1', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-5:  {calc_acc(results['Hard'], 'top5', effective_trials['Hard']):.1%}\")\n",
        "    print(f\"   Top-10: {calc_acc(results['Hard'], 'top10', effective_trials['Hard']):.1%}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vobcQ2LPPltw",
        "outputId": "b20b98d2-4c28-4141-da15-b0228faab7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading V5 CRNN (No Smooth) Model from /content/pitch_modelV5_nosmooth/best.pth...\n",
            "âœ… Model loaded successfully.\n",
            "ðŸ—ï¸ Building Geometric DB (RAW PITCH) from 400 songs in /content/eval...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:06<00:00, 66.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DB Built: 63580 windows across 400 songs.\n",
            "\n",
            "ðŸš€ Running 400 Trials with Geometric Scoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:04<00:00, 88.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“Š V5 CRNN (NO SMOOTHING) RESULTS (400 Files)\n",
            "==================================================\n",
            "Total Effective Soft Trials: 400\n",
            "Total Effective Hard Trials: 400\n",
            "--------------------------------------------------\n",
            "ðŸŽ¤ Soft Hum:\n",
            "   Top-1:  51.2%\n",
            "   Top-5:  59.8%\n",
            "   Top-10: 61.0%\n",
            "\n",
            "ðŸ”¥ Hard Hum:\n",
            "   Top-1:  47.2%\n",
            "   Top-5:  52.2%\n",
            "   Top-10: 56.0%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¤ Soft Hum:\n",
        "   Top-1:  51.2%\n",
        "   Top-5:  59.8%\n",
        "   Top-10: 61.0%\n",
        "\n",
        "ðŸ”¥ Hard Hum:\n",
        "   Top-1:  47.2%\n",
        "   Top-5:  52.2%\n",
        "   Top-10: 56.0%"
      ],
      "metadata": {
        "id": "jbnXM6Ve9xnI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwMx_p42Dc2d",
        "outputId": "f1412c3c-1aae-4c77-8797-8cb5cafe5856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: /content/pitch_modelV6/best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Eval DB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:24<00:00,  8.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval DB size: 199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11<00:00,  8.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================\n",
            "ðŸŽµ CLEAN RESULTS\n",
            "Top-1: 0.28\n",
            "Top-5: 0.46\n",
            "\n",
            "ðŸŽ¤ SOFT HUM RESULTS\n",
            "Top-1: 0.24\n",
            "Top-5: 0.32\n",
            "\n",
            "ðŸ”¥ HARD HUM RESULTS\n",
            "Top-1: 0.19\n",
            "Top-5: 0.27\n",
            "====================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽµ CLEAN RESULTS\n",
        "Top-1: 0.28\n",
        "Top-5: 0.46\n",
        "\n",
        "ðŸŽ¤ SOFT HUM RESULTS\n",
        "Top-1: 0.24\n",
        "Top-5: 0.32\n",
        "\n",
        "ðŸ”¥ HARD HUM RESULTS\n",
        "Top-1: 0.19\n",
        "Top-5: 0.27"
      ],
      "metadata": {
        "id": "maMbLMuT93MX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3o1EG2apt_vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nS9XZB1s7fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QLg_nsSzRfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2qUA9TszRie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZgM_Q0d4S5AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6zQ6T2CS5B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVAyj_dMS5DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQ2GbHAQS5FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YduRaCaAS5Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFEbZfpiS5H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3j1gF9aS5JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8KNL8XCSWFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azkqrBzzWpYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "pcOJmWvzmlBL",
        "outputId": "aaa8d648-57d3-42a3-835a-3237d9fb1953"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cS9yB56LZA6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Xn0SHllZdG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPDzOgtfWpDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YK41qmiXYu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqnqusNhXljd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQLRKE9XXYrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7sDN0CrWpAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}